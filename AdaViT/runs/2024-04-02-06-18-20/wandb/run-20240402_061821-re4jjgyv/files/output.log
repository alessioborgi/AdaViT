Archive found at AdaViT/imagenette_dataset/imagenette.zip, skipping download
Extracted file found at AdaViT/imagenette_dataset/imagenette2-160, skipping extraction
Warning: num_classes is not used for Imagenette dataset.
Ignoring the argument and using default number of classes in this dataset (10).
Loading timm pretrained weights:  ['facebookresearch/deit:main', 'deit_tiny_patch16_224']
Downloading timm pretrained weights:  ['facebookresearch/deit:main', 'deit_tiny_patch16_224']
Downloading: "https://github.com/facebookresearch/deit/zipball/main" to /home/studio-lab-user/.cache/torch/hub/main.zip
Downloading: "https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth" to /home/studio-lab-user/.cache/torch/hub/checkpoints/deit_tiny_patch16_224-a1311bcf.pth
 15%|████████████████████████████████▏                                                                                                                                                                                      | 3.27M/21.9M [00:00<00:01, 12.8MB/s]
Loading weights for a different number of classes. Replacing head with random weights. You should fine-tune the model.
The Model is:  AdaptiveVisionTransformer(
  (conv_proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
  (encoder): AViTEncoder(
    (dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-11): 12 x AViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
    )
    (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (head): Linear(in_features=192, out_features=10, bias=True)
)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21.9M/21.9M [00:00<00:00, 37.6MB/s]
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/studio-lab-user/sagemaker-studiolab-notebooks/AdaViT/train/train.py", line 80, in train
    main_criterion = instantiate(cfg.loss.classification_loss)
omegaconf.errors.ConfigAttributeError: Key 'classification_loss' is not in struct
    full_key: loss.classification_loss
    object_type=dict
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.