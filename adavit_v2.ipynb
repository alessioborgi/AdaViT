{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":109264,"sourceType":"datasetVersion","datasetId":56828},{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506},{"sourceId":2251350,"sourceType":"datasetVersion","datasetId":1354190},{"sourceId":7270893,"sourceType":"datasetVersion","datasetId":4214880},{"sourceId":7319863,"sourceType":"datasetVersion","datasetId":4247810}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TO-DO\n\nWe need to try/finish to do:\n- 1 Group: \n    - Implement attention map.\n    - Try patchyfing with convolutions.\n    - Find out how Tensorboard works. \n- 2 Group:\n    - Try Ensemble on Classification (Potential Bright Idea :) \n    - Try different versions of Attention (ARPR).\n    - Try different versions of Embedding (LaPE, 2D Positional Embeddings). \n\n- Others:\n    - Make a version with MNIST. \n    - Put LogSoftmax also in the MHSA? \n    - Retry to use Augmentation. \n    - Start writing Report. ","metadata":{}},{"cell_type":"markdown","source":"### QUESTIONS/DOUBTS\n- 3. A-ViT, second column, after equation (4): ...\"We incorporate H(.) into the existing Vision trasnformer block by allocating a single neuron in the MLP layer to do the task\". ","metadata":{}},{"cell_type":"markdown","source":"### REFERENCES\n\n- https://arxiv.org/pdf/2112.07658.pdf\n- https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c","metadata":{}},{"cell_type":"markdown","source":"### 0: IMPORTING LIBRARIES AND SETTING THE SEEDS","metadata":{}},{"cell_type":"code","source":"\n# Importing PyTorch-related libraries\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToPILImage\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchmetrics.classification import Accuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n\n# Importing PyTorch Lightning-Related libraries\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\nfrom pytorch_lightning.callbacks import TQDMProgressBar, LearningRateMonitor, ModelCheckpoint\n\n# Importing General Libraries\nimport os\nimport csv\nimport PIL\nimport random\nimport numpy as np\nfrom PIL import Image\nimport seaborn as sns\nfrom pathlib import Path\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results.\n    \n    Arguments:\n        - seed {int} : Number of the seed.\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    pl.seed_everything(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n# Set the seed.\nseed_everything(31)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1: DATA INSPECTION","metadata":{}},{"cell_type":"markdown","source":"#### 1.1: CREATION OF THE LABEL DICTIONARY","metadata":{}},{"cell_type":"code","source":"\n# Initialize the Mapping Dictionary to be empty.\nmapping_dict = {}\n\n# Open the file in read mode.\nwith open('/kaggle/input/tiny-imagenet/tiny-imagenet-200/words.txt', 'r') as file:\n    \n    # Read each line from the file.\n    for line in file:\n        # Split the line into tokens based on whitespace.\n        tokens = line.strip().split('\\t')\n        \n        # Check if there are at least two tokens.\n        if len(tokens) >= 2:\n            # Extract the encoded label (left) and actual label (right).\n            encoded_label, actual_label = tokens[0], tokens[1]\n            \n            # Add the mapping to the dictionary.\n            mapping_dict[encoded_label] = actual_label\n\n# Print the mapping dictionary.\n#print(mapping_dict)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2: DISPLAYING EXAMPLES OF THE DATASET","metadata":{}},{"cell_type":"code","source":"\n# Loading the dataset using ImageFolder.\ndataset0 = datasets.ImageFolder(root=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", transform=None)\n\n# Extract class names and their counts.\nclass_names = dataset0.classes\nclass_counts = [dataset0.targets.count(i) for i in range(len(class_names))]\n\n# Setting the seed.\nnp.random.seed(31)\n\n# Create a grid of 10 images with labels.\nplt.figure(figsize=(15, 8))\nfor i in range(10):\n    \n    # Randomly select an image and its corresponding label.\n    index = np.random.randint(len(dataset0))\n    image, label = dataset0[index]\n\n    # Display the image with its label\n    plt.subplot(2, 5, i+1)\n    plt.imshow(np.array(image))  # Convert the PIL Image to a numpy array\n    plt.title(f\"Label: {class_names[label]}\")\n    plt.axis('off')\n\n# Displaying Datasets examples.\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3: DISPLAYING EXAMPLES OF THE DATASET WITH DECODED LABELS","metadata":{}},{"cell_type":"code","source":"\n# Loading the dataset using ImageFolder.\ndataset0 = datasets.ImageFolder(root=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", transform=None)\n\n# Extract class names and their counts.\nclass_names = dataset0.classes\nclass_counts = [dataset0.targets.count(i) for i in range(len(class_names))]\n\n# Setting the seed.\nnp.random.seed(31)\n\n# Create a grid of 10 images with labels.\nplt.figure(figsize=(15, 8))\n\nfor i in range(10):\n    \n    # Randomly select an image and its corresponding label.\n    index = np.random.randint(len(dataset0))\n    image, encoded_label = dataset0[index]\n    \n    # Look up the actual label using the mapping dictionary.\n    actual_label = mapping_dict.get(class_names[encoded_label], \"Unknown Label\")\n    \n    # Trim the label if it exceeds the maximum length.\n    actual_label_trimmed = actual_label[:15] + '...' if len(actual_label) > 15 else actual_label\n\n    # Display the image with its label..\n    plt.subplot(2, 5, i+1)\n    plt.imshow(np.array(image))  \n    plt.title(f\"Label: {actual_label_trimmed}\", wrap=True)\n    plt.axis('off')\n\n# Displaying Dataset examples.\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2: DATA-MODULE DEFINITION","metadata":{}},{"cell_type":"markdown","source":"#### 2.0: CUSTOMIZED TRANSFORM CLASS","metadata":{}},{"cell_type":"code","source":"\nclass AdaViT_Transformations:\n    \n    def __init__(self):\n        \n        # Constructor - Nothing to initialize in this case\n        pass\n\n    def __call__(self, sample):\n        \"\"\"\n        Call method to perform transformations on the input sample.\n\n        Args:\n        - sample (PIL.Image.Image or torch.Tensor): Input image sample.\n\n        Returns:\n        - transformed_sample (torch.Tensor): Transformed image sample.\n        \"\"\"\n\n        # Define a series of image transformations using \"torchvision.Compose\" function.\n        transform = transforms.Compose([\n            transforms.ToTensor(),  \n            # Additional transformations can be added here.\n            # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  \n            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n        ])\n\n        # Apply the defined transformations to the input sample.\n        transformed_sample = transform(sample)\n\n        return transformed_sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1: CUSTOMIZED TRAINING SET VERSION","metadata":{}},{"cell_type":"code","source":"\nclass CustomTrainingTinyImagenet(ImageFolder):\n    \n    def __init__(self, root, transform=None):\n        \"\"\"\n        Custom dataset class for Tiny ImageNet Training data.\n\n        Args:\n        - root (str): Root directory containing the dataset.\n        - transform (callable, optional): Optional transform to be applied to the Input Image.\n        \"\"\"\n        super(CustomTrainingTinyImagenet, self).__init__(root, transform=transform)\n\n        # Create mappings between class labels and numerical indices\n        self.class_to_index = {cls: idx for idx, cls in enumerate(sorted(self.classes))}\n        self.index_to_class = {idx: cls for cls, idx in self.class_to_index.items()}\n\n    def __getitem__(self, index):\n        \"\"\"\n        Method to retrieve an item from the dataset.\n\n        Args:\n        - index (int): Index of the item to retrieve.\n\n        Returns:\n        - sample (torch.Tensor): Transformed image sample.\n        - target (int): Numerical index corresponding to the class label.\n        \"\"\"\n        # Retrieve the item and its label from the Dataset.\n        path, target = self.samples[index]\n\n        # Load the image using the default loader.\n        sample = self.loader(path)\n\n        # Apply the specified transformations, if any.\n        if self.transform is not None:\n            sample = self.transform(sample)\n\n        # Adjust the directory depth to get the target label.\n        target_str = os.path.basename(os.path.dirname(os.path.dirname(path)))\n\n        # Convert string label to numerical index using the mapping.\n        target = self.class_to_index[target_str]\n\n        return sample, target\n\n    def get_class_from_index(self, index):\n        \"\"\"\n        Method to retrieve the class label from a numerical index.\n\n        Args:\n        - index (int): Numerical index corresponding to the class label.\n\n        Returns:\n        - class_label (str): Class label corresponding to the numerical index.\n        \"\"\"\n        \n        return self.index_to_class[index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2: CUSTOMIZED VALIDATION SET VERSION","metadata":{}},{"cell_type":"code","source":"\nclass CustomValidationTinyImagenet(pl.LightningDataModule):\n    \n    def __init__(self, root, transform=None):\n        \"\"\"\n        Custom data module for Tiny ImageNet Validation data.\n\n        Args:\n        - root (str): Root directory containing the dataset.\n        - transform (callable, optional): Optional transform to be applied to the Input Image.\n        \"\"\"\n        self.root = Path(root)\n        self.transform = transform\n\n        # Load and preprocess labels\n        self.labels = self.load_labels()\n        self.label_to_index = {label: idx for idx, label in enumerate(sorted(set(self.labels.values())))}\n        self.index_to_label = {idx: label for label, idx in self.label_to_index.items()}\n\n    def load_labels(self):\n        \"\"\"\n        Method to load and Pre-Process Labels from the Validation Dataset.\n\n        Returns:\n        - labels (dict): Dictionary mapping image names to labels.\n        \"\"\"\n        label_path = \"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/val_annotations.txt\"\n        labels = {}\n\n        with open(label_path, \"r\") as f:\n            lines = f.readlines()\n\n        for line in lines:\n            parts = line.split(\"\\t\")\n            image_name, label = parts[0], parts[1]\n            labels[image_name] = label\n\n        return labels\n\n    def __len__(self):\n        \"\"\"\n        Method to get the length of the dataset.\n\n        Returns:\n        - length (int): Number of items in the dataset.\n        \"\"\"\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Method to retrieve an item from the dataset.\n\n        Args:\n        - index (int): Index of the item to retrieve.\n\n        Returns:\n        - image (torch.Tensor): Transformed image sample.\n        - label (int): Numerical index corresponding to the class label.\n        \"\"\"\n        image_name = f\"val_{index}.JPEG\"\n        image_path = self.root / image_name\n\n        # Open the image using PIL and convert to RGB.\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Apply the specified transformations, if any.\n        if self.transform:\n            image = self.transform(image)\n\n        # Use the get method to handle cases where the key is not present.\n        label_str = self.labels.get(image_name, 'Label not found')\n\n        # Convert string label to numerical index using the mapping.\n        label = self.label_to_index[label_str]\n\n        return image, label\n\n    def get_label_from_index(self, index):\n        \"\"\"\n        Method to retrieve the class label from a numerical index.\n\n        Args:\n        - index (int): Numerical index corresponding to the class label.\n\n        Returns:\n        - class_label (str): Class label corresponding to the numerical index.\n        \"\"\"\n        return self.index_to_label[index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3: GENERAL DATA-MODULE DEFINITION","metadata":{}},{"cell_type":"code","source":"\nclass AViT_DataModule(pl.LightningDataModule):\n    \n    def __init__(self, train_data_dir, val_data_dir, batch_size, num_workers=4):\n        \"\"\"\n        Custom data module for AViT model training and validation.\n\n        Args:\n        - train_data_dir (str): Directory path for the training dataset.\n        - val_data_dir (str): Directory path for the validation dataset.\n        - batch_size (int): Batch size for training and validation DataLoader.\n        - num_workers (int, optional): Number of workers for DataLoader (default is 4).\n        \"\"\"\n        super(AViT_DataModule, self).__init__()\n        self.train_data_dir = train_data_dir\n        self.val_data_dir = val_data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        # Use AdaViT transformations for data augmentation\n        self.transform = AdaViT_Transformations()\n\n    def setup(self, stage=None):\n        \"\"\"\n        Method to load and configure datasets for Training and Validation.\n\n        Args:\n        - stage (str, optional): 'fit' for Training and 'test' for Validation (default is None).\n        \"\"\"\n        # Load Train dataset using CustomTrainingTinyImagenet with the new directory structure.\n        self.train_dataset = CustomTrainingTinyImagenet(self.train_data_dir, transform=self.transform)\n\n        # Load Validation dataset.\n        self.val_dataset = CustomValidationTinyImagenet(self.val_data_dir, transform=self.transform)\n\n    def train_dataloader(self):\n        \"\"\"\n        Method to return the DataLoader for the Training Dataset.\n\n        Returns:\n        - train_dataloader (DataLoader): DataLoader for Training.\n        \"\"\"\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n\n    def val_dataloader(self):\n        \"\"\"\n        Method to return the DataLoader for the Validation Dataset.\n\n        Returns:\n        - val_dataloader (DataLoader): DataLoader for Validation.\n        \"\"\"\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.4: TESTING TRAINING AND VALIDATION DATALOADERS","metadata":{}},{"cell_type":"code","source":"\ndef show_images_labels(images, labels, title):\n    \"\"\"\n    Display Images with corresponding Labels.\n\n    Parameters:\n    - images (list of tensors): List of Image tensors.\n    - labels (list): List of corresponding Labels.\n    - title (str): Title for the entire subplot.\n\n    Returns:\n    None\n    \"\"\"\n    # Create a Subplot with 1 row and len(images) columns.\n    fig, axs = plt.subplots(1, len(images), figsize=(8, 4))\n    \n    # Set the title for the entire subplot.\n    fig.suptitle(title)\n\n    # Iterate over Images and Labels.\n    for i, (img, label) in enumerate(zip(images, labels)):\n        # Display each Image in a subplot.\n        axs[i].imshow(transforms.ToPILImage()(img))\n        \n        # Set the title for each subplot with the corresponding label.\n        axs[i].set_title(f\"Label: {label}\")\n        \n        # Turn off axis labels for better Visualization.\n        axs[i].axis('off')\n\n    # Show the entire subplot.\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define the AViT_DataModule.\ndata_module = AViT_DataModule(\n    train_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\",\n    val_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/images/\",\n    batch_size=512  \n)\n\n# Setup the Dataloaders.\ndata_module.setup()\n\n# Get a batch from the Training DataLoader.\ntrain_dataloader = data_module.train_dataloader()\ntrain_batch = next(iter(train_dataloader))\n\n# Get a batch from the Validation DataLoader.\nval_dataloader = data_module.val_dataloader()\nval_batch = next(iter(val_dataloader))\n\n# Show two Images from the Training Batch.\nshow_images_labels(train_batch[0][:2], train_batch[1][:2], title='Training Batch')\n\n# Show two Images from the  Validation Batch\nshow_images_labels(val_batch[0][:2], val_batch[1][:2], title='Validation Batch')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3: MODEL DEFINITION","metadata":{}},{"cell_type":"markdown","source":"#### 3.0: PATCHING FUNCTION DEFINITION","metadata":{}},{"cell_type":"code","source":"\ndef Make_Patches_from_Image(images, n_patches):\n    \"\"\"\n    Extract patches from input images.\n\n    Parameters:\n    - images (torch.Tensor): Input images tensor with shape (batch_size, channels, height, width).\n    - n_patches (int): Number of patches in each dimension.\n\n    Returns:\n    torch.Tensor: Extracted patches tensor with shape (batch_size, n_patches^2, patch_size^2 * channels).\n    \"\"\"\n    # Get the dimensions of the input images.\n    n, c, h, w = images.shape\n\n    # Ensure that the input images are square.\n    assert h == w, \"make_patches_from_image method is implemented for square images only!\"\n\n    # Initialize a tensor to store the extracted patches.\n    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n    patch_size = h // n_patches\n\n    # Loop over each image in the batch.\n    for idx, image in enumerate(images):\n        # Loop over each patch in both dimensions.\n        for i in range(n_patches):\n            for j in range(n_patches):\n                # Extract the patch from the image.\n                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n                # Flatten the patch and store it in the patches tensor.\n                patches[idx, i * n_patches + j] = patch.flatten()\n\n    return patches\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Helper function to Visualize Patches.\ndef visualize_patches(images, n_patches, title):\n    \"\"\"\n    Visualize patches extracted from Images.\n\n    Parameters:\n    - images (torch.Tensor): Input images tensor with shape (batch_size, channels, height, width).\n    - n_patches (int): Number of patches in each dimension.\n    - title (str): Title for the entire subplot.\n\n    Returns:\n    None\n    \"\"\"\n    # Extract patches from the input images using the make_patches_from_image function.\n    patches = Make_Patches_from_Image(images, n_patches)\n    \n    # Create a subplot for visualizing patches.\n    fig, axs = plt.subplots(n_patches, n_patches, figsize=(8, 8))\n    fig.suptitle(title)\n    \n    # Calculate the patch size based on the input images.\n    patch_size = images.shape[-1] // n_patches\n\n    # Loop over each patch in both dimensions.\n    for i in range(n_patches):\n        for j in range(n_patches):\n            # Calculate the index of the patch.\n            patch_index = i * n_patches + j\n            # Reshape each patch to (3, patch_size, patch_size).\n            patch = patches[0, patch_index].reshape(3, patch_size, patch_size).cpu().numpy()\n            # Display the patch in the subplot.\n            axs[i, j].imshow(patch.transpose(1, 2, 0))\n            axs[i, j].axis('off')\n\n    # Show the entire subplot.\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize patches for a Training Image.\nvisualize_patches(train_batch[0], n_patches=8, title='Training Patches')\n\n# Visualize patches for a Validation Image\nvisualize_patches(val_batch[0], n_patches=8, title='Validation Patches')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1: POSITIONAL EMBEDDING DEFINITION","metadata":{}},{"cell_type":"code","source":"def get_positional_embeddings_Basic(sequence_length, d):\n    result = torch.ones(sequence_length, d)\n    for i in range(sequence_length):\n        for j in range(d):\n            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n    return result\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.1.1: SINUSOIDAL POSITIONAL ENCODING (SPE)\n\n","metadata":{}},{"cell_type":"code","source":"\ndef get_positional_embeddings_SPE(sequence_length, d):\n    \"\"\"\n    Generate Positional Embeddings for the Transformer Model.\n\n    Parameters:\n    - sequence_length (int): Length of the input sequence.\n    - d (int): Dimension of the embeddings.\n\n    Returns:\n    torch.Tensor: Positional Embeddings tensor of shape (sequence_length, d).\n    \"\"\"\n    # Generate a tensor of positions from 0 to sequence_length - 1.\n    positions = torch.arange(0, sequence_length).float().view(-1, 1)\n    \n    # Calculate div_term for both sin and cos terms.\n    div_term = torch.exp(torch.arange(0, d, 2).float() * -(np.log(10000.0) / d))\n\n    # Initialize the embeddings tensor with zeros.\n    embeddings = torch.zeros(sequence_length, d)\n    \n    # Compute sin and cos terms and assign them to the embeddings tensor.\n    embeddings[:, 0::2] = torch.sin(positions / div_term)\n    embeddings[:, 1::2] = torch.cos(positions / div_term)\n\n    return embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.1.2: LAYER-ADAPTIVE POSITIONAL EMBEDDING (LaPE)","metadata":{}},{"cell_type":"code","source":"\ndef get_positional_embeddings_LaPE(sequence_length, d, num_layers):\n    \"\"\"\n    Generate Layer-adaptive Positional Embeddings for the Transformer Model.\n\n    Parameters:\n    - sequence_length (int): Length of the input sequence.\n    - d (int): Dimension of the embeddings.\n    - num_layers (int): Number of layers in the Transformer model.\n\n    Returns:\n    torch.Tensor: Layer-adaptive Positional Embeddings tensor of shape (sequence_length, d, num_layers).\n    \"\"\"\n    # Generate a tensor of positions from 0 to sequence_length - 1.\n    positions = torch.arange(0, sequence_length).float().view(-1, 1)\n\n    # Precompute div_terms for each layer.\n    div_terms = torch.exp(torch.arange(0, d, 2).float() * -(np.log(10000.0) / d))\n\n    # Initialize the embeddings tensor with zeros.\n    embeddings = torch.zeros(sequence_length, d, num_layers)\n\n    # Divide the sequence_length by 2 once for efficiency.\n    seq_len_div_2 = sequence_length // 2\n\n    # Compute sin and cos terms for each layer and assign them to the embeddings tensor.\n    for layer in range(num_layers):\n        embeddings[:, :, layer][:, 0:seq_len_div_2] = torch.sin(positions / div_terms[layer])\n        embeddings[:, :, layer][:, seq_len_div_2:] = torch.cos(positions / div_terms[layer])\n\n    return embeddings\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.1.2.1: IMAGE POSITIONAL EMBEDDINGS.","metadata":{}},{"cell_type":"markdown","source":"##### 3.1.3: VISUALIZE POSITIONAL EMBEDDINGS","metadata":{}},{"cell_type":"code","source":"\n# Helper function to Visualize Positional Embeddings.\ndef visualize_positional_embeddings(embeddings):\n    \"\"\"\n    Visualize the Positional Embeddings.\n\n    Parameters:\n    - embeddings (torch.Tensor): Positional embeddings tensor.\n\n    Returns:\n    None\n    \"\"\"\n    \n    # Get the number of dimensions (d) from the Embeddings Tensor.\n    d = embeddings.size(1)\n\n    # Set the figure size for a larger image.\n    plt.figure(figsize=(12, 6))\n\n    # Plot each dimension separately.\n    for i in range(d):\n        plt.plot(embeddings[:, i].numpy(), label=f'Dimension {i}')\n\n    # Set plot labels.\n    plt.xlabel('Position')\n    plt.ylabel('Embedding Value')\n    plt.title('Visualization of Positional Embeddings')\n\n    # Place the legend on the right and diminish its size.\n    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n    \n    # Show the plot.\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function to Visualize Positional Embeddings as a Heatmap.\ndef visualize_positional_embeddings_heatmap(embeddings):\n    \"\"\"\n    Visualize the Positional Embeddings as a Heatmap.\n\n    Parameters:\n    - embeddings (torch.Tensor): Positional embeddings tensor.\n\n    Returns:\n    None\n    \"\"\"\n    \n    # Get the number of dimensions (d) from the Embeddings Tensor.\n    d = embeddings.size(1)\n\n    # Set the figure size for a larger image.\n    plt.figure(figsize=(12, 6))\n\n    # Create a heatmap for the positional embeddings.\n    sns.heatmap(embeddings.T.numpy(), cmap='viridis', cbar_kws={'label': 'Embedding Value'})\n\n    # Set plot labels and title.\n    plt.xlabel('Position')\n    plt.ylabel('Dimension')\n    plt.title('Visualization of Positional Embeddings (Heatmap)')\n    \n    # Show the plot.\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positional_embeddings = get_positional_embeddings_SPE(65, 32)\nvisualize_positional_embeddings(positional_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positional_embeddings = get_positional_embeddings_Basic(65, 32)\nvisualize_positional_embeddings(positional_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positional_embeddings = get_positional_embeddings_LaPE(65, 32, 4)\nvisualize_positional_embeddings(positional_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positional_embeddings = get_positional_embeddings_SPE(65, 32)\nvisualize_positional_embeddings_heatmap(positional_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positional_embeddings = get_positional_embeddings_Basic(65, 32)\nvisualize_positional_embeddings_heatmap(positional_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2: MULTI-HEAD SELF-ATTENTION DEFINITION","metadata":{}},{"cell_type":"code","source":"\nclass MyMHSA(nn.Module):\n    \n    def __init__(self, d, n_heads=2):\n        \"\"\"\n        Multi-Head Self Attention (MHSA) Module.\n\n        Parameters:\n        - d (int): Dimension of the input tokens.\n        - n_heads (int): Number of attention heads.\n\n        Returns:\n        None\n        \"\"\"\n        \n        super(MyMHSA, self).__init__()\n        self.d = d\n        self.n_heads = n_heads\n\n        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n\n        # Split the dimension into n_heads parts.\n        d_head = int(d / n_heads)\n        \n        # Linear mappings for Query(q), Key(k), and Value(v) for each head.\n        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        \n        self.d_head = d_head\n        self.softmax = nn.Softmax(dim=-1)\n        \n        # Initialize weights.\n        self.initialize_weights_msa()\n        \n    def forward(self, sequences):\n        \"\"\"\n        Forward pass of the MHSA module.\n\n        Parameters:\n        - sequences (torch.Tensor): Input token sequences with shape (N, seq_length, token_dim).\n\n        Returns:\n        torch.Tensor: Output tensor after MHSA with shape (N, seq_length, item_dim).\n        \"\"\"\n        \n        result = []\n        for sequence in sequences:\n            \n            seq_result = []\n            for head in range(self.n_heads):\n                \n                # Compute the q,k,v for every head. \n                q_mapping, k_mapping, v_mapping = self.q_mappings[head], self.k_mappings[head], self.v_mappings[head]\n\n                # Extract the corresponding part of the sequence for the current head.\n                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n\n                # Calculate attention scores and apply softmax.\n                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n                seq_result.append(attention @ v)\n            \n            # Concatenate the results coming from the different Heads and Stack Vertically the result.\n            result.append(torch.hstack(seq_result))\n        \n        # Concatenate results for all the sequences.\n        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n    \n    def initialize_weights_msa(self):\n        \"\"\"\n        Initialize weights for linear layers in the MHSA module.\n\n        Parameters:\n        None\n\n        Returns:\n        None\n        \"\"\"\n        \n        # Initialize weights for the q, k, v values.\n        for q_mapping, k_mapping, v_mapping in zip(self.q_mappings, self.k_mappings, self.v_mappings):\n            nn.init.xavier_uniform_(q_mapping.weight)\n            nn.init.xavier_uniform_(k_mapping.weight)\n            nn.init.xavier_uniform_(v_mapping.weight)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3: ViT BLOCK DEFINITION","metadata":{}},{"cell_type":"code","source":"class MyViTBlock(nn.Module):\n    def __init__(self, hidden_d, n_heads, mlp_ratio=10):\n        super(MyViTBlock, self).__init__()\n        \n        self.hidden_d = hidden_d\n        self.n_heads = n_heads\n\n        self.norm1 = nn.LayerNorm(hidden_d)\n        self.mhsa = MyMHSA(hidden_d, n_heads)\n        self.norm2 = nn.LayerNorm(hidden_d)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n            nn.GELU(),\n            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n        )\n        \n        # Initialize weights.\n        self.initialize_weights_block()\n\n    def forward(self, x):\n        \n        out = x + self.mhsa(self.norm1(x))\n        out = out + self.mlp(self.norm2(out))\n        return out\n    \n    def initialize_weights_block(self):\n        \n        # Initialize weights for linear layers in mlp.\n        for layer in self.mlp:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4: ViT MODEL DEFINITION","metadata":{}},{"cell_type":"markdown","source":"##### 3.4.1: TARGET DISTRIBUTIONAL PRIOR DEFINITION","metadata":{}},{"cell_type":"code","source":"\ndef get_distribution_target(length=4, max=1, target_depth=3, buffer=0.02):\n    \"\"\"\n    Generate the Target Distributional Prior.\n\n    Parameters:\n    - length (int): Length of the distribution.\n    - max (float): Maximum value of the distribution.\n    - target_depth (int): Depth of the target distribution.\n    - buffer (float): Buffer to control scaling factor.\n\n    Returns:\n    numpy.ndarray: Target distributional prior.\n    \"\"\"\n    \n    # Generate a series of values from 0 to length - 1.\n    data = np.arange(length)\n    \n    # Generate a Gausian Normal Distribution centered around target_depth.\n    data = norm.pdf(data, loc=target_depth, scale=1)\n    \n    # Scale the distribution to have a maximum value of 1.\n    scaling_factor = (1. - buffer) / sum(data[:target_depth])\n    data *= scaling_factor\n\n    return data\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.4.2: MYVIT CLASS DEFINITION","metadata":{}},{"cell_type":"code","source":"\nclass MyViT(nn.Module):\n        \n    def __init__(self, chw, n_patches, n_blocks, hidden_d, n_heads, out_d):\n        \"\"\"\n        Initialize the MyViT model.\n\n        Parameters:\n        - chw (tuple): Input shape (C, H, W).\n        - n_patches (int): Number of patches.\n        - n_blocks (int): Number of transformer blocks.\n        - hidden_d (int): Dimension of the hidden layer.\n        - n_heads (int): Number of attention heads.\n        - out_d (int): Output dimension.\n        \"\"\"\n        \n        # Super Constructor.\n        super(MyViT, self).__init__()\n        \n        # Attributes.\n        self.chw = chw # ( C , H , W )\n        self.n_patches = n_patches\n        self.n_blocks = n_blocks\n        self.n_heads = n_heads\n        self.hidden_d = hidden_d\n        self.mlp_ratio=100\n        \n        # Halting Prior Distribution Loss and Target Distribution.\n        self.ponder_loss = 0\n        self.distr_prior_loss = 0\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n        self.distr_target = torch.Tensor(get_distribution_target())\n        \n        # Input and Patches Sizes.\n        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n\n        # 1) Linear Mapper.\n        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n        \n        # 2) Learnable Classification Token.\n        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n        \n        # 3) Positional Embedding.\n        self.register_buffer('positional_embeddings', get_positional_embeddings_SPE(n_patches ** 2 + 1, hidden_d), persistent=False)\n        \n        # 4) Transformer Encoder Blocks.\n        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n        \n        # 5) Classification MLP.\n        self.mlp = nn.Sequential(\n            nn.Linear(self.hidden_d, self.mlp_ratio * self.hidden_d),\n            nn.GELU(),\n            nn.Linear(self.mlp_ratio * self.hidden_d, out_d),\n            nn.LogSoftmax(dim=-1)\n        )\n        \n        # Initialize weights.\n        self.initialize_weights()\n\n    def forward(self, images):\n        \"\"\"\n        Forward pass of the MyViT model.\n\n        Parameters:\n        - images (torch.Tensor): Input images tensor.\n\n        Returns:\n        torch.Tensor: Output tensor.\n        \"\"\"\n        \n        # Dividing Images into Patches.\n        n, c, h, w = images.shape\n        patches = Make_Patches_from_Image(images, self.n_patches).to(self.positional_embeddings.device)\n        \n        # Running Linear Layer Tokenization.\n        # Map the Vector corresponding to each patch to the Hidden Size Dimension.\n        tokens = self.linear_mapper(patches)\n        \n        # Adding Classification Token to the Tokens.\n        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n        \n        # Adding Positional Embedding.\n        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n        \n        ### Halting Procedure ###\n        total_token_count=len(out[1]) # out.shape = [512,65,32]\n        bs = out.size()[0]  # The batch size\n        c_token = torch.zeros(bs,total_token_count)\n        r = torch.ones(bs,total_token_count)\n        rho = torch.zeros(bs,total_token_count)\n        mask = torch.ones(bs,total_token_count)\n        \n        # Halting Hyperparameters.\n        gamma = 5\n        beta = -10\n        alpha_p = 5e-4\n        alpha_d = 0.1\n        eps = 0.01\n        output = None  # Final output of Adaptive Vision Transformer Block.\n        halting_score_layer=[] # List of Layer Halting Score Average.\n        \n        # Transformer Blocks.\n        for i,block in enumerate(self.blocks):\n            \n            # Previous Layers Token are masked.\n            out.data = out.data * mask.float().view(bs,total_token_count, 1)\n            \n            # Pass data trough each layer(block).\n            out = block(out.data) #out.shape = [512,65,32]\n            \n            # Compute Halting Scores.                 \n            t_0 = out[:,:,0] #out[:,:,0] = contains all the halting scores of images tokens ( t_O.shape = [512,65])\n            h_score = torch.sigmoid(gamma*t_0 + beta)\n            h=[-1,h_score]\n            _, h_token = h\n            \n            # Update list with mean of halting score just computed.\n            halting_score_layer.append(torch.mean(h[1][1:])) \n            \n            # Set all token halting score to one if we reached last layer(block).\n            if i == len(self.blocks)-1:\n                h_token = torch.ones(bs,total_token_count) \n                \n            # Last Layer Protection.\n            out = out * mask.float().view(bs,total_token_count, 1) #out.shape = [512,65,32]\n            \n            # Update Accumulator.\n            c_token = c_token + h_token #c_token.shape = [512,65]\n            \n            #update rho.\n            rho += mask.float() #rho.shape = [512,65]\n        \n            # Case 1: Threshold eached in this Iteration.\n            # token part\n            reached_token = c_token > 1 - eps #shape [512,65]\n            reached_token = reached_token.float() * mask.float()  #shape [512,65]\n            delta1 = out * r.view(bs, total_token_count, 1) * reached_token.view(bs, total_token_count, 1) # [512,65,32] * [512,65,1] * [512,65,1]\n            rho = rho + r * reached_token  #shape [512,65]\n\n            # Case 2: Threshold not reached.\n            # token part\n            not_reached_token = c_token < 1 - eps\n            not_reached_token = not_reached_token.float()\n            r = r - (not_reached_token.float() * h_token)\n            delta2 = out * h_token.view(bs, total_token_count, 1) * not_reached_token.view(bs, total_token_count, 1)\n            \n            # Update the mask.\n            mask = c_token < 1 - eps\n            \n            if output is None:\n                output = delta1 + delta2\n            else:\n                output = output + (delta1 + delta2)\n                \n        # Halting Prior Distribution.\n        halting_score_distr = torch.stack(halting_score_layer)\n        halting_score_distr = halting_score_distr / torch.sum(halting_score_distr)\n        halting_score_distr = torch.clamp(halting_score_distr, 0.01, 0.99) \n        \n        # Kullback-Leibler Divergence. \n        self.distr_prior_loss = alpha_d * self.kl_loss(halting_score_distr.log(), self.distr_target)\n        \n        # Ponder Loss.\n        self.ponder_loss = alpha_p * torch.mean(rho)\n        \n        # Getting the Classification Token only.\n        output = output[:, 0] #shape=[512,32]\n        \n        return self.mlp(output) # Map to output dimension (classification head).\n    \n\n    def initialize_weights(self):\n        \"\"\"\n        Initialize weights for linear layers, embeddings, etc.\n        \"\"\"\n        \n        # Initialize Weights for Linear Layers, Embeddings, etc.\n        nn.init.xavier_uniform_(self.linear_mapper.weight)\n        nn.init.normal_(self.class_token.data)\n\n        # Initialize Weights for Classification MLP.\n        nn.init.xavier_uniform_(self.mlp[0].weight)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.5: GENERAL AViT MODEL DEFINITION","metadata":{}},{"cell_type":"code","source":"\nclass AViT_Model(MyViT, pl.LightningModule):\n    \n    def __init__(self, input_d, n_patches, n_blocks, hidden_d, n_heads, out_d):\n        \"\"\"\n        Initialize the AViT_Model, a LightningModule using MyViT as a base.\n\n        Parameters:\n        - input_d (int): Dimension of the input.\n        - n_patches (int): Number of patches.\n        - n_blocks (int): Number of transformer blocks.\n        - hidden_d (int): Dimension of the hidden layer.\n        - n_heads (int): Number of attention heads.\n        - out_d (int): Output dimension.\n        \"\"\"\n        super(AViT_Model, self).__init__(input_d, n_patches, n_blocks, hidden_d, n_heads, out_d)\n\n        # Definition of the Cross Entropy Loss.\n        self.loss = CrossEntropyLoss()\n\n        # Definition of Accuracies, F1Score, Precision, and Recall Metrics.\n        self.acc_top1 = Accuracy(task=\"multiclass\", num_classes=out_d)\n        self.acc_top3 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=3)\n        self.acc_top5 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=5)\n        self.acc_top10 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=10)\n        self.f1score = MulticlassF1Score(num_classes=out_d, average='macro')\n        self.precision = MulticlassPrecision(num_classes=out_d, average='macro')\n        self.recall = MulticlassRecall(num_classes=out_d, average='macro')\n\n        # Definition of lists to be used in the \"on_ ... _epoch_end\" functions.\n        self.training_step_outputs = []\n        self.validation_step_outputs = []\n        self.test_step_outputs = []\n\n    def _step(self, batch):\n        \"\"\"\n        Common computation of the metrics among Training, Validation, and Test Set.\n\n        Parameters:\n        - batch (tuple): Input batch tuple.\n\n        Returns:\n        tuple: Tuple containing loss and various metrics.\n        \"\"\"\n        x, y = batch\n        preds = self(x)\n        loss = self.loss(preds, y) + self.ponder_loss + self.distr_prior_loss\n        acc1 = self.acc_top1(preds, y)\n        acc3 = self.acc_top3(preds, y)\n        acc5 = self.acc_top5(preds, y)\n        acc10 = self.acc_top10(preds, y)\n        f1score = self.f1score(preds, y)\n        precision = self.precision(preds, y)\n        recall = self.recall(preds, y)\n\n        return loss, acc1, acc3, acc5, acc10, f1score, precision, recall\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Training step function.\n\n        Parameters:\n        - batch (tuple): Input batch tuple.\n        - batch_idx (int): Batch index.\n\n        Returns:\n        torch.Tensor: Training loss.\n        \"\"\"\n        # Compute the Training Loss and Accuracy.\n        loss, acc, _, _, _, _, _, _ = self._step(batch)\n\n        # Create a Dictionary to represent the output of the Training step.\n        training_step_output = {\n            \"train_loss\": loss.item(),\n            \"train_acc\": acc.item()\n        }\n\n        # Append the dictionary to the list.\n        self.training_step_outputs.append(training_step_output)\n\n        # Perform logging.\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        Validation step function.\n\n        Parameters:\n        - batch (tuple): Input batch tuple.\n        - batch_idx (int): Batch index.\n\n        Returns:\n        None\n        \"\"\"\n        # Compute the Validation Loss and Accuracy.\n        loss, acc1, acc3, acc5, acc10, _, _, _ = self._step(batch)\n\n        # Create a Dictionary to represent the output of the validation step.\n        validation_step_output = {\n            \"val_loss\": loss.item(),\n            \"val_acc\": acc1.item(),\n            \"val_acc_3\": acc3.item(),\n            \"val_acc_5\": acc5.item(),\n            \"val_acc_10\": acc10.item(),\n        }\n\n        # Append the dictionary to the list.\n        self.validation_step_outputs.append(validation_step_output)\n\n        # Perform logging.\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc\", acc1, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc_3\", acc3, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc_5\", acc5, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc_10\", acc10, on_epoch=True, prog_bar=True, logger=True)\n\n    def on_validation_epoch_end(self):\n        \"\"\"\n        Method called at the end of the validation epoch.\n\n        Returns:\n        None\n        \"\"\"\n        # Calculate the Mean Loss and Accuracy from the list of dictionaries.\n        loss_tot = torch.tensor([item[\"val_loss\"] for item in self.validation_step_outputs]).mean()\n        acc_tot = torch.tensor([item[\"val_acc\"] for item in self.validation_step_outputs]).mean()\n        acc_tot_3 = torch.tensor([item[\"val_acc_3\"] for item in self.validation_step_outputs]).mean()\n        acc_tot_5 = torch.tensor([item[\"val_acc_5\"] for item in self.validation_step_outputs]).mean()\n        acc_tot_10 = torch.tensor([item[\"val_acc_10\"] for item in self.validation_step_outputs]).mean()\n\n        # Log the mean values.\n        self.log(\"val_loss\", loss_tot)\n        self.log(\"val_acc\", acc_tot)\n        self.log(\"val_acc_3\", acc_tot_3)\n        self.log(\"val_acc_5\", acc_tot_5)\n        self.log(\"val_acc_10\", acc_tot_10)\n\n        # Print messages.\n        message_loss = f'Epoch {self.current_epoch} Validation Loss -> {loss_tot}'\n        message_accuracy = f'      Validation Accuracy -> {acc_tot}'\n        message_accuracy_3 = f'      Validation Accuracy Top-3 -> {acc_tot_3}'\n        message_accuracy_5 = f'      Validation Accuracy Top-5-> {acc_tot_5}'\n        message_accuracy_10 = f'      Validation Accuracy Top-10-> {acc_tot_10}'\n        print(message_loss + message_accuracy + message_accuracy_3 + message_accuracy_5 + message_accuracy_10)\n\n        # Clear the list to free memory.\n        self.validation_step_outputs.clear()\n\n\n    def configure_optimizers(self):\n        \"\"\"\n        Configure the optimizer.\n\n        Returns:\n        torch.optim.Optimizer: The optimizer.\n        \"\"\"\n        # Configure the Adam Optimizer.\n        optimizer = optim.Adam(self.parameters(), lr=1.5e-3, weight_decay=1.5e-4)\n\n        # Configure the Cosine Annealing Learning Rate Scheduler.\n        # scheduler = CosineAnnealingLR(optimizer, T_max=5, eta_min=1.5e-6)\n\n        # return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n        return optimizer\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4: MODEL TRAINING","metadata":{}},{"cell_type":"markdown","source":"#### 4.1: CALLBACKS DEFINITION","metadata":{}},{"cell_type":"code","source":"# Checkpoint CallBack Definition.\nmy_checkpoint_call = ModelCheckpoint(\n    dirpath=\"/kaggle/working/checkpoints/\",\n    filename=\"Best_Model\",\n    monitor=\"val_acc\",\n    mode=\"max\",\n    save_top_k=1,\n    save_last=True\n)\n\n# Learning Rate CallBack Definition.\nmy_lr_monitor_call = LearningRateMonitor(logging_interval=\"epoch\")\n\n# Early Stopping CallBack Definition.\nmy_early_stopping_call = pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=30, mode=\"min\", min_delta=0.001)\n\n# Progress Bar CallBack Definition.\nmy_progress_bar_call = TQDMProgressBar(refresh_rate=10)\n\n# TensorBoardLogger CallBack Definition.\ntb_logger = TensorBoardLogger(save_dir=\"/kaggle/working/logs\", name=\"AViT\")\n\n# CSV CallBack Definition.\ncsv_logger = CSVLogger(\"/kaggle/working/logs\", name=\"AViT\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2: MODEL INSTANTIATION & TRAINING","metadata":{}},{"cell_type":"code","source":"\n# Instantiate the Adaptive Vision Transformer Model.\nmodel = AViT_Model((3, 64, 64), \n                   n_patches=8, \n                   n_blocks=4, \n                   hidden_d=32, \n                   n_heads=4, \n                   out_d = 200)\n\ndatamodule = AViT_DataModule(train_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", \n                             val_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/images/\", \n                             batch_size=512)\n\n# Setup the Dataloaders. \ndata_module.setup()\n\n# Create a PyTorch Lightning Trainer.\ntrainer = pl.Trainer(\n    max_epochs=10,\n    accelerator=\"auto\", \n    devices=\"auto\",\n    log_every_n_steps=1,\n    logger=tb_logger,\n    callbacks=[my_progress_bar_call,\n               my_checkpoint_call,\n               my_lr_monitor_call,\n               my_early_stopping_call,\n               ]\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrainer.fit(model, datamodule)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5: BEST MODEL EXTRAPOLATION","metadata":{}},{"cell_type":"code","source":"# Get the path of the best Model.\nbest_model_path = my_checkpoint_call.best_model_path\n\n# Load the best model from the Checkpoint.\nbest_model = AViT_Model.load_from_checkpoint(\n    checkpoint_path=best_model_path,\n    input_d=(3, 64, 64),\n    n_patches=8,\n    n_blocks=4,\n    hidden_d=32,\n    n_heads=4,\n    out_d=200\n)\n\n# Access the Best Model's Accuracy.\nbest_model_accuracy = trainer.checkpoint_callback.best_model_score.item()\nprint(f\"Best Model Accuracy: {best_model_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6: SAVING THE BEST MODEL","metadata":{}},{"cell_type":"code","source":"# Save it as a pth file.\n# Specify the path where you want to save the model.\nmodel_path = f\"/kaggle/working/best_model_acc_{best_model_accuracy:.5f}.pth\"\n\n# Save the model's state dict to the specified file.\ntorch.save(best_model.state_dict(), model_path)\n\n# Save it as a CheckPoint (Specific of PyTorch Lightning = Model State Dictionary + Training State + Optimizer State).\n# Specify the path where you want to save the model checkpoint.\nckpt_path = f\"/kaggle/working/best_model_acc_{best_model_accuracy:.5f}.ckpt\"\n\n# Save the model's state dict to the specified file.\ntorch.save(best_model.state_dict(), ckpt_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7: TRAINING FROM A SAVED CHECKPOINT","metadata":{}},{"cell_type":"markdown","source":"# Load the Best Model from the Checkpoint.\ncheckpoint = torch.load(\"/kaggle/input/adavit-model-checkpoints/best_model_acc.pth\")\n\n# 'Checkpoint' is an OrderedDict or an odict_keys Object.\ncheckpoint_keys = list(checkpoint.keys()) if isinstance(checkpoint, OrderedDict) else checkpoint.keys()\n\n# Convert checkpoint_keys to a Dictionary.\nstate_dict = {key: checkpoint[key] for key in checkpoint_keys}\n\n# Instantiate the Loaded Model (same schema as the checkpoint).\nloaded_model = AViT_Model((3, 64, 64), \n                   n_patches=8, \n                   n_blocks=4, \n                   hidden_d=32, \n                   n_heads=4, \n                   out_d = 200)\n\n# Now, load the state_dict into the Model. \nloaded_model.load_state_dict(state_dict)","metadata":{}},{"cell_type":"markdown","source":"\n# Resume the Trainer from the last Checkpoint.\nresume_trainer = pl.Trainer(\n    max_epochs=1,\n    accelerator=\"auto\", \n    devices=\"auto\",\n    log_every_n_steps=1,\n    logger=tb_logger,\n    callbacks=[my_progress_bar_call,\n               my_checkpoint_call,\n               my_lr_monitor_call,\n               my_early_stopping_call,\n               ]\n)\n\n# Train the Model.\nresume_trainer.fit(new_model, datamodule)","metadata":{}}]}