Model Hypeparameters: input_size=(3, 224, 224),n_patches=14,n_blocks=12,n_heads=3,hidden_d=192,output_size=10,batch_size=128,learning_rate=0.0015,weight_decay=0.00015,temp=0.5,class_mlp_ratio=4,vit_mlp_ratio=4
Training Loss,Training Accuracy,Validation Loss,Validation Top-1 Accuracy,Validation Top-3 Accuracy,Validation Top-5 Accuracy,Validation Top-10 Accuracy,Percentage halted tokens layer_0,Percentage halted tokens layer_1,Percentage halted tokens layer_2,Percentage halted tokens layer_3,Percentage halted tokens layer_4,Percentage halted tokens layer_5,Percentage halted tokens layer_6,Percentage halted tokens layer_7,Percentage halted tokens layer_8,Percentage halted tokens layer_9,Percentage halted tokens layer_10,Percentage halted tokens layer_11,Variance halted tokens layer_0,Variance halted tokens layer_1,Variance halted tokens layer_2,Variance halted tokens layer_3,Variance halted tokens layer_4,Variance halted tokens layer_5,Variance halted tokens layer_6,Variance halted tokens layer_7,Variance halted tokens layer_8,Variance halted tokens layer_9,Variance halted tokens layer_10,Variance halted tokens layer_11
