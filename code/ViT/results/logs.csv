Model Hypeparameters: input_size=(3, 224, 224),n_patches=14,n_blocks=12,n_heads=3,hidden_d=192,output_size=10,batch_size=64,learning_rate=0.0015,weight_decay=0.00015,temp=0.5,class_mlp_ratio=4,vit_mlp_ratio=4
Training Loss,Training Accuracy,Validation Loss,Validation Top-1 Accuracy,Validation Top-3 Accuracy,Validation Top-5 Accuracy,Validation Top-10 Accuracy,Percentage halted tokens layer_0,Percentage halted tokens layer_1,Percentage halted tokens layer_2,Percentage halted tokens layer_3,Percentage halted tokens layer_4,Percentage halted tokens layer_5,Percentage halted tokens layer_6,Percentage halted tokens layer_7,Percentage halted tokens layer_8,Percentage halted tokens layer_9,Percentage halted tokens layer_10,Percentage halted tokens layer_11,Variance halted tokens layer_0,Variance halted tokens layer_1,Variance halted tokens layer_2,Variance halted tokens layer_3,Variance halted tokens layer_4,Variance halted tokens layer_5,Variance halted tokens layer_6,Variance halted tokens layer_7,Variance halted tokens layer_8,Variance halted tokens layer_9,Variance halted tokens layer_10,Variance halted tokens layer_11
4.960090160369873,0.2489669770002365,4.097795009613037,0.319260835647583,0.6412259340286255,0.8120492696762085,1.0
3.9267807006835938,0.38652288913726807,3.404973268508911,0.44411057233810425,0.7477464079856873,0.905348539352417,1.0
