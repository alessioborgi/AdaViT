{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (AdaViT) ADAPTIVE VISION TRANSFORMERS\n",
    "\n",
    "### SAPIENZA UNIVESITY of ROME (DEPARTMENT OF COMPUTER, CONTROL AND MANAGEMENT ENGINEEERING)\n",
    "\n",
    "### COURSE IN ARTIFICIAL INTELLIGENCE AND ROBOTICS (ACADEMIC YEAR 2023/2024)\n",
    "\n",
    "### AUTHOR:  **ALESSIO BORGI**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to the ZIP file\n",
    "zip_file = \"imagenette-160-px.zip\"\n",
    "\n",
    "# Directory where you want to extract the contents of the ZIP file\n",
    "extract_dir = \"imagenette-160-px\"\n",
    "\n",
    "# Create a directory if it doesn't exist\n",
    "if not os.path.exists(extract_dir):\n",
    "    os.makedirs(extract_dir)\n",
    "\n",
    "# Open the ZIP file\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    # Extract all the contents of the ZIP file\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Path to the compressed TAR file\n",
    "tar_file = \"./imagenette-160-px/imagenette-160.tgz\"\n",
    "\n",
    "# Directory where you want to extract the contents of the TAR file\n",
    "extract_dir = \"imagenette-160-px\"\n",
    "\n",
    "# Create a directory if it doesn't exist\n",
    "if not os.path.exists(extract_dir):\n",
    "    os.makedirs(extract_dir)\n",
    "\n",
    "# Open the TAR file\n",
    "with tarfile.open(tar_file, 'r:gz') as tar_ref:\n",
    "    # Extract all the contents of the TAR file\n",
    "    tar_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"Extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0: IMPORTING LIBRARIES \\& SETTINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing PyTorch-related Libraries.\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts\n",
    "from torchmetrics.classification import Accuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "\n",
    "# Importing PyTorch Lightning-Related Libraries.\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Importing General Libraries.\n",
    "import os\n",
    "import csv\n",
    "import PIL\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Importing from Other Files.\n",
    "from Train_Val_Test_Classes import CustomTrainingTinyImagenet, CustomValidationTinyImagenet, CustomTestTinyImagenet\n",
    "from Transformations import ViT_Transformations\n",
    "from Multi_Head_Self_Attention import MHSA\n",
    "from Positional_Embeddings import get_positional_embeddings_RoPE, get_positional_embeddings_SPE, get_positional_embeddings_BERT\n",
    "from Gumbel_Noise import add_gumbel_noise\n",
    "from utils import set_device, seed_everything\n",
    "from Patchifying import make_patches_from_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: DATAMODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViT_DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir, batch_size, num_workers=4):\n",
    "        \"\"\"\n",
    "        Custom data module for ViT model training and validation with Imagenette-160px dataset.\n",
    "\n",
    "        Args:\n",
    "        - data_dir (str): Directory path where the dataset is stored or will be downloaded.\n",
    "        - batch_size (int): Batch size for training and validation DataLoader.\n",
    "        - num_workers (int, optional): Number of workers for DataLoader (default is 4).\n",
    "        \"\"\"\n",
    "        super(ViT_DataModule, self).__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_test = 5\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Define transformations\n",
    "        self.transform = ViT_Transformations()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Method to load and configure datasets for Training, Validation, and Test.\n",
    "\n",
    "        Args:\n",
    "        - stage (str, optional): 'fit' for Training and 'test' for Validation (default is None).\n",
    "        \"\"\"\n",
    "        self.train_dataset = ImageFolder(root=self.data_dir + '/train', transform=self.transform)\n",
    "        self.val_dataset = ImageFolder(root=self.data_dir + '/val', transform=self.transform)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Method to return the DataLoader for the Training Dataset.\n",
    "\n",
    "        Returns:\n",
    "        - train_dataloader (DataLoader): DataLoader for Training.\n",
    "        \"\"\"\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Method to return the DataLoader for the Validation Dataset.\n",
    "\n",
    "        Returns:\n",
    "        - val_dataloader (DataLoader): DataLoader for Validation.\n",
    "        \"\"\"\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Define batch size and number of workers\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "data_dir = \"./dataset\"\n",
    "# Create an instance of the ViT_DataModule class\n",
    "data_module = ViT_DataModule(data_dir, batch_size, num_workers)\n",
    "\n",
    "# Call the setup method to load and configure datasets\n",
    "data_module.setup()\n",
    "\n",
    "def show_images(data_loader, num_images=25, title=None):\n",
    "    \"\"\"\n",
    "    Function to visualize images from a DataLoader.\n",
    "\n",
    "    Args:\n",
    "    - data_loader (DataLoader): DataLoader containing the images.\n",
    "    - num_images (int): Number of images to visualize.\n",
    "    - title (str): Title for the plot (default is None).\n",
    "    \"\"\"\n",
    "    # Fetch a batch of images from the DataLoader\n",
    "    data_iter = iter(data_loader)\n",
    "    images, _ = next(data_iter)\n",
    "    \n",
    "    # Restrict the number of images to visualize\n",
    "    images = images[:num_images]\n",
    "\n",
    "    # Create a grid of images\n",
    "    image_grid = make_grid(images, nrow=int(np.sqrt(num_images)), padding=5, normalize=True)\n",
    "\n",
    "    # Convert tensor to numpy array\n",
    "    image_grid = np.transpose(image_grid.numpy(), (1, 2, 0))\n",
    "\n",
    "    # Display the grid of images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image_grid)\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#print(\"Size of training images:\", train_image_size)\n",
    "#print(\"Size of validation images:\", val_image_size)\n",
    "# Example usage:\n",
    "# Assuming you have created a ViT_DataModule instance called 'data_module'\n",
    "# Show training images\n",
    "show_images(data_module.train_dataloader(), title=\"Training Images\")\n",
    "# Show validation images\n",
    "show_images(data_module.val_dataloader(), title=\"Validation Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: VIT-BLOCK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_Block_Layer_Norm(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=10):\n",
    "        super(ViT_Block_Layer_Norm, self).__init__()\n",
    "        \n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MHSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
    "        )\n",
    "        \n",
    "        # Initialize weights.\n",
    "        self.initialize_weights_block()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out\n",
    "    \n",
    "    def initialize_weights_block(self):\n",
    "        \n",
    "        # Initialize weights for linear layers in mlp.\n",
    "        for layer in self.mlp:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: VIT DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vision_Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, chw, batch_size, n_patches, n_blocks, hidden_d, n_heads, out_d, lr, nepochs, warmup_epochs, wd, temp, cmlp_ratio, vmlp_ratio):\n",
    "        \"\"\"\n",
    "        Initialize the Vision_Transformer model.\n",
    "\n",
    "        Parameters:\n",
    "        - input_d (int): Dimension of the input.\n",
    "        - n_patches (int): Number of patches.\n",
    "        - n_blocks (int): Number of transformer blocks.\n",
    "        - hidden_d (int): Dimension of the hidden layer.\n",
    "        - n_heads (int): Number of attention heads.\n",
    "        - out_d (int): Output dimension.\n",
    "        - lr (float): Learning rate.\n",
    "        - wd (float): Weight decay.\n",
    "        - temp (float): Temperature scaling.\n",
    "        - cmlp_ratio (int): Ratio for the classification MLP.\n",
    "        - vmlp_ratio (int): Ratio for the ViT MLP.\n",
    "        \"\"\"\n",
    "\n",
    "        # Super Constructor.\n",
    "        super(Vision_Transformer, self).__init__()\n",
    "\n",
    "        # Model Attributes.\n",
    "        self.chw = chw # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "        self.class_mlp_ratio = cmlp_ratio\n",
    "        self.vit_mlp_ratio = vmlp_ratio\n",
    "\n",
    "        print(\"the channel shape would be\", chw)\n",
    "        # Input and Patches Sizes.\n",
    "        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "        \n",
    "\n",
    "        # 1) Linear Mapper.\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable Classification Token.\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional Embedding.\n",
    "        #self.register_buffer('positional_embeddings', get_positional_embeddings_SPE(n_patches ** 2 + 1, self.hidden_d), persistent=False)\n",
    "        self.register_buffer('positional_embeddings', get_positional_embeddings_RoPE(n_patches ** 2 + 1, self.hidden_d), persistent=False)\n",
    "\n",
    "        # 4) Transformer Encoder Blocks.\n",
    "        # Select one of the following blocks.\n",
    "        # 4.1) Transformer Block with Layer Normalization.\n",
    "        self.blocks = nn.ModuleList([ViT_Block_Layer_Norm(self.hidden_d, self.n_heads,self.vit_mlp_ratio) for _ in range(self.n_blocks)])\n",
    "\n",
    "        # 5) Classification MLP.\n",
    "        self.mlp = nn.Sequential(\n",
    "          nn.Linear(self.hidden_d, class_mlp_ratio * hidden_d),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(class_mlp_ratio * hidden_d, out_d),\n",
    "        )\n",
    "\n",
    "        # Initialize weights.\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass of the Vision_Transformer model.\n",
    "\n",
    "        Parameters:\n",
    "        - images (torch.Tensor): Input images tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Dividing Images into Patches.\n",
    "        n, c, h, w = images.shape\n",
    "        patches = make_patches_from_image(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "\n",
    "        # Running Linear Layer Tokenization.\n",
    "        # Map the Vector corresponding to each patch to the Hidden Size Dimension.\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding Classification Token to the Tokens.\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1).to(\"cuda\")\n",
    "\n",
    "        # Adding Positional Embedding.\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1).to(\"cuda\")\n",
    "\n",
    "        # Transformer Blocks.\n",
    "        for block in self.blocks:\n",
    "            \n",
    "            out = block(out)\n",
    "            \n",
    "        out = out[:, 0]\n",
    "        \n",
    "        return self.mlp(out)\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for linear layers, embeddings, etc.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize Weights for Linear Layers, Embeddings, etc.\n",
    "        nn.init.xavier_uniform_(self.linear_mapper.weight)\n",
    "        nn.init.normal_(self.class_token.data)\n",
    "\n",
    "        # Initialize Weights for Classification MLP.\n",
    "        nn.init.xavier_uniform_(self.mlp[0].weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViT(Vision_Transformer, pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_d, batch_size, n_patches, n_blocks, hidden_d, n_heads, out_d, lr, nepochs, warmup_epochs, wd, temp, cmlp_ratio, vmlp_ratio):\n",
    "        \"\"\"\n",
    "        Initialize the ViT, a LightningModule using Vision_Transformer as a base.\n",
    "\n",
    "        Parameters:\n",
    "        - input_d (int): Dimension of the input.\n",
    "        - n_patches (int): Number of patches.\n",
    "        - n_blocks (int): Number of transformer blocks.\n",
    "        - hidden_d (int): Dimension of the hidden layer.\n",
    "        - n_heads (int): Number of attention heads.\n",
    "        - out_d (int): Output dimension.\n",
    "        - lr (float): Learning rate.\n",
    "        - wd (float): Weight decay.\n",
    "        - temp (float): Temperature scaling.\n",
    "        - cmlp_ratio (int): Ratio for the classification MLP.\n",
    "        - vmlp_ratio (int): Ratio for the ViT MLP.\n",
    "        \"\"\"\n",
    "        super(ViT, self).__init__(input_d, batch_size, n_patches, n_blocks, hidden_d, n_heads, out_d, lr, nepochs, warmup_epochs, wd, temp, cmlp_ratio, vmlp_ratio)\n",
    "        # Optimizer hyperparams.\n",
    "        self.learning_rate = lr\n",
    "        self.nepochs = nepochs\n",
    "        self.weight_decay = wd\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Definition of the Cross Entropy Loss.\n",
    "        self.loss = CrossEntropyLoss()\n",
    "        self.temperature = temp\n",
    "\n",
    "        # Definition of Accuracies, F1Score, Precision, and Recall Metrics.\n",
    "        self.acc_top1 = Accuracy(task=\"multiclass\", num_classes=out_d)\n",
    "        self.acc_top3 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=3)\n",
    "        self.acc_top5 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=5)\n",
    "        self.acc_top10 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=10)\n",
    "        self.f1score = MulticlassF1Score(num_classes=out_d, average='macro')\n",
    "        self.precision = MulticlassPrecision(num_classes=out_d, average='macro')\n",
    "        self.recall = MulticlassRecall(num_classes=out_d, average='macro')\n",
    "\n",
    "        # Definition of lists to be used in the \"on_ ... _epoch_end\" functions.\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "        #list of train and validation per epochs performance\n",
    "        self.train_loss=[]\n",
    "        self.train_acc=[]\n",
    "        self.val_loss=[]\n",
    "        self.val_acc1=[]\n",
    "        self.val_acc3=[]\n",
    "        self.val_acc5=[]\n",
    "        self.val_acc10=[]\n",
    "        self.perc=[]\n",
    "        self.var=[]\n",
    "\n",
    "    def _step(self, batch):\n",
    "        \"\"\"\n",
    "        Common computation of the metrics among Training, Validation, and Test Set.\n",
    "\n",
    "        Parameters:\n",
    "        - batch (tuple): Input batch tuple.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Tuple containing loss and various metrics.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        # Compute the Logits.\n",
    "        preds = self(x)\n",
    "        # Scale the logits using a Temperature Scaling and add Gumbel Noise, s.t. you obtain Gumbel Softmax then.\n",
    "        preds_scaled = add_gumbel_noise(preds) / self.temperature\n",
    "        loss = self.loss(preds_scaled, y) \n",
    "        acc1 = self.acc_top1(preds, y)\n",
    "        acc3 = self.acc_top3(preds, y)\n",
    "        acc5 = self.acc_top5(preds, y)\n",
    "        acc10 = self.acc_top10(preds, y)\n",
    "        f1score = self.f1score(preds, y)\n",
    "        precision = self.precision(preds, y)\n",
    "        recall = self.recall(preds, y)\n",
    "\n",
    "        return loss, acc1, acc3, acc5, acc10, f1score, precision, recall\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step function.\n",
    "\n",
    "        Parameters:\n",
    "        - batch (tuple): Input batch tuple.\n",
    "        - batch_idx (int): Batch index.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Training loss.\n",
    "        \"\"\"\n",
    "        # Compute the Training Loss and Accuracy.\n",
    "        loss, acc, _, _, _, _, _, _ = self._step(batch)\n",
    "\n",
    "        # Create a Dictionary to represent the output of the Training step.\n",
    "        training_step_output = {\n",
    "            \"train_loss\": loss.item(),\n",
    "            \"train_acc\": acc.item()\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list.\n",
    "        self.training_step_outputs.append(training_step_output)\n",
    "\n",
    "        # Perform logging.\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Method called at the end of the training epoch.\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Calculate the Mean Loss and Accuracy from the list of dictionaries.\n",
    "        loss_tot = torch.tensor([item[\"train_loss\"] for item in self.training_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot = torch.tensor([item[\"train_acc\"] for item in self.training_step_outputs]).mean().to(\"cuda\")\n",
    "\n",
    "        # Log the mean values.\n",
    "        self.log(\"train_loss\", loss_tot)\n",
    "        self.log(\"train_acc\", acc_tot)\n",
    "\n",
    "        # Print messages.\n",
    "        message_loss = f'Epoch {self.current_epoch} Training Loss -> {loss_tot}'\n",
    "        message_accuracy = f'      Training Accuracy -> {acc_tot}'\n",
    "        print(message_loss + message_accuracy)\n",
    "\n",
    "        # Clear the list to free memory.\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "        #updating training performance lists\n",
    "        self.train_loss.append(loss_tot.item())\n",
    "        self.train_acc.append(acc_tot.item())\n",
    "\n",
    "\n",
    "        #updating csv logs file\n",
    "        new_row=str(self.train_loss[-1])+\",\"+str(self.train_acc[-1])+\",\"+str(self.val_loss[-1])+\",\"+str(self.val_acc1[-1])+\",\"+str(self.val_acc3[-1])+\",\"+str(self.val_acc5[-1])+\",\"+str(self.val_acc10[-1])\n",
    "        with open(\"./results/logs.csv\",'a',newline='') as file:\n",
    "            #writing row\n",
    "            file.write(new_row+\"\\n\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step function.\n",
    "\n",
    "        Parameters:\n",
    "        - batch (tuple): Input batch tuple.\n",
    "        - batch_idx (int): Batch index.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Compute the Validation Loss and Accuracy.\n",
    "        loss, acc1, acc3, acc5, acc10, _, _, _ = self._step(batch)\n",
    "\n",
    "        # Create a Dictionary to represent the output of the validation step.\n",
    "        validation_step_output = {\n",
    "            \"val_loss\": loss.item(),\n",
    "            \"val_acc\": acc1.item(),\n",
    "            \"val_acc_3\": acc3.item(),\n",
    "            \"val_acc_5\": acc5.item(),\n",
    "            \"val_acc_10\": acc10.item(),\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list.\n",
    "        self.validation_step_outputs.append(validation_step_output)\n",
    "\n",
    "        # Perform logging.\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", acc1, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_3\", acc3, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_5\", acc5, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_10\", acc10, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Method called at the end of the validation epoch.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Calculate the Mean Loss and Accuracy from the list of dictionaries.\n",
    "        loss_tot = torch.tensor([item[\"val_loss\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot = torch.tensor([item[\"val_acc\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot_3 = torch.tensor([item[\"val_acc_3\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot_5 = torch.tensor([item[\"val_acc_5\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot_10 = torch.tensor([item[\"val_acc_10\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "\n",
    "        # Log the mean values.\n",
    "        self.log(\"val_loss\", loss_tot)\n",
    "        self.log(\"val_acc\", acc_tot)\n",
    "        self.log(\"val_acc_3\", acc_tot_3)\n",
    "        self.log(\"val_acc_5\", acc_tot_5)\n",
    "        self.log(\"val_acc_10\", acc_tot_10)\n",
    "\n",
    "        # Print messages.\n",
    "        message_loss = f'Epoch {self.current_epoch} Validation Loss -> {loss_tot}'\n",
    "        message_accuracy = f'      Validation Accuracy -> {acc_tot}'\n",
    "        message_accuracy_3 = f'      Validation Accuracy Top-3 -> {acc_tot_3}'\n",
    "        message_accuracy_5 = f'      Validation Accuracy Top-5-> {acc_tot_5}'\n",
    "        message_accuracy_10 = f'      Validation Accuracy Top-10-> {acc_tot_10}'\n",
    "        print(message_loss + message_accuracy + message_accuracy_3 + message_accuracy_5 + message_accuracy_10)\n",
    "\n",
    "        # Clear the list to free memory.\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "        #updating validation performance lists\n",
    "        self.val_loss.append(loss_tot.item())\n",
    "        self.val_acc1.append(acc_tot.item())\n",
    "        self.val_acc3.append(acc_tot_3.item())\n",
    "        self.val_acc5.append(acc_tot_5.item())\n",
    "        self.val_acc10.append(acc_tot_10.item())\n",
    "\n",
    "    def predict(self, input_image):\n",
    "        \"\"\"\n",
    "        Method called at Inference Time.\n",
    "\n",
    "        Returns:\n",
    "        predicted_labels: prediction over the labels.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = self(input_image)\n",
    "\n",
    "            # Scale the logits using a Temperature Scaling.\n",
    "            preds_scaled = F.log_softmax(preds / self.temperature, dim=1)\n",
    "\n",
    "            # Get the predicted class labels.\n",
    "            predicted_labels = torch.argmax(preds_scaled, dim=1).cpu().numpy()\n",
    "\n",
    "            return predicted_labels\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure the optimizer.\n",
    "\n",
    "        Returns:\n",
    "        torch.optim.Optimizer: The optimizer.\n",
    "        \"\"\"\n",
    "        # Configure the Adam Optimizer.\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001)\n",
    "        #optimizer, scheduler = get_optimizer_scheduler(\n",
    "        #                                               model,\n",
    "        #                                               lr=self.learning_rate,\n",
    "        #                                               warmup_epochs=self.warmup_epochs,\n",
    "        #                                               num_epochs=self.nepochs,\n",
    "        #                                               weight_decay=self.weight_decay,\n",
    "        #                                               batch_size=self.batch_size\n",
    "        #                                               )\n",
    "\n",
    "        #return optimizer\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: CALLBACK DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint CallBack Definition.\n",
    "my_checkpoint_call = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints/\",\n",
    "    filename=\"Best_Model\",\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "# Learning Rate CallBack Definition.\n",
    "my_lr_monitor_call = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "# Early Stopping CallBack Definition.\n",
    "my_early_stopping_call = pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, mode=\"min\", min_delta=0.001)\n",
    "\n",
    "# Progress Bar CallBack Definition.\n",
    "my_progress_bar_call = TQDMProgressBar(refresh_rate=10)\n",
    "\n",
    "# TensorBoardLogger CallBack Definition.\n",
    "#tb_logger = TensorBoardLogger(save_dir=\"./results/logs/\", name=\"ViT\")\n",
    "\n",
    "# CSV CallBack Definition.\n",
    "csv_logger = CSVLogger(\"./results/logs/\", name=\"ViT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: MODEL HYPERPAMETERS \\& MODEL INSTANTIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the channel shape would be (3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the Adaptive Vision Transformer Model.\n",
    "# Model Hyperparameters-\n",
    "blocks=12\n",
    "heads=3\n",
    "classes=10\n",
    "hidden_dim=192\n",
    "batch=64\n",
    "\n",
    "learning_rate=1.5e-3\n",
    "weight_decay=1.5e-4\n",
    "temperature=0.5\n",
    "class_mlp_ratio=4\n",
    "vit_mlp_ratio=4\n",
    "input_size=(3, 224, 224)\n",
    "patches=14\n",
    "\n",
    "number_epochs = 200\n",
    "warmup_epochs = 5\n",
    "\n",
    "model = ViT(input_d = input_size,\n",
    "            batch_size=batch,\n",
    "            n_patches = patches,\n",
    "            n_blocks = blocks,\n",
    "            hidden_d = hidden_dim,\n",
    "            n_heads = heads,\n",
    "            out_d = classes,\n",
    "            lr = learning_rate,\n",
    "            nepochs = number_epochs,\n",
    "            warmup_epochs = warmup_epochs,\n",
    "            wd = weight_decay,\n",
    "            temp = temperature,\n",
    "            cmlp_ratio = class_mlp_ratio,\n",
    "            vmlp_ratio = vit_mlp_ratio\n",
    "        )\n",
    "\n",
    "data_module = ViT_DataModule(\n",
    "    data_dir=\"./dataset\",\n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "# Setup the Dataloaders.\n",
    "data_module.setup()\n",
    "\n",
    "# Create a PyTorch Lightning Trainer.\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=number_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else 0,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[my_progress_bar_call,\n",
    "               my_checkpoint_call,\n",
    "               my_lr_monitor_call,\n",
    "               my_early_stopping_call,\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"./results/logs.csv\", 'w') as file:\n",
    "    #writing header\n",
    "    perf_header=\"Training Loss\"+\",\"+\"Training Accuracy\"+\",\"+\"Validation Loss\"+\",\"+\"Validation Top-1 Accuracy\"+\",\"+\"Validation Top-3 Accuracy\"+\",\"+\"Validation Top-5 Accuracy\"+\",\"+\"Validation Top-10 Accuracy\"\n",
    "    halt_header=''\n",
    "    for i in range(2*blocks):\n",
    "        if i<blocks:\n",
    "            string=f\"Percentage halted tokens layer_{i}\"\n",
    "        else:\n",
    "            string=f\"Variance halted tokens layer_{i-blocks}\"\n",
    "        halt_header+=\",\"+string\n",
    "    file.write(f\"Model Hypeparameters: input_size={input_size},n_patches={patches},n_blocks={blocks},n_heads={heads},hidden_d={hidden_dim},output_size={classes},batch_size={batch},learning_rate={learning_rate},weight_decay={weight_decay},temp={temperature},class_mlp_ratio={class_mlp_ratio},vit_mlp_ratio={vit_mlp_ratio}\\n\")\n",
    "    file.write(perf_header+halt_header+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /home/studio-lab-user/sagemaker-studiolab-notebooks/AdaViT/code/ViT/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name          | Type                | Params\n",
      "-------------------------------------------------------\n",
      "0  | linear_mapper | Linear              | 147 K \n",
      "1  | blocks        | ModuleList          | 4.0 M \n",
      "2  | mlp           | Sequential          | 155 K \n",
      "3  | loss          | CrossEntropyLoss    | 0     \n",
      "4  | acc_top1      | MulticlassAccuracy  | 0     \n",
      "5  | acc_top3      | MulticlassAccuracy  | 0     \n",
      "6  | acc_top5      | MulticlassAccuracy  | 0     \n",
      "7  | acc_top10     | MulticlassAccuracy  | 0     \n",
      "8  | f1score       | MulticlassF1Score   | 0     \n",
      "9  | precision     | MulticlassPrecision | 0     \n",
      "10 | recall        | MulticlassRecall    | 0     \n",
      "   | other params  | n/a                 | 192   \n",
      "-------------------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.251    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Loss -> 4.631761074066162      Validation Accuracy -> 0.25      Validation Accuracy Top-3 -> 0.546875      Validation Accuracy Top-5-> 0.703125      Validation Accuracy Top-10-> 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52b77924a0e4e42a784a1cfebeb5520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Loss -> 4.097795009613037      Validation Accuracy -> 0.319260835647583      Validation Accuracy Top-3 -> 0.6412259340286255      Validation Accuracy Top-5-> 0.8120492696762085      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 0 Training Loss -> 4.960090160369873      Training Accuracy -> 0.2489669770002365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss -> 3.404973268508911      Validation Accuracy -> 0.44411057233810425      Validation Accuracy Top-3 -> 0.7477464079856873      Validation Accuracy Top-5-> 0.905348539352417      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 1 Training Loss -> 3.9267807006835938      Training Accuracy -> 0.38652288913726807\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss -> 3.3463385105133057      Validation Accuracy -> 0.481370210647583      Validation Accuracy Top-3 -> 0.801832914352417      Validation Accuracy Top-5-> 0.90234375      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 2 Training Loss -> 3.593829393386841      Training Accuracy -> 0.4391019344329834\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss -> 3.041835308074951      Validation Accuracy -> 0.5157752633094788      Validation Accuracy Top-3 -> 0.803786039352417      Validation Accuracy Top-5-> 0.917067289352417      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 3 Training Loss -> 3.3336968421936035      Training Accuracy -> 0.480577290058136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Loss -> 3.13761305809021      Validation Accuracy -> 0.5285456776618958      Validation Accuracy Top-3 -> 0.819411039352417      Validation Accuracy Top-5-> 0.9214242696762085      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 4 Training Loss -> 3.1793367862701416      Training Accuracy -> 0.5083814263343811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Loss -> 3.033083200454712      Validation Accuracy -> 0.5202824473381042      Validation Accuracy Top-3 -> 0.8257211446762085      Validation Accuracy Top-5-> 0.9253305196762085      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 5 Training Loss -> 3.060930013656616      Training Accuracy -> 0.5317889451980591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Loss -> 2.8865439891815186      Validation Accuracy -> 0.5491285920143127      Validation Accuracy Top-3 -> 0.8248196840286255      Validation Accuracy Top-5-> 0.920973539352417      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 6 Training Loss -> 2.864349126815796      Training Accuracy -> 0.547501266002655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Loss -> 2.559784173965454      Validation Accuracy -> 0.5872896909713745      Validation Accuracy Top-3 -> 0.8599759340286255      Validation Accuracy Top-5-> 0.9311898946762085      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 7 Training Loss -> 2.726029396057129      Training Accuracy -> 0.5748637914657593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Loss -> 2.7879340648651123      Validation Accuracy -> 0.5833834409713745      Validation Accuracy Top-3 -> 0.8439002633094788      Validation Accuracy Top-5-> 0.920973539352417      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 8 Training Loss -> 2.667369842529297      Training Accuracy -> 0.5850117802619934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Loss -> 2.5308773517608643      Validation Accuracy -> 0.6185396909713745      Validation Accuracy Top-3 -> 0.8502103090286255      Validation Accuracy Top-5-> 0.9453125      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 9 Training Loss -> 2.5796496868133545      Training Accuracy -> 0.6043596267700195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Loss -> 2.9276158809661865      Validation Accuracy -> 0.5680589079856873      Validation Accuracy Top-3 -> 0.8605769276618958      Validation Accuracy Top-5-> 0.9322415590286255      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 10 Training Loss -> 2.7471864223480225      Training Accuracy -> 0.5775186419487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Loss -> 2.5068936347961426      Validation Accuracy -> 0.6012620329856873      Validation Accuracy Top-3 -> 0.833082914352417      Validation Accuracy Top-5-> 0.9468148946762085      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 11 Training Loss -> 2.743626356124878      Training Accuracy -> 0.5860273241996765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Loss -> 2.5155816078186035      Validation Accuracy -> 0.6478365659713745      Validation Accuracy Top-3 -> 0.879957914352417      Validation Accuracy Top-5-> 0.943359375      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 12 Training Loss -> 2.64731502532959      Training Accuracy -> 0.5984858870506287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Loss -> 2.4325289726257324      Validation Accuracy -> 0.6424278616905212      Validation Accuracy Top-3 -> 0.8731971383094788      Validation Accuracy Top-5-> 0.9487680196762085      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 13 Training Loss -> 2.4738917350769043      Training Accuracy -> 0.6183078289031982\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Loss -> 2.5932750701904297      Validation Accuracy -> 0.6350660920143127      Validation Accuracy Top-3 -> 0.8595252633094788      Validation Accuracy Top-5-> 0.9390023946762085      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 14 Training Loss -> 2.5269203186035156      Training Accuracy -> 0.6193558573722839\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.fit(model.to(\"cuda\"), data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"./results/\"\n",
    "# Plotting Performace metrics\n",
    "def plotter(data, xlabel, ylabel, title, is_halting):\n",
    "    if is_halting:\n",
    "        plt.style.use('bmh')\n",
    "    plt.xticks([i for i in range(len(data))])\n",
    "    plt.plot(data,marker=\"o\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    name=title.replace(\" \",\"_\")\n",
    "    plt.savefig(save_path+f\"{name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Removing first element.\n",
    "val_loss=[x for x in model.val_loss[1:]]\n",
    "val_acc1=[x for x in model.val_acc1[1:]]\n",
    "val_acc3=[x for x in model.val_acc3[1:]]\n",
    "val_acc5=[x for x in model.val_acc5[1:]]\n",
    "val_acc10=[x for x in model.val_acc10[1:]]\n",
    "\n",
    "# print(\"Performance lists: \")\n",
    "# print(model.train_loss)\n",
    "# print(model.train_acc)\n",
    "# print(val_loss)\n",
    "# print(val_acc1)\n",
    "# print(val_acc3)\n",
    "# print(val_acc5)\n",
    "# print(val_acc10)\n",
    "# print(model.perc)\n",
    "# print(model.var)\n",
    "\n",
    "# Plotting.\n",
    "plotter(model.train_loss,\"epochs\",\"loss\",\"Train Loss\",False)\n",
    "plotter(model.train_acc,\"epochs\",\"accuracy\",\"Train Accuracy\",False)\n",
    "plotter(val_loss,\"epochs\",\"loss\",\"Validation Loss\",False)\n",
    "plotter(val_acc1,\"epochs\",\"accuracy\",\"Validation Top-1 Accuracy\",False)\n",
    "plotter(val_acc3,\"epochs\",\"accuracy\",\"Validation Top-3 Accuracy\",False)\n",
    "plotter(val_acc5,\"epochs\",\"accuracy\",\"Validation Top-5 Accuracy\",False)\n",
    "plotter(val_acc10,\"epochs\",\"accuracy\",\"Validation Top-10 Accuracy\",False)\n",
    "plotter(model.perc[-1],\"layers\",\"percentage\",\"Percentage Halted Tokens\",True)\n",
    "plotter(model.var[-1],\"layers\",\"variance\",\"Variance Halted Tokens\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: MODEL SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the best Model.\n",
    "best_model_path = my_checkpoint_call.best_model_path\n",
    "\n",
    "# Load the best model from the Checkpoint.\n",
    "best_model = ViT.load_from_checkpoint(\n",
    "                   checkpoint_path=best_model_path,\n",
    "                   input_d = input_size,\n",
    "                   batch_size=batch,\n",
    "                   n_patches = patches,\n",
    "                   n_blocks = blocks,\n",
    "                   hidden_d = hidden_dim,\n",
    "                   n_heads = heads,\n",
    "                   out_d = classes,\n",
    "                   lr = learning_rate,\n",
    "                   nepochs = number_epochs,\n",
    "                   warmup_epochs = warmup_epochs,\n",
    "                   wd = weight_decay,\n",
    "                   temp = temperature,\n",
    "                   cmlp_ratio = class_mlp_ratio,\n",
    "                   vmlp_ratio = vit_mlp_ratio,\n",
    "                   )\n",
    "\n",
    "# Access the Best Model's Accuracy.\n",
    "best_model_accuracy = trainer.checkpoint_callback.best_model_score.item()\n",
    "print(f\"Best Model Accuracy: {best_model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save it as a pth file.\n",
    "# Specify the path where you want to save the model.\n",
    "model_path = f\"./models/best_model_acc_{best_model_accuracy:.5f}.pth\"\n",
    "\n",
    "# Save the model's state dict to the specified file.\n",
    "torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "# Save it as a CheckPoint (Specific of PyTorch Lightning = Model State Dictionary + Training State + Optimizer State).\n",
    "# Specify the path where you want to save the model checkpoint.\n",
    "ckpt_path = f\"./models/best_model_acc_{best_model_accuracy:.5f}.ckpt\"\n",
    "\n",
    "# Save the model's state dict to the specified file.\n",
    "torch.save(best_model.state_dict(), ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
