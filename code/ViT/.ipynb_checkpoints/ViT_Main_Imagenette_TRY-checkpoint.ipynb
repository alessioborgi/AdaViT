{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (AdaViT) ADAPTIVE VISION TRANSFORMERS\n",
    "\n",
    "### SAPIENZA UNIVESITY of ROME (DEPARTMENT OF COMPUTER, CONTROL AND MANAGEMENT ENGINEEERING)\n",
    "\n",
    "### COURSE IN ARTIFICIAL INTELLIGENCE AND ROBOTICS (ACADEMIC YEAR 2023/2024)\n",
    "\n",
    "### AUTHOR:  **ALESSIO BORGI**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to the ZIP file\n",
    "zip_file = \"imagenette-160-px.zip\"\n",
    "\n",
    "# Directory where you want to extract the contents of the ZIP file\n",
    "extract_dir = \"imagenette-160-px\"\n",
    "\n",
    "# Create a directory if it doesn't exist\n",
    "if not os.path.exists(extract_dir):\n",
    "    os.makedirs(extract_dir)\n",
    "\n",
    "# Open the ZIP file\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    # Extract all the contents of the ZIP file\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Path to the compressed TAR file\n",
    "tar_file = \"./imagenette-160-px/imagenette-160.tgz\"\n",
    "\n",
    "# Directory where you want to extract the contents of the TAR file\n",
    "extract_dir = \"imagenette-160-px\"\n",
    "\n",
    "# Create a directory if it doesn't exist\n",
    "if not os.path.exists(extract_dir):\n",
    "    os.makedirs(extract_dir)\n",
    "\n",
    "# Open the TAR file\n",
    "with tarfile.open(tar_file, 'r:gz') as tar_ref:\n",
    "    # Extract all the contents of the TAR file\n",
    "    tar_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"Extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0: IMPORTING LIBRARIES \\& SETTINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Importing PyTorch-related Libraries.\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts\n",
    "from torchmetrics.classification import Accuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "\n",
    "# Importing PyTorch Lightning-Related Libraries.\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Importing General Libraries.\n",
    "import os\n",
    "import csv\n",
    "import PIL\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Importing from Other Files.\n",
    "from Gumbel_Noise import add_gumbel_noise\n",
    "from Multi_Head_Self_Attention import MHSA\n",
    "from utils import set_device, seed_everything, show_images_imagenette\n",
    "from Transformations import ViT_Transformations\n",
    "from Patchifying import make_patches_from_image\n",
    "from adapters import adapt_timm_state_dict, adapt_torch_state_dict\n",
    "from Train_Val_Test_Classes import CustomTrainingTinyImagenet, CustomValidationTinyImagenet, CustomTestTinyImagenet\n",
    "from Positional_Embeddings import get_positional_embeddings_RoPE, get_positional_embeddings_SPE, get_positional_embeddings_BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: DATAMODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViT_DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir, batch_size, num_workers=4):\n",
    "        \"\"\"\n",
    "        Custom data module for ViT model training and validation with Imagenette-160px dataset.\n",
    "\n",
    "        Args:\n",
    "        - data_dir (str): Directory path where the dataset is stored or will be downloaded.\n",
    "        - batch_size (int): Batch size for training and validation DataLoader.\n",
    "        - num_workers (int, optional): Number of workers for DataLoader (default is 4).\n",
    "        \"\"\"\n",
    "        super(ViT_DataModule, self).__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_test = 5\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Define transformations\n",
    "        self.transform = ViT_Transformations()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Method to load and configure datasets for Training, Validation, and Test.\n",
    "\n",
    "        Args:\n",
    "        - stage (str, optional): 'fit' for Training and 'test' for Validation (default is None).\n",
    "        \"\"\"\n",
    "        self.train_dataset = ImageFolder(root=self.data_dir + '/train', transform=self.transform)\n",
    "        self.val_dataset = ImageFolder(root=self.data_dir + '/val', transform=self.transform)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Method to return the DataLoader for the Training Dataset.\n",
    "\n",
    "        Returns:\n",
    "        - train_dataloader (DataLoader): DataLoader for Training.\n",
    "        \"\"\"\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Method to return the DataLoader for the Validation Dataset.\n",
    "\n",
    "        Returns:\n",
    "        - val_dataloader (DataLoader): DataLoader for Validation.\n",
    "        \"\"\"\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define batch size and number of workers\\nbatch_size = 32\\nnum_workers = 4\\ndata_dir = \"./dataset\"\\n# Create an instance of the ViT_DataModule class\\ndata_module = ViT_DataModule(data_dir, batch_size, num_workers)\\n\\n# Call the setup method to load and configure datasets\\ndata_module.setup()\\n\\n# Show training images\\nshow_images_imagenette(data_module.train_dataloader(), title=\"Training Images\")\\n# Show validation images\\nshow_images_imagenette(data_module.val_dataloader(), title=\"Validation Images\")\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Define batch size and number of workers\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "data_dir = \"./dataset\"\n",
    "# Create an instance of the ViT_DataModule class\n",
    "data_module = ViT_DataModule(data_dir, batch_size, num_workers)\n",
    "\n",
    "# Call the setup method to load and configure datasets\n",
    "data_module.setup()\n",
    "\n",
    "# Show training images\n",
    "show_images_imagenette(data_module.train_dataloader(), title=\"Training Images\")\n",
    "# Show validation images\n",
    "show_images_imagenette(data_module.val_dataloader(), title=\"Validation Images\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: VIT-BLOCK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViT_Block_Layer_Norm(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_d, n_heads, dropout_mhsa, dropout_block=None, mlp_ratio=10):\n",
    "        \n",
    "        super(ViT_Block_Layer_Norm, self).__init__()\n",
    "        \n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MHSA(hidden_d, hidden_d, n_heads)\n",
    "        self.dropout_block = nn.Dropout(p=dropout_block)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = x + self.dropout_block(self.mhsa(self.norm1(x)))\n",
    "        out = x + self.mlp(self.norm2(x1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: VIT DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Vision_Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, chw, batch_size, n_patches, n_blocks, hidden_d, n_heads, out_d, lr, nepochs, warmup_epochs, wd, temp, dropout_mhsa, dropout_block, dropout_ViT, cmlp_ratio, vmlp_ratio,\n",
    "                 torch_pretrained_weights=None, timm_pretrained_weights=None):\n",
    "        \"\"\"\n",
    "        Initialize the Vision_Transformer model.\n",
    "\n",
    "        Parameters:\n",
    "        - input_d (int): Dimension of the input.\n",
    "        - n_patches (int): Number of patches.\n",
    "        - n_blocks (int): Number of transformer blocks.\n",
    "        - hidden_d (int): Dimension of the hidden layer.\n",
    "        - n_heads (int): Number of attention heads.\n",
    "        - out_d (int): Output dimension.\n",
    "        - lr (float): Learning rate.\n",
    "        - wd (float): Weight decay.\n",
    "        - temp (float): Temperature scaling.\n",
    "        - dropout_mhsa (float): Dropout probability of the MHSA.\n",
    "        - dropout_block (float): Dropout probability of the ViT Block.\n",
    "        - dropout_ViT (float): Dropout probability of the Vision Transformer.\n",
    "        - cmlp_ratio (int): Ratio for the classification MLP.\n",
    "        - vmlp_ratio (int): Ratio for the ViT MLP.\n",
    "        \"\"\"\n",
    "\n",
    "        # Super Constructor.\n",
    "        super(Vision_Transformer, self).__init__()\n",
    "\n",
    "        # Model Attributes.\n",
    "        self.chw = chw # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "        self.dropout_mhsa = dropout_mhsa\n",
    "        self.dropout_block = dropout_block\n",
    "        self.dropout_vit = nn.Dropout(dropout_ViT)\n",
    "        self.class_mlp_ratio = cmlp_ratio\n",
    "        self.vit_mlp_ratio = vmlp_ratio\n",
    "\n",
    "        print(\"the channel shape would be\", chw)\n",
    "        # Input and Patches Sizes.\n",
    "        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "        \n",
    "\n",
    "        # 1) Linear Mapper.\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable Classification Token.\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional Embedding.\n",
    "        #self.register_buffer('positional_embeddings', get_positional_embeddings_SPE(n_patches ** 2 + 1, self.hidden_d), persistent=False)\n",
    "        self.register_buffer('positional_embeddings', get_positional_embeddings_RoPE(n_patches ** 2 + 1, self.hidden_d), persistent=False)\n",
    "        \n",
    "        # 4) Transformer Encoder Blocks.\n",
    "        # Select one of the following blocks.\n",
    "        # 4.1) Transformer Block with Layer Normalization.\n",
    "        self.blocks = nn.ModuleList([ViT_Block_Layer_Norm(self.hidden_d, self.n_heads, self.dropout_mhsa, self.dropout_block, self.vit_mlp_ratio) for _ in range(self.n_blocks)])\n",
    "\n",
    "        # 5) Classification MLP.\n",
    "        self.mlp = nn.Sequential(\n",
    "          nn.Linear(self.hidden_d, class_mlp_ratio * hidden_d),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(class_mlp_ratio * hidden_d, out_d),\n",
    "        )\n",
    "        print(\"I am here!\")\n",
    "        # Load pretrained weights using 'facebookresearch/deit_base_patch16_224'.\n",
    "        self.load_weights(timm_pretrained_weights=timm_pretrained_weights)\n",
    "\n",
    "        print(\"I am here 2!\")\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass of the Vision_Transformer model.\n",
    "\n",
    "        Parameters:\n",
    "        - images (torch.Tensor): Input images tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Dividing Images into Patches.\n",
    "        n, c, h, w = images.shape\n",
    "        patches = make_patches_from_image(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "\n",
    "        # Running Linear Layer Tokenization.\n",
    "        # Map the Vector corresponding to each patch to the Hidden Size Dimension.\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding Classification Token to the Tokens.\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1).to(\"cuda\")\n",
    "\n",
    "        # Adding Positional Embedding.\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1).to(\"cuda\")\n",
    "        \n",
    "        # Passing over the Dropout Layer.\n",
    "        out = self.dropout_vit(out)\n",
    "        \n",
    "        # Transformer Blocks.\n",
    "        for block in self.blocks:\n",
    "            \n",
    "            out = block(out)\n",
    "            \n",
    "        out = out[:, 0]\n",
    "        \n",
    "        return self.mlp(out)\n",
    "        \n",
    "    def load_weights(self, torch_pretrained_weights = None, timm_pretrained_weights = None):\n",
    "        \"\"\"\n",
    "        Loads pretrained weights into the model.\n",
    "\n",
    "        Args:\n",
    "            torch_pretrained_weights (str, optional): Path to the torch pretrained weights file or a URL to download from. Defaults to None.\n",
    "            timm_pretrained_weights (List, optional): List containing the name of the timm model and the variant to load pretrained weights from. Defaults to None.\n",
    "        \n",
    "        Example:\n",
    "            torch_pretrained_weights = 'ViT_B_16_Weights[IMAGENET1K_V1]'\n",
    "            timm_pretrained_weights = ['facebookresearch/deit_base_patch16_224', 'deit_base_patch16_224']\n",
    "            torch_pretrained_weights = 'path/to/torchweights.pth'\n",
    "            timm_pretrained_weights = 'path/to/timmweights.pth'\n",
    "        \"\"\"\n",
    "        seed_everything(31)\n",
    "        # they cannot be both not None\n",
    "        assert not (torch_pretrained_weights and timm_pretrained_weights), \"You cannot load weights from both torch and timm at the same time.\"\n",
    "        \n",
    "        if torch_pretrained_weights is not None:\n",
    "            \n",
    "            print('Loading torch pretrained weights: ', torch_pretrained_weights)\n",
    "                        \n",
    "            if not os.path.exists(str(torch_pretrained_weights)):\n",
    "                \n",
    "                print('Downloading torch pretrained weights: ', torch_pretrained_weights)\n",
    "                torch_pretrained_weights = eval(torch_pretrained_weights).get_state_dict(progress=False)\n",
    "                adapted_state_dict = adapt_torch_state_dict(torch_pretrained_weights, num_classes=self.num_classes)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                torch_pretrained_weights = torch.load(torch_pretrained_weights)\n",
    "                print(f'Loaded torch pretrained weights with these keys {list(torch_pretrained_weights.keys())}. I assume the model weights are in the the \"model\" key.')\n",
    "                torch_pretrained_weights = torch_pretrained_weights['model']\n",
    "                adapted_state_dict = adapt_torch_state_dict(torch_pretrained_weights, num_classes=self.num_classes)\n",
    "                \n",
    "            self.load_state_dict(adapted_state_dict, strict=True)\n",
    "            \n",
    "        elif timm_pretrained_weights is not None:\n",
    "            \n",
    "            print('Loading timm pretrained weights: ', timm_pretrained_weights)\n",
    "                        \n",
    "            if not os.path.exists(str(timm_pretrained_weights)):\n",
    "                \n",
    "                print('Downloading timm pretrained weights: ', timm_pretrained_weights)\n",
    "                model = torch.hub.load(timm_pretrained_weights[0], timm_pretrained_weights[1], pretrained=True)\n",
    "                timm_pretrained_weights = model.state_dict()\n",
    "                del model\n",
    "                \n",
    "            else:\n",
    "\n",
    "                timm_pretrained_weights = torch.load(timm_pretrained_weights)\n",
    "                print(f'Loaded timm pretrained weights with these keys {list(timm_pretrained_weights.keys())}. I assume the model weights are in the the \"model\" key.')\n",
    "                timm_pretrained_weights = timm_pretrained_weights['model']\n",
    "                \n",
    "            adapted_state_dict = adapt_timm_state_dict(timm_pretrained_weights, num_classes=self.num_classes)\n",
    "            self.load_state_dict(adapted_state_dict, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class ViT(Vision_Transformer, pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_d, batch_size, n_patches, n_blocks, hidden_d, n_heads, out_d, lr, nepochs, warmup_epochs, wd, temp, dropout_mhsa, dropout_block, dropout_vit, cmlp_ratio, vmlp_ratio, \n",
    "                 torch_pretrained_weights = None, timm_pretrained_weights = None):\n",
    "        \"\"\"\n",
    "        Initialize the ViT, a LightningModule using Vision_Transformer as a base.\n",
    "\n",
    "        Parameters:\n",
    "        - input_d (int): Dimension of the input.\n",
    "        - n_patches (int): Number of patches.\n",
    "        - n_blocks (int): Number of transformer blocks.\n",
    "        - hidden_d (int): Dimension of the hidden layer.\n",
    "        - n_heads (int): Number of attention heads.\n",
    "        - out_d (int): Output dimension.\n",
    "        - lr (float): Learning rate.\n",
    "        - wd (float): Weight decay.\n",
    "        - temp (float): Temperature scaling.\n",
    "        - dropout_mhsa (float): Dropout probability of the MHSA.\n",
    "        - dropout_block(float): Dropout probability of the ViT Block.\n",
    "        - dropout_vit(float): Dropout probability of the ViT after Positional Embedding and before ViT Blocks.\n",
    "        - cmlp_ratio (int): Ratio for the classification MLP.\n",
    "        - vmlp_ratio (int): Ratio for the ViT MLP.\n",
    "        - torch_pretrained_weights\n",
    "        - timm_pretrained_weights\n",
    "        \"\"\"\n",
    "        super(ViT, self).__init__(input_d, batch_size, n_patches, n_blocks, hidden_d, n_heads, out_d, lr, nepochs, warmup_epochs, wd, temp, dropout_mhsa, dropout_block, dropout_vit, cmlp_ratio, vmlp_ratio, torch_pretrained_weights = None, timm_pretrained_weights = None)\n",
    "        # Optimizer hyperparams.\n",
    "        self.learning_rate = lr\n",
    "        self.nepochs = nepochs\n",
    "        self.weight_decay = wd\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Definition of the Cross Entropy Loss.\n",
    "        self.loss = CrossEntropyLoss()\n",
    "        self.temperature = temp\n",
    "\n",
    "        # Definition of Accuracies, F1Score, Precision, and Recall Metrics.\n",
    "        self.acc_top1 = Accuracy(task=\"multiclass\", num_classes=out_d)\n",
    "        self.acc_top3 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=3)\n",
    "        self.acc_top5 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=5)\n",
    "        self.acc_top10 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=10)\n",
    "        self.f1score = MulticlassF1Score(num_classes=out_d, average='macro')\n",
    "        self.precision = MulticlassPrecision(num_classes=out_d, average='macro')\n",
    "        self.recall = MulticlassRecall(num_classes=out_d, average='macro')\n",
    "\n",
    "        # Definition of lists to be used in the \"on_ ... _epoch_end\" functions.\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "        #list of train and validation per epochs performance\n",
    "        self.train_loss=[]\n",
    "        self.train_acc=[]\n",
    "        self.val_loss=[]\n",
    "        self.val_acc1=[]\n",
    "        self.val_acc3=[]\n",
    "        self.val_acc5=[]\n",
    "        self.val_acc10=[]\n",
    "        self.perc=[]\n",
    "        self.var=[]\n",
    "\n",
    "    def _step(self, batch):\n",
    "        \"\"\"\n",
    "        Common computation of the metrics among Training, Validation, and Test Set.\n",
    "\n",
    "        Parameters:\n",
    "        - batch (tuple): Input batch tuple.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Tuple containing loss and various metrics.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        # Compute the Logits.\n",
    "        preds = self(x)\n",
    "        # Scale the logits using a Temperature Scaling and add Gumbel Noise, s.t. you obtain Gumbel Softmax then.\n",
    "        preds_scaled = add_gumbel_noise(preds) / self.temperature\n",
    "        loss = self.loss(preds_scaled, y) \n",
    "        acc1 = self.acc_top1(preds, y)\n",
    "        acc3 = self.acc_top3(preds, y)\n",
    "        acc5 = self.acc_top5(preds, y)\n",
    "        acc10 = self.acc_top10(preds, y)\n",
    "        f1score = self.f1score(preds, y)\n",
    "        precision = self.precision(preds, y)\n",
    "        recall = self.recall(preds, y)\n",
    "\n",
    "        return loss, acc1, acc3, acc5, acc10, f1score, precision, recall\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step function.\n",
    "\n",
    "        Parameters:\n",
    "        - batch (tuple): Input batch tuple.\n",
    "        - batch_idx (int): Batch index.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Training loss.\n",
    "        \"\"\"\n",
    "        # Compute the Training Loss and Accuracy.\n",
    "        loss, acc, _, _, _, _, _, _ = self._step(batch)\n",
    "\n",
    "        # Create a Dictionary to represent the output of the Training step.\n",
    "        training_step_output = {\n",
    "            \"train_loss\": loss.item(),\n",
    "            \"train_acc\": acc.item()\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list.\n",
    "        self.training_step_outputs.append(training_step_output)\n",
    "\n",
    "        # Perform logging.\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Method called at the end of the training epoch.\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Calculate the Mean Loss and Accuracy from the list of dictionaries.\n",
    "        loss_tot = torch.tensor([item[\"train_loss\"] for item in self.training_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot = torch.tensor([item[\"train_acc\"] for item in self.training_step_outputs]).mean().to(\"cuda\")\n",
    "\n",
    "        # Log the mean values.\n",
    "        self.log(\"train_loss\", loss_tot)\n",
    "        self.log(\"train_acc\", acc_tot)\n",
    "\n",
    "        # Print messages.\n",
    "        message_loss = f'Epoch {self.current_epoch} Training Loss -> {loss_tot}'\n",
    "        message_accuracy = f'      Training Accuracy -> {acc_tot}'\n",
    "        print(message_loss + message_accuracy)\n",
    "\n",
    "        # Clear the list to free memory.\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "        #updating training performance lists\n",
    "        self.train_loss.append(loss_tot.item())\n",
    "        self.train_acc.append(acc_tot.item())\n",
    "\n",
    "\n",
    "        #updating csv logs file\n",
    "        new_row=str(self.train_loss[-1])+\",\"+str(self.train_acc[-1])+\",\"+str(self.val_loss[-1])+\",\"+str(self.val_acc1[-1])+\",\"+str(self.val_acc3[-1])+\",\"+str(self.val_acc5[-1])+\",\"+str(self.val_acc10[-1])\n",
    "        with open(\"./results/logs.csv\",'a',newline='') as file:\n",
    "            #writing row\n",
    "            file.write(new_row+\"\\n\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step function.\n",
    "\n",
    "        Parameters:\n",
    "        - batch (tuple): Input batch tuple.\n",
    "        - batch_idx (int): Batch index.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Compute the Validation Loss and Accuracy.\n",
    "        loss, acc1, acc3, acc5, acc10, _, _, _ = self._step(batch)\n",
    "\n",
    "        # Create a Dictionary to represent the output of the validation step.\n",
    "        validation_step_output = {\n",
    "            \"val_loss\": loss.item(),\n",
    "            \"val_acc\": acc1.item(),\n",
    "            \"val_acc_3\": acc3.item(),\n",
    "            \"val_acc_5\": acc5.item(),\n",
    "            \"val_acc_10\": acc10.item(),\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list.\n",
    "        self.validation_step_outputs.append(validation_step_output)\n",
    "\n",
    "        # Perform logging.\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", acc1, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_3\", acc3, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_5\", acc5, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_10\", acc10, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Method called at the end of the validation epoch.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Calculate the Mean Loss and Accuracy from the list of dictionaries.\n",
    "        loss_tot = torch.tensor([item[\"val_loss\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot = torch.tensor([item[\"val_acc\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot_3 = torch.tensor([item[\"val_acc_3\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot_5 = torch.tensor([item[\"val_acc_5\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot_10 = torch.tensor([item[\"val_acc_10\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "\n",
    "        # Log the mean values.\n",
    "        self.log(\"val_loss\", loss_tot)\n",
    "        self.log(\"val_acc\", acc_tot)\n",
    "        self.log(\"val_acc_3\", acc_tot_3)\n",
    "        self.log(\"val_acc_5\", acc_tot_5)\n",
    "        self.log(\"val_acc_10\", acc_tot_10)\n",
    "\n",
    "        # Print messages.\n",
    "        message_loss = f'Epoch {self.current_epoch} Validation Loss -> {loss_tot}'\n",
    "        message_accuracy = f'      Validation Accuracy -> {acc_tot}'\n",
    "        message_accuracy_3 = f'      Validation Accuracy Top-3 -> {acc_tot_3}'\n",
    "        message_accuracy_5 = f'      Validation Accuracy Top-5-> {acc_tot_5}'\n",
    "        message_accuracy_10 = f'      Validation Accuracy Top-10-> {acc_tot_10}'\n",
    "        print(message_loss + message_accuracy + message_accuracy_3 + message_accuracy_5 + message_accuracy_10)\n",
    "\n",
    "        # Clear the list to free memory.\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "        #updating validation performance lists\n",
    "        self.val_loss.append(loss_tot.item())\n",
    "        self.val_acc1.append(acc_tot.item())\n",
    "        self.val_acc3.append(acc_tot_3.item())\n",
    "        self.val_acc5.append(acc_tot_5.item())\n",
    "        self.val_acc10.append(acc_tot_10.item())\n",
    "\n",
    "    def predict(self, input_image):\n",
    "        \"\"\"\n",
    "        Method called at Inference Time.\n",
    "\n",
    "        Returns:\n",
    "        predicted_labels: prediction over the labels.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = self(input_image)\n",
    "\n",
    "            # Scale the logits using a Temperature Scaling.\n",
    "            preds_scaled = F.log_softmax(preds / self.temperature, dim=1)\n",
    "\n",
    "            # Get the predicted class labels.\n",
    "            predicted_labels = torch.argmax(preds_scaled, dim=1).cpu().numpy()\n",
    "\n",
    "            return predicted_labels\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure the optimizer.\n",
    "\n",
    "        Returns:\n",
    "        torch.optim.Optimizer: The optimizer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Needed for initializing the lr scheduler\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=5, max_iters=100)\n",
    "\n",
    "        # Configure the Adam Optimizer.\n",
    "        #optimizer = optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001)\n",
    "        \n",
    "\n",
    "        #return optimizer\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: CALLBACK DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checkpoint CallBack Definition.\n",
    "my_checkpoint_call = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints/\",\n",
    "    filename=\"Best_Model\",\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "# Learning Rate CallBack Definition.\n",
    "my_lr_monitor_call = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "# Early Stopping CallBack Definition.\n",
    "my_early_stopping_call = pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, mode=\"min\", min_delta=0.001)\n",
    "\n",
    "# Progress Bar CallBack Definition.\n",
    "my_progress_bar_call = TQDMProgressBar(refresh_rate=10)\n",
    "\n",
    "# TensorBoardLogger CallBack Definition.\n",
    "#tb_logger = TensorBoardLogger(save_dir=\"./results/logs/\", name=\"ViT\")\n",
    "\n",
    "# CSV CallBack Definition.\n",
    "csv_logger = CSVLogger(\"./results/logs/\", name=\"ViT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: MODEL HYPERPAMETERS \\& MODEL INSTANTIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 31\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the channel shape would be (3, 224, 224)\n",
      "I am here!\n",
      "I am here 2!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the Adaptive Vision Transformer Model.\n",
    "# Model Hyperparameters-\n",
    "blocks=12\n",
    "heads=3\n",
    "classes=10\n",
    "hidden_dim=192\n",
    "batch=128\n",
    "\n",
    "learning_rate=1.5e-3\n",
    "weight_decay=1.5e-4\n",
    "temperature=0.5\n",
    "class_mlp_ratio=4\n",
    "vit_mlp_ratio=4\n",
    "input_size=(3, 224, 224)\n",
    "patches=14\n",
    "dropout_mhsa=0.1\n",
    "dropout_block=0.1\n",
    "dropout_vit=0.1\n",
    "\n",
    "number_epochs = 200\n",
    "warmup_epochs = 5\n",
    "\n",
    "model = ViT(input_d = input_size,\n",
    "            batch_size=batch,\n",
    "            n_patches = patches,\n",
    "            n_blocks = blocks,\n",
    "            hidden_d = hidden_dim,\n",
    "            n_heads = heads,\n",
    "            out_d = classes,\n",
    "            lr = learning_rate,\n",
    "            nepochs = number_epochs,\n",
    "            warmup_epochs = warmup_epochs,\n",
    "            wd = weight_decay,\n",
    "            temp = temperature,\n",
    "            dropout_mhsa = dropout_mhsa,\n",
    "            dropout_block = dropout_block,\n",
    "            dropout_vit = dropout_vit,\n",
    "            cmlp_ratio = class_mlp_ratio,\n",
    "            vmlp_ratio = vit_mlp_ratio,\n",
    "            torch_pretrained_weights = None, \n",
    "            timm_pretrained_weights = ['facebookresearch/deit_base_patch16_224', 'deit_base_patch16_224']\n",
    "        )\n",
    "\n",
    "data_module = ViT_DataModule(\n",
    "    data_dir=\"./dataset\",\n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "# Setup the Dataloaders.\n",
    "data_module.setup()\n",
    "\n",
    "# Create a PyTorch Lightning Trainer.\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=number_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else 0,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[my_progress_bar_call,\n",
    "               my_checkpoint_call,\n",
    "               my_lr_monitor_call,\n",
    "               my_early_stopping_call,\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"./results/logs.csv\", 'w') as file:\n",
    "    #writing header\n",
    "    perf_header=\"Training Loss\"+\",\"+\"Training Accuracy\"+\",\"+\"Validation Loss\"+\",\"+\"Validation Top-1 Accuracy\"+\",\"+\"Validation Top-3 Accuracy\"+\",\"+\"Validation Top-5 Accuracy\"+\",\"+\"Validation Top-10 Accuracy\"\n",
    "    halt_header=''\n",
    "    for i in range(2*blocks):\n",
    "        if i<blocks:\n",
    "            string=f\"Percentage halted tokens layer_{i}\"\n",
    "        else:\n",
    "            string=f\"Variance halted tokens layer_{i-blocks}\"\n",
    "        halt_header+=\",\"+string\n",
    "    file.write(f\"Model Hypeparameters: input_size={input_size},n_patches={patches},n_blocks={blocks},n_heads={heads},hidden_d={hidden_dim},output_size={classes},batch_size={batch},learning_rate={learning_rate},weight_decay={weight_decay},temp={temperature},class_mlp_ratio={class_mlp_ratio},vit_mlp_ratio={vit_mlp_ratio}\\n\")\n",
    "    file.write(perf_header+halt_header+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 31\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name          | Type                | Params\n",
      "-------------------------------------------------------\n",
      "0  | dropout_vit   | Dropout             | 0     \n",
      "1  | linear_mapper | Linear              | 147 K \n",
      "2  | blocks        | ModuleList          | 5.3 M \n",
      "3  | mlp           | Sequential          | 155 K \n",
      "4  | loss          | CrossEntropyLoss    | 0     \n",
      "5  | acc_top1      | MulticlassAccuracy  | 0     \n",
      "6  | acc_top3      | MulticlassAccuracy  | 0     \n",
      "7  | acc_top5      | MulticlassAccuracy  | 0     \n",
      "8  | acc_top10     | MulticlassAccuracy  | 0     \n",
      "9  | f1score       | MulticlassF1Score   | 0     \n",
      "10 | precision     | MulticlassPrecision | 0     \n",
      "11 | recall        | MulticlassRecall    | 0     \n",
      "   | other params  | n/a                 | 192   \n",
      "-------------------------------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.568    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Loss -> 5.382458686828613      Validation Accuracy -> 0.0      Validation Accuracy Top-3 -> 0.21875      Validation Accuracy Top-5-> 0.4140625      Validation Accuracy Top-10-> 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c4d4f917e845be92822a560d2cc51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Loss -> 5.053160667419434      Validation Accuracy -> 0.09765625      Validation Accuracy Top-3 -> 0.29296875      Validation Accuracy Top-5-> 0.49838361144065857      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 0 Training Loss -> 5.145489692687988      Training Accuracy -> 0.09883809834718704\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss -> 5.357664108276367      Validation Accuracy -> 0.09765625      Validation Accuracy Top-3 -> 0.29296875      Validation Accuracy Top-5-> 0.49838361144065857      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 1 Training Loss -> 5.139466762542725      Training Accuracy -> 0.09841063618659973\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss -> 5.026998043060303      Validation Accuracy -> 0.09765625      Validation Accuracy Top-3 -> 0.29296875      Validation Accuracy Top-5-> 0.49838361144065857      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 2 Training Loss -> 5.17178201675415      Training Accuracy -> 0.09899280220270157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss -> 4.995522975921631      Validation Accuracy -> 0.09765625      Validation Accuracy Top-3 -> 0.29296875      Validation Accuracy Top-5-> 0.49838361144065857      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 3 Training Loss -> 5.182971954345703      Training Accuracy -> 0.09812158346176147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Loss -> 5.289522647857666      Validation Accuracy -> 0.09765625      Validation Accuracy Top-3 -> 0.29296875      Validation Accuracy Top-5-> 0.49838361144065857      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 4 Training Loss -> 5.146728515625      Training Accuracy -> 0.09814193099737167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Loss -> 5.290524482727051      Validation Accuracy -> 0.09765625      Validation Accuracy Top-3 -> 0.29296875      Validation Accuracy Top-5-> 0.49838361144065857      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 5 Training Loss -> 5.141898155212402      Training Accuracy -> 0.09920451045036316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Loss -> 5.319032192230225      Validation Accuracy -> 0.09765625      Validation Accuracy Top-3 -> 0.29296875      Validation Accuracy Top-5-> 0.49838361144065857      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 6 Training Loss -> 5.127798557281494      Training Accuracy -> 0.09732119739055634\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Loss -> 5.090214729309082      Validation Accuracy -> 0.09765625      Validation Accuracy Top-3 -> 0.29296875      Validation Accuracy Top-5-> 0.49838361144065857      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 7 Training Loss -> 5.127041339874268      Training Accuracy -> 0.09817206859588623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Loss -> 5.0709452629089355      Validation Accuracy -> 0.09765625      Validation Accuracy Top-3 -> 0.29296875      Validation Accuracy Top-5-> 0.49838361144065857      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 8 Training Loss -> 5.139040946960449      Training Accuracy -> 0.09657128900289536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Loss -> 5.138837814331055      Validation Accuracy -> 0.09765625      Validation Accuracy Top-3 -> 0.29296875      Validation Accuracy Top-5-> 0.49838361144065857      Validation Accuracy Top-10-> 1.0\n",
      "Epoch 9 Training Loss -> 5.15295934677124      Training Accuracy -> 0.0965476855635643\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "seed_everything(31)\n",
    "trainer.fit(model.to(\"cuda\"), data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"./results/\"\n",
    "# Plotting Performace metrics\n",
    "def plotter(data, xlabel, ylabel, title, is_halting):\n",
    "    if is_halting:\n",
    "        plt.style.use('bmh')\n",
    "    plt.xticks([i for i in range(len(data))])\n",
    "    plt.plot(data,marker=\"o\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    name=title.replace(\" \",\"_\")\n",
    "    plt.savefig(save_path+f\"{name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Removing first element.\n",
    "val_loss=[x for x in model.val_loss[1:]]\n",
    "val_acc1=[x for x in model.val_acc1[1:]]\n",
    "val_acc3=[x for x in model.val_acc3[1:]]\n",
    "val_acc5=[x for x in model.val_acc5[1:]]\n",
    "val_acc10=[x for x in model.val_acc10[1:]]\n",
    "\n",
    "# print(\"Performance lists: \")\n",
    "# print(model.train_loss)\n",
    "# print(model.train_acc)\n",
    "# print(val_loss)\n",
    "# print(val_acc1)\n",
    "# print(val_acc3)\n",
    "# print(val_acc5)\n",
    "# print(val_acc10)\n",
    "# print(model.perc)\n",
    "# print(model.var)\n",
    "\n",
    "# Plotting.\n",
    "plotter(model.train_loss,\"epochs\",\"loss\",\"Train Loss\",False)\n",
    "plotter(model.train_acc,\"epochs\",\"accuracy\",\"Train Accuracy\",False)\n",
    "plotter(val_loss,\"epochs\",\"loss\",\"Validation Loss\",False)\n",
    "plotter(val_acc1,\"epochs\",\"accuracy\",\"Validation Top-1 Accuracy\",False)\n",
    "plotter(val_acc3,\"epochs\",\"accuracy\",\"Validation Top-3 Accuracy\",False)\n",
    "plotter(val_acc5,\"epochs\",\"accuracy\",\"Validation Top-5 Accuracy\",False)\n",
    "plotter(val_acc10,\"epochs\",\"accuracy\",\"Validation Top-10 Accuracy\",False)\n",
    "plotter(model.perc[-1],\"layers\",\"percentage\",\"Percentage Halted Tokens\",True)\n",
    "plotter(model.var[-1],\"layers\",\"variance\",\"Variance Halted Tokens\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: MODEL SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the best Model.\n",
    "best_model_path = my_checkpoint_call.best_model_path\n",
    "\n",
    "# Load the best model from the Checkpoint.\n",
    "best_model = ViT.load_from_checkpoint(\n",
    "                   checkpoint_path=best_model_path,\n",
    "                   input_d = input_size,\n",
    "                   batch_size=batch,\n",
    "                   n_patches = patches,\n",
    "                   n_blocks = blocks,\n",
    "                   hidden_d = hidden_dim,\n",
    "                   n_heads = heads,\n",
    "                   out_d = classes,\n",
    "                   lr = learning_rate,\n",
    "                   nepochs = number_epochs,\n",
    "                   warmup_epochs = warmup_epochs,\n",
    "                   wd = weight_decay,\n",
    "                   temp = temperature,\n",
    "                   dropout_mhsa = dropout_mhsa,\n",
    "                   dropout_block = dropout_block,\n",
    "                   dropout_vit = dropout_vit,\n",
    "                   cmlp_ratio = class_mlp_ratio,\n",
    "                   vmlp_ratio = vit_mlp_ratio,\n",
    "                   )\n",
    "\n",
    "# Access the Best Model's Accuracy.\n",
    "best_model_accuracy = trainer.checkpoint_callback.best_model_score.item()\n",
    "print(f\"Best Model Accuracy: {best_model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save it as a pth file.\n",
    "# Specify the path where you want to save the model.\n",
    "model_path = f\"./models/best_model_acc_{best_model_accuracy:.5f}.pth\"\n",
    "\n",
    "# Save the model's state dict to the specified file.\n",
    "torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "# Save it as a CheckPoint (Specific of PyTorch Lightning = Model State Dictionary + Training State + Optimizer State).\n",
    "# Specify the path where you want to save the model checkpoint.\n",
    "ckpt_path = f\"./models/best_model_acc_{best_model_accuracy:.5f}.ckpt\"\n",
    "\n",
    "# Save the model's state dict to the specified file.\n",
    "torch.save(best_model.state_dict(), ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
