{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (AdaViT) ADAPTIVE VISION TRANSFORMERS\n",
    "\n",
    "### SAPIENZA UNIVESITY of ROME (DEPARTMENT OF COMPUTER, CONTROL AND MANAGEMENT ENGINEEERING)\n",
    "\n",
    "### COURSE IN ARTIFICIAL INTELLIGENCE AND ROBOTICS (ACADEMIC YEAR 2023/2024)\n",
    "\n",
    "### AUTHOR:  **ALESSIO BORGI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d alessioborgi/pico-imagenet-10\n",
    "#!unzip -q pico-imagenet-10.zip\n",
    "#!mv \"pico-imagenet\" dataset\n",
    "\n",
    "\n",
    "#!kaggle datasets download -d alessioborgi/nano-imagenet-30\n",
    "#!unzip -q nano-imagenet-30.zip\n",
    "#!mv \"nano-imagenet\" dataset\n",
    "\n",
    "\n",
    "#!kaggle datasets download -d alessioborgi/tiny-imagenet-200\n",
    "# !unzip -q tiny-imagenet-200.zip\n",
    "# !mv \"tiny-imagenet-200\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0: IMPORTING LIBRARIES \\& SETTINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing PyTorch-related Libraries.\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts\n",
    "from torchmetrics.classification import Accuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "\n",
    "# Importing PyTorch Lightning-Related Libraries.\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Importing General Libraries.\n",
    "import os\n",
    "import csv\n",
    "import PIL\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Importing from Other Files.\n",
    "from Train_Val_Test_Classes import CustomTrainingTinyImagenet, CustomValidationTinyImagenet, CustomTestTinyImagenet\n",
    "from Transformations import ViT_Transformations\n",
    "from Multi_Head_Self_Attention import MHSA\n",
    "from Positional_Embeddings import get_positional_embeddings_RoPE, get_positional_embeddings_SPE, get_positional_embeddings_BERT\n",
    "from Gumbel_Noise import add_gumbel_noise\n",
    "from utils import set_device, seed_everything\n",
    "from Patchifying import make_matches_from_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: DATAMODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainingTinyImagenet(ImageFolder):\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset class for Tiny ImageNet Training data.\n",
    "\n",
    "        Args:\n",
    "        - root (str): Root directory containing the dataset.\n",
    "        - transform (callable, optional): Optional transform to be applied to the Input Image.\n",
    "        \"\"\"\n",
    "        super(CustomTrainingTinyImagenet, self).__init__(root, transform=transform)\n",
    "\n",
    "        # Create mappings between class labels and numerical indices\n",
    "        self.class_to_index = {cls: idx for idx, cls in enumerate(sorted(self.classes))}\n",
    "        self.index_to_class = {idx: cls for cls, idx in self.class_to_index.items()}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Method to retrieve an item from the dataset.\n",
    "\n",
    "        Args:\n",
    "        - index (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        - sample (torch.Tensor): Transformed image sample.\n",
    "        - target (int): Numerical index corresponding to the class label.\n",
    "        \"\"\"\n",
    "        # Retrieve the item and its label from the Dataset.\n",
    "        path, target = self.samples[index]\n",
    "\n",
    "        # Load the image using the default loader.\n",
    "        sample = self.loader(path)\n",
    "\n",
    "        # Apply the specified transformations, if any.\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # Adjust the directory depth to get the target label.\n",
    "        target_str = os.path.basename(os.path.dirname(os.path.dirname(path)))\n",
    "\n",
    "        # Convert string label to numerical index using the mapping.\n",
    "        target = self.class_to_index[target_str]\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    def get_class_from_index(self, index):\n",
    "        \"\"\"\n",
    "        Method to retrieve the class label from a numerical index.\n",
    "\n",
    "        Args:\n",
    "        - index (int): Numerical index corresponding to the class label.\n",
    "\n",
    "        Returns:\n",
    "        - class_label (str): Class label corresponding to the numerical index.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.index_to_class[index]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class CustomValidationTinyImagenet(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        \"\"\"\n",
    "        Custom data module for Tiny ImageNet Validation data.\n",
    "\n",
    "        Args:\n",
    "        - root (str): Root directory containing the dataset.\n",
    "        - transform (callable, optional): Optional transform to be applied to the Input Image.\n",
    "        \"\"\"\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load and preprocess labels\n",
    "        self.labels = self.load_labels()\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(sorted(set(self.labels.values())))}\n",
    "        self.index_to_label = {idx: label for label, idx in self.label_to_index.items()}\n",
    "\n",
    "    def load_labels(self):\n",
    "        \"\"\"\n",
    "        Method to load and Pre-Process Labels from the Validation Dataset.\n",
    "\n",
    "        Returns:\n",
    "        - labels (dict): Dictionary mapping image names to labels.\n",
    "        \"\"\"\n",
    "        label_path = \"./dataset/pico-imagenet-10/val/val_annotations.txt\"\n",
    "        labels = {}\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for i,line in enumerate(lines):\n",
    "            if i == 0:\n",
    "                parts = line.split(\"\\t\")\n",
    "                image_name, label = parts[0], parts[1]\n",
    "                labels['val_0.JPEG'] = label\n",
    "            else:\n",
    "                parts = line.split(\"\\t\")\n",
    "                image_name, label = parts[0], parts[1]\n",
    "                labels[image_name] = label\n",
    "\n",
    "        return labels\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Method to get the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - length (int): Number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Method to retrieve an item from the dataset.\n",
    "\n",
    "        Args:\n",
    "        - index (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        - image (torch.Tensor): Transformed image sample.\n",
    "        - label (int): Numerical index corresponding to the class label.\n",
    "        \"\"\"\n",
    "        image_name = f\"val_{index}.JPEG\"\n",
    "        image_path = self.root / image_name\n",
    "\n",
    "        # Open the image using PIL and convert to RGB.\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply the specified transformations, if any.\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Use the get method to handle cases where the key is not present.\n",
    "        label_str = self.labels.get(image_name, 'Label not found')\n",
    "\n",
    "        # Convert string label to numerical index using the mapping.\n",
    "        label = self.label_to_index[label_str]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_label_from_index(self, index):\n",
    "        \"\"\"\n",
    "        Method to retrieve the class label from a numerical index.\n",
    "\n",
    "        Args:\n",
    "        - index (int): Numerical index corresponding to the class label.\n",
    "\n",
    "        Returns:\n",
    "        - class_label (str): Class label corresponding to the numerical index.\n",
    "        \"\"\"\n",
    "        return self.index_to_label[index]\n",
    "    \n",
    "    \n",
    "\n",
    "class CustomTestTinyImagenet(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset class for Tiny ImageNet Test data.\n",
    "\n",
    "        Args:\n",
    "        - root (str): Root directory containing the dataset.\n",
    "        - transform (callable, optional): Optional transform to be applied to the Input Image.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_paths = self._get_image_paths()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Method to get the total number of items in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - int: Total number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Method to retrieve an item from the dataset.\n",
    "\n",
    "        Args:\n",
    "        - index (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        - sample (torch.Tensor): Transformed image sample.\n",
    "        - filename (str): Filename of the image.\n",
    "        \"\"\"\n",
    "        # Get the image path based on the index.\n",
    "        image_path = self.image_paths[index]\n",
    "\n",
    "        # Load the image using the default loader.\n",
    "        sample = Image.open(image_path)\n",
    "\n",
    "        # Apply the specified transformations, if any.\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # Extract the filename from the image path.\n",
    "        filename = os.path.basename(image_path)\n",
    "\n",
    "        # Return a tuple containing the sample and filename.\n",
    "        return sample, filename\n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        \"\"\"\n",
    "        Helper method to get the paths of all images in the test dataset.\n",
    "\n",
    "        Returns:\n",
    "        - list: List of image paths.\n",
    "        \"\"\"\n",
    "        image_paths = [os.path.join(self.root, filename) for filename in os.listdir(self.root)]\n",
    "        return image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViT_DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_data_dir, val_data_dir, test_data_dir, batch_size, num_workers=4):\n",
    "        \"\"\"\n",
    "        Custom data module for AViT model training and validation.\n",
    "\n",
    "        Args:\n",
    "        - train_data_dir (str): Directory path for the training dataset.\n",
    "        - val_data_dir (str): Directory path for the validation dataset.\n",
    "        - batch_size (int): Batch size for training and validation DataLoader.\n",
    "        - num_workers (int, optional): Number of workers for DataLoader (default is 4).\n",
    "        \"\"\"\n",
    "        super(ViT_DataModule, self).__init__()\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.val_data_dir = val_data_dir\n",
    "        self.test_data_dir = test_data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_test = 5\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Use AdaViT transformations for data augmentation\n",
    "        self.transform = ViT_Transformations()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Method to load and configure datasets for Training and Validation.\n",
    "\n",
    "        Args:\n",
    "        - stage (str, optional): 'fit' for Training and 'test' for Validation (default is None).\n",
    "        \"\"\"\n",
    "        # Load Train dataset using CustomTrainingTinyImagenet with the new directory structure.\n",
    "        self.train_dataset = CustomTrainingTinyImagenet(self.train_data_dir, transform=self.transform)\n",
    "\n",
    "        # Load Validation dataset.\n",
    "        self.val_dataset = CustomValidationTinyImagenet(self.val_data_dir, transform=self.transform)\n",
    "\n",
    "        # Load Test dataset.\n",
    "        self.test_dataset = CustomTestTinyImagenet(self.test_data_dir, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Method to return the DataLoader for the Training Dataset.\n",
    "\n",
    "        Returns:\n",
    "        - train_dataloader (DataLoader): DataLoader for Training.\n",
    "        \"\"\"\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Method to return the DataLoader for the Validation Dataset.\n",
    "\n",
    "        Returns:\n",
    "        - val_dataloader (DataLoader): DataLoader for Validation.\n",
    "        \"\"\"\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"\n",
    "        Method to return the DataLoader for the Test Dataset.\n",
    "\n",
    "        Returns:\n",
    "        - test_dataloader (DataLoader): DataLoader for Test Set.\n",
    "        \"\"\"\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size_test, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: VIT-BLOCK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_Block_Layer_Norm(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=10):\n",
    "        super(ViT_Block_Layer_Norm, self).__init__()\n",
    "        \n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MHSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
    "        )\n",
    "        \n",
    "        # Initialize weights.\n",
    "        self.initialize_weights_block()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out\n",
    "    \n",
    "    def initialize_weights_block(self):\n",
    "        \n",
    "        # Initialize weights for linear layers in mlp.\n",
    "        for layer in self.mlp:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: VIT DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vision_Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, chw, batch_size, n_patches, n_blocks, hidden_d, n_heads, out_d, lr, nepochs, warmup_epochs, wd, temp, cmlp_ratio, vmlp_ratio):\n",
    "        \"\"\"\n",
    "        Initialize the Vision_Transformer model.\n",
    "\n",
    "        Parameters:\n",
    "        - input_d (int): Dimension of the input.\n",
    "        - n_patches (int): Number of patches.\n",
    "        - n_blocks (int): Number of transformer blocks.\n",
    "        - hidden_d (int): Dimension of the hidden layer.\n",
    "        - n_heads (int): Number of attention heads.\n",
    "        - out_d (int): Output dimension.\n",
    "        - lr (float): Learning rate.\n",
    "        - wd (float): Weight decay.\n",
    "        - temp (float): Temperature scaling.\n",
    "        - cmlp_ratio (int): Ratio for the classification MLP.\n",
    "        - vmlp_ratio (int): Ratio for the ViT MLP.\n",
    "        \"\"\"\n",
    "\n",
    "        # Super Constructor.\n",
    "        super(Vision_Transformer, self).__init__()\n",
    "\n",
    "        # Model Attributes.\n",
    "        self.chw = chw # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "        self.class_mlp_ratio = cmlp_ratio\n",
    "        self.vit_mlp_ratio = vmlp_ratio\n",
    "\n",
    "    \n",
    "        # Input and Patches Sizes.\n",
    "        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear Mapper.\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable Classification Token.\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional Embedding.\n",
    "        #self.register_buffer('positional_embeddings', get_positional_embeddings_SPE(n_patches ** 2 + 1, self.hidden_d), persistent=False)\n",
    "        self.register_buffer('positional_embeddings', get_positional_embeddings_RoPE(n_patches ** 2 + 1, self.hidden_d), persistent=False)\n",
    "\n",
    "        # 4) Transformer Encoder Blocks.\n",
    "        # Select one of the following blocks.\n",
    "        # 4.1) Transformer Block with Layer Normalization.\n",
    "        self.blocks = nn.ModuleList([ViT_Block_Layer_Norm(self.hidden_d, self.n_heads,self.vit_mlp_ratio) for _ in range(self.n_blocks)])\n",
    "\n",
    "        # 5) Classification MLP.\n",
    "        self.mlp = nn.Sequential(\n",
    "          nn.Linear(self.hidden_d, class_mlp_ratio * hidden_d),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(class_mlp_ratio * hidden_d, out_d),\n",
    "        )\n",
    "\n",
    "        # Initialize weights.\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass of the Vision_Transformer model.\n",
    "\n",
    "        Parameters:\n",
    "        - images (torch.Tensor): Input images tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Dividing Images into Patches.\n",
    "        n, c, h, w = images.shape\n",
    "        patches = make_matches_from_image(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "\n",
    "        # Running Linear Layer Tokenization.\n",
    "        # Map the Vector corresponding to each patch to the Hidden Size Dimension.\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding Classification Token to the Tokens.\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1).to(\"cuda\")\n",
    "\n",
    "        # Adding Positional Embedding.\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1).to(\"cuda\")\n",
    "\n",
    "        # Transformer Blocks.\n",
    "        for block in self.blocks:\n",
    "            \n",
    "            out = block(out)\n",
    "            \n",
    "        out = out[:, 0]\n",
    "        \n",
    "        return self.mlp(out)\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for linear layers, embeddings, etc.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize Weights for Linear Layers, Embeddings, etc.\n",
    "        nn.init.xavier_uniform_(self.linear_mapper.weight)\n",
    "        nn.init.normal_(self.class_token.data)\n",
    "\n",
    "        # Initialize Weights for Classification MLP.\n",
    "        nn.init.xavier_uniform_(self.mlp[0].weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViT(Vision_Transformer, pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_d, batch_size, n_patches, n_blocks, hidden_d, n_heads, out_d, lr, nepochs, warmup_epochs, wd, temp, cmlp_ratio, vmlp_ratio):\n",
    "        \"\"\"\n",
    "        Initialize the ViT, a LightningModule using Vision_Transformer as a base.\n",
    "\n",
    "        Parameters:\n",
    "        - input_d (int): Dimension of the input.\n",
    "        - n_patches (int): Number of patches.\n",
    "        - n_blocks (int): Number of transformer blocks.\n",
    "        - hidden_d (int): Dimension of the hidden layer.\n",
    "        - n_heads (int): Number of attention heads.\n",
    "        - out_d (int): Output dimension.\n",
    "        - lr (float): Learning rate.\n",
    "        - wd (float): Weight decay.\n",
    "        - temp (float): Temperature scaling.\n",
    "        - cmlp_ratio (int): Ratio for the classification MLP.\n",
    "        - vmlp_ratio (int): Ratio for the ViT MLP.\n",
    "        \"\"\"\n",
    "        super(ViT, self).__init__(input_d, batch_size, n_patches, n_blocks, hidden_d, n_heads, out_d, lr, nepochs, warmup_epochs, wd, temp, cmlp_ratio, vmlp_ratio)\n",
    "        # Optimizer hyperparams.\n",
    "        self.learning_rate = lr\n",
    "        self.nepochs = nepochs\n",
    "        self.weight_decay = wd\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Definition of the Cross Entropy Loss.\n",
    "        self.loss = CrossEntropyLoss()\n",
    "        self.temperature = temp\n",
    "\n",
    "        # Definition of Accuracies, F1Score, Precision, and Recall Metrics.\n",
    "        self.acc_top1 = Accuracy(task=\"multiclass\", num_classes=out_d)\n",
    "        self.acc_top3 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=3)\n",
    "        self.acc_top5 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=5)\n",
    "        self.acc_top10 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=10)\n",
    "        self.f1score = MulticlassF1Score(num_classes=out_d, average='macro')\n",
    "        self.precision = MulticlassPrecision(num_classes=out_d, average='macro')\n",
    "        self.recall = MulticlassRecall(num_classes=out_d, average='macro')\n",
    "\n",
    "        # Definition of lists to be used in the \"on_ ... _epoch_end\" functions.\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "        #list of train and validation per epochs performance\n",
    "        self.train_loss=[]\n",
    "        self.train_acc=[]\n",
    "        self.val_loss=[]\n",
    "        self.val_acc1=[]\n",
    "        self.val_acc3=[]\n",
    "        self.val_acc5=[]\n",
    "        self.val_acc10=[]\n",
    "        self.perc=[]\n",
    "        self.var=[]\n",
    "\n",
    "    def _step(self, batch):\n",
    "        \"\"\"\n",
    "        Common computation of the metrics among Training, Validation, and Test Set.\n",
    "\n",
    "        Parameters:\n",
    "        - batch (tuple): Input batch tuple.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Tuple containing loss and various metrics.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        # Compute the Logits.\n",
    "        preds = self(x)\n",
    "        # Scale the logits using a Temperature Scaling and add Gumbel Noise, s.t. you obtain Gumbel Softmax then.\n",
    "        preds_scaled = add_gumbel_noise(preds) / self.temperature\n",
    "        loss = self.loss(preds_scaled, y) \n",
    "        acc1 = self.acc_top1(preds, y)\n",
    "        acc3 = self.acc_top3(preds, y)\n",
    "        acc5 = self.acc_top5(preds, y)\n",
    "        acc10 = self.acc_top10(preds, y)\n",
    "        f1score = self.f1score(preds, y)\n",
    "        precision = self.precision(preds, y)\n",
    "        recall = self.recall(preds, y)\n",
    "\n",
    "        return loss, acc1, acc3, acc5, acc10, f1score, precision, recall\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step function.\n",
    "\n",
    "        Parameters:\n",
    "        - batch (tuple): Input batch tuple.\n",
    "        - batch_idx (int): Batch index.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Training loss.\n",
    "        \"\"\"\n",
    "        # Compute the Training Loss and Accuracy.\n",
    "        loss, acc, _, _, _, _, _, _ = self._step(batch)\n",
    "\n",
    "        # Create a Dictionary to represent the output of the Training step.\n",
    "        training_step_output = {\n",
    "            \"train_loss\": loss.item(),\n",
    "            \"train_acc\": acc.item()\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list.\n",
    "        self.training_step_outputs.append(training_step_output)\n",
    "\n",
    "        # Perform logging.\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Method called at the end of the training epoch.\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Calculate the Mean Loss and Accuracy from the list of dictionaries.\n",
    "        loss_tot = torch.tensor([item[\"train_loss\"] for item in self.training_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot = torch.tensor([item[\"train_acc\"] for item in self.training_step_outputs]).mean().to(\"cuda\")\n",
    "\n",
    "        # Log the mean values.\n",
    "        self.log(\"train_loss\", loss_tot)\n",
    "        self.log(\"train_acc\", acc_tot)\n",
    "\n",
    "        # Print messages.\n",
    "        message_loss = f'Epoch {self.current_epoch} Training Loss -> {loss_tot}'\n",
    "        message_accuracy = f'      Training Accuracy -> {acc_tot}'\n",
    "        print(message_loss + message_accuracy)\n",
    "\n",
    "        # Clear the list to free memory.\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "        #updating training performance lists\n",
    "        self.train_loss.append(loss_tot.item())\n",
    "        self.train_acc.append(acc_tot.item())\n",
    "\n",
    "\n",
    "        #updating csv logs file\n",
    "        new_row=str(self.train_loss[-1])+\",\"+str(self.train_acc[-1])+\",\"+str(self.val_loss[-1])+\",\"+str(self.val_acc1[-1])+\",\"+str(self.val_acc3[-1])+\",\"+str(self.val_acc5[-1])+\",\"+str(self.val_acc10[-1])\n",
    "        with open(\"./results/logs.csv\",'a',newline='') as file:\n",
    "            #writing row\n",
    "            file.write(new_row+\"\\n\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step function.\n",
    "\n",
    "        Parameters:\n",
    "        - batch (tuple): Input batch tuple.\n",
    "        - batch_idx (int): Batch index.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Compute the Validation Loss and Accuracy.\n",
    "        loss, acc1, acc3, acc5, acc10, _, _, _ = self._step(batch)\n",
    "\n",
    "        # Create a Dictionary to represent the output of the validation step.\n",
    "        validation_step_output = {\n",
    "            \"val_loss\": loss.item(),\n",
    "            \"val_acc\": acc1.item(),\n",
    "            \"val_acc_3\": acc3.item(),\n",
    "            \"val_acc_5\": acc5.item(),\n",
    "            \"val_acc_10\": acc10.item(),\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list.\n",
    "        self.validation_step_outputs.append(validation_step_output)\n",
    "\n",
    "        # Perform logging.\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", acc1, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_3\", acc3, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_5\", acc5, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_10\", acc10, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Method called at the end of the validation epoch.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Calculate the Mean Loss and Accuracy from the list of dictionaries.\n",
    "        loss_tot = torch.tensor([item[\"val_loss\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot = torch.tensor([item[\"val_acc\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot_3 = torch.tensor([item[\"val_acc_3\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot_5 = torch.tensor([item[\"val_acc_5\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "        acc_tot_10 = torch.tensor([item[\"val_acc_10\"] for item in self.validation_step_outputs]).mean().to(\"cuda\")\n",
    "\n",
    "        # Log the mean values.\n",
    "        self.log(\"val_loss\", loss_tot)\n",
    "        self.log(\"val_acc\", acc_tot)\n",
    "        self.log(\"val_acc_3\", acc_tot_3)\n",
    "        self.log(\"val_acc_5\", acc_tot_5)\n",
    "        self.log(\"val_acc_10\", acc_tot_10)\n",
    "\n",
    "        # Print messages.\n",
    "        message_loss = f'Epoch {self.current_epoch} Validation Loss -> {loss_tot}'\n",
    "        message_accuracy = f'      Validation Accuracy -> {acc_tot}'\n",
    "        message_accuracy_3 = f'      Validation Accuracy Top-3 -> {acc_tot_3}'\n",
    "        message_accuracy_5 = f'      Validation Accuracy Top-5-> {acc_tot_5}'\n",
    "        message_accuracy_10 = f'      Validation Accuracy Top-10-> {acc_tot_10}'\n",
    "        print(message_loss + message_accuracy + message_accuracy_3 + message_accuracy_5 + message_accuracy_10)\n",
    "\n",
    "        # Clear the list to free memory.\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "        #updating validation performance lists\n",
    "        self.val_loss.append(loss_tot.item())\n",
    "        self.val_acc1.append(acc_tot.item())\n",
    "        self.val_acc3.append(acc_tot_3.item())\n",
    "        self.val_acc5.append(acc_tot_5.item())\n",
    "        self.val_acc10.append(acc_tot_10.item())\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Test step function.\n",
    "\n",
    "        Parameters:\n",
    "        - batch (tuple): Input batch tuple.\n",
    "        - batch_idx (int): Batch index.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Model predictions.\n",
    "        \"\"\"\n",
    "        x = batch\n",
    "        # Compute the Logits.\n",
    "        preds = self(x)\n",
    "        print(\"The prediction is: \", preds)\n",
    "        return preds\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Method called at the end of the test epoch.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        print(\"Test Epoch Complete.\")\n",
    "\n",
    "    def predict(self, input_image):\n",
    "        \"\"\"\n",
    "        Method called at Inference Time.\n",
    "\n",
    "        Returns:\n",
    "        predicted_labels: prediction over the labels.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = self(input_image)\n",
    "\n",
    "            # Scale the logits using a Temperature Scaling.\n",
    "            preds_scaled = F.log_softmax(preds / self.temperature, dim=1)\n",
    "\n",
    "            # Get the predicted class labels.\n",
    "            predicted_labels = torch.argmax(preds_scaled, dim=1).cpu().numpy()\n",
    "\n",
    "            return predicted_labels\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure the optimizer.\n",
    "\n",
    "        Returns:\n",
    "        torch.optim.Optimizer: The optimizer.\n",
    "        \"\"\"\n",
    "        # Configure the Adam Optimizer.\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001)\n",
    "        #optimizer, scheduler = get_optimizer_scheduler(\n",
    "        #                                               model,\n",
    "        #                                               lr=self.learning_rate,\n",
    "        #                                               warmup_epochs=self.warmup_epochs,\n",
    "        #                                               num_epochs=self.nepochs,\n",
    "        #                                               weight_decay=self.weight_decay,\n",
    "        #                                               batch_size=self.batch_size\n",
    "        #                                               )\n",
    "\n",
    "        #return optimizer\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: CALLBACK DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint CallBack Definition.\n",
    "my_checkpoint_call = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints/\",\n",
    "    filename=\"Best_Model\",\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "# Learning Rate CallBack Definition.\n",
    "my_lr_monitor_call = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "# Early Stopping CallBack Definition.\n",
    "my_early_stopping_call = pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, mode=\"min\", min_delta=0.001)\n",
    "\n",
    "# Progress Bar CallBack Definition.\n",
    "my_progress_bar_call = TQDMProgressBar(refresh_rate=10)\n",
    "\n",
    "# TensorBoardLogger CallBack Definition.\n",
    "#tb_logger = TensorBoardLogger(save_dir=\"./results/logs/\", name=\"ViT\")\n",
    "\n",
    "# CSV CallBack Definition.\n",
    "csv_logger = CSVLogger(\"./results/logs/\", name=\"ViT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: MODEL HYPERPAMETERS \\& MODEL INSTANTIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_84/1324034528.py\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Setup the Dataloaders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mdata_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Create a PyTorch Lightning Trainer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_84/3267260986.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \"\"\"\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Load Train dataset using CustomTrainingTinyImagenet with the new directory structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTrainingTinyImagenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Load Validation dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/AdaViT/code/ViT/Train_Val_Test_Classes.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Importing General Libraries.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the Adaptive Vision Transformer Model.\n",
    "# Model Hyperparameters-\n",
    "blocks=12\n",
    "heads=3\n",
    "classes=10\n",
    "hidden_dim=192\n",
    "batch=64\n",
    "\n",
    "learning_rate=1.5e-3\n",
    "weight_decay=1.5e-4\n",
    "temperature=0.5\n",
    "class_mlp_ratio=4\n",
    "vit_mlp_ratio=4\n",
    "input_size=(3,224, 224)\n",
    "patches=14\n",
    "\n",
    "number_epochs = 200\n",
    "warmup_epochs = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Halting Hyperparameters.\n",
    "gamma = 5\n",
    "beta = -10\n",
    "alpha_ponder = 5e-4\n",
    "alpha_distribution = 0.1\n",
    "epsilon = 0.01\n",
    "expected_stop_depth=2\n",
    "b_laplace=2 #Set it to \"None\" if not using laplace_distribution\n",
    "\n",
    "model = ViT(input_d = input_size,\n",
    "            batch_size=batch,\n",
    "            n_patches = patches,\n",
    "            n_blocks = blocks,\n",
    "            hidden_d = hidden_dim,\n",
    "            n_heads = heads,\n",
    "            out_d = classes,\n",
    "            lr = learning_rate,\n",
    "            nepochs = number_epochs,\n",
    "            warmup_epochs = warmup_epochs,\n",
    "            wd = weight_decay,\n",
    "            temp = temperature,\n",
    "            cmlp_ratio = class_mlp_ratio,\n",
    "            vmlp_ratio = vit_mlp_ratio\n",
    "        )\n",
    "\n",
    "data_module = ViT_DataModule(\n",
    "    train_data_dir=\"./dataset/pico-imagenet-10/train/\",\n",
    "    val_data_dir=\"./dataset/pico-imagenet-10/val/images/\",\n",
    "    test_data_dir=\"./dataset/pico-imagenet-10/test/images/\",\n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "# Setup the Dataloaders.\n",
    "data_module.setup()\n",
    "\n",
    "# Create a PyTorch Lightning Trainer.\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=number_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else 0,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[my_progress_bar_call,\n",
    "               my_checkpoint_call,\n",
    "               my_lr_monitor_call,\n",
    "               my_early_stopping_call,\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halting_row=f\"Halting Hyperparameters: gamma={gamma},beta={beta},alpha_ponder={alpha_ponder},alpha_distribution={alpha_distribution},epsilon={epsilon},expected_stop_depth={expected_stop_depth}\"\n",
    "if b_laplace is not None:\n",
    "    halting_row +=f\",b_laplace={b_laplace}\"\n",
    "\n",
    "with open(\"./results/logs.csv\", 'w') as file:\n",
    "    #writing header\n",
    "    perf_header=\"Training Loss\"+\",\"+\"Training Accuracy\"+\",\"+\"Validation Loss\"+\",\"+\"Validation Top-1 Accuracy\"+\",\"+\"Validation Top-3 Accuracy\"+\",\"+\"Validation Top-5 Accuracy\"+\",\"+\"Validation Top-10 Accuracy\"\n",
    "    halt_header=''\n",
    "    for i in range(2*blocks):\n",
    "        if i<blocks:\n",
    "            string=f\"Percentage halted tokens layer_{i}\"\n",
    "        else:\n",
    "            string=f\"Variance halted tokens layer_{i-blocks}\"\n",
    "        halt_header+=\",\"+string\n",
    "    file.write(f\"Model Hypeparameters: input_size={input_size},n_patches={patches},n_blocks={blocks},n_heads={heads},hidden_d={hidden_dim},output_size={classes},batch_size={batch},learning_rate={learning_rate},weight_decay={weight_decay},temp={temperature},class_mlp_ratio={class_mlp_ratio},vit_mlp_ratio={vit_mlp_ratio}\\n\")\n",
    "    file.write(halting_row+\"\\n\")\n",
    "    file.write(perf_header+halt_header+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(model.to(\"cuda\"), data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"./results/\"\n",
    "# Plotting Performace metrics\n",
    "def plotter(data, xlabel, ylabel, title, is_halting):\n",
    "    if is_halting:\n",
    "        plt.style.use('bmh')\n",
    "    plt.xticks([i for i in range(len(data))])\n",
    "    plt.plot(data,marker=\"o\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    name=title.replace(\" \",\"_\")\n",
    "    plt.savefig(save_path+f\"{name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Removing first element.\n",
    "val_loss=[x for x in model.val_loss[1:]]\n",
    "val_acc1=[x for x in model.val_acc1[1:]]\n",
    "val_acc3=[x for x in model.val_acc3[1:]]\n",
    "val_acc5=[x for x in model.val_acc5[1:]]\n",
    "val_acc10=[x for x in model.val_acc10[1:]]\n",
    "\n",
    "# print(\"Performance lists: \")\n",
    "# print(model.train_loss)\n",
    "# print(model.train_acc)\n",
    "# print(val_loss)\n",
    "# print(val_acc1)\n",
    "# print(val_acc3)\n",
    "# print(val_acc5)\n",
    "# print(val_acc10)\n",
    "# print(model.perc)\n",
    "# print(model.var)\n",
    "\n",
    "# Plotting.\n",
    "plotter(model.train_loss,\"epochs\",\"loss\",\"Train Loss\",False)\n",
    "plotter(model.train_acc,\"epochs\",\"accuracy\",\"Train Accuracy\",False)\n",
    "plotter(val_loss,\"epochs\",\"loss\",\"Validation Loss\",False)\n",
    "plotter(val_acc1,\"epochs\",\"accuracy\",\"Validation Top-1 Accuracy\",False)\n",
    "plotter(val_acc3,\"epochs\",\"accuracy\",\"Validation Top-3 Accuracy\",False)\n",
    "plotter(val_acc5,\"epochs\",\"accuracy\",\"Validation Top-5 Accuracy\",False)\n",
    "plotter(val_acc10,\"epochs\",\"accuracy\",\"Validation Top-10 Accuracy\",False)\n",
    "plotter(model.perc[-1],\"layers\",\"percentage\",\"Percentage Halted Tokens\",True)\n",
    "plotter(model.var[-1],\"layers\",\"variance\",\"Variance Halted Tokens\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: MODEL SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the best Model.\n",
    "best_model_path = my_checkpoint_call.best_model_path\n",
    "\n",
    "# Load the best model from the Checkpoint.\n",
    "best_model = ViT.load_from_checkpoint(\n",
    "                   checkpoint_path=best_model_path,\n",
    "                   input_d = input_size,\n",
    "                   batch_size=batch,\n",
    "                   n_patches = patches,\n",
    "                   n_blocks = blocks,\n",
    "                   hidden_d = hidden_dim,\n",
    "                   n_heads = heads,\n",
    "                   out_d = classes,\n",
    "                   lr = learning_rate,\n",
    "                   nepochs = number_epochs,\n",
    "                   warmup_epochs = warmup_epochs,\n",
    "                   wd = weight_decay,\n",
    "                   temp = temperature,\n",
    "                   cmlp_ratio = class_mlp_ratio,\n",
    "                   vmlp_ratio = vit_mlp_ratio,\n",
    "                   )\n",
    "\n",
    "# Access the Best Model's Accuracy.\n",
    "best_model_accuracy = trainer.checkpoint_callback.best_model_score.item()\n",
    "print(f\"Best Model Accuracy: {best_model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save it as a pth file.\n",
    "# Specify the path where you want to save the model.\n",
    "model_path = f\"./models/best_model_acc_{best_model_accuracy:.5f}.pth\"\n",
    "\n",
    "# Save the model's state dict to the specified file.\n",
    "torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "# Save it as a CheckPoint (Specific of PyTorch Lightning = Model State Dictionary + Training State + Optimizer State).\n",
    "# Specify the path where you want to save the model checkpoint.\n",
    "ckpt_path = f\"./models/best_model_acc_{best_model_accuracy:.5f}.ckpt\"\n",
    "\n",
    "# Save the model's state dict to the specified file.\n",
    "torch.save(best_model.state_dict(), ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
