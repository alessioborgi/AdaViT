{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): ViTEncoder(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (0): ViTBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): SelfAttention(\n",
      "          (self_attention): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ViTBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): SelfAttention(\n",
      "          (self_attention): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ViTBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): SelfAttention(\n",
      "          (self_attention): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (3): ViTBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): SelfAttention(\n",
      "          (self_attention): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import os, sys, torch\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../'))\n",
    "\n",
    "from AdaViT.models.vit import VisionTransformer\n",
    "\n",
    "vit = VisionTransformer(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=1000,\n",
    "    hidden_dim=768,\n",
    "    num_layers=4,\n",
    "    num_class_tokens=1,\n",
    "    num_heads=12,\n",
    "    mlp_dim=3072,\n",
    "    dropout=0.1)\n",
    "\n",
    "print(vit) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (5.21.0)\n",
      "Requirement already satisfied: packaging in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from plotly) (23.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from plotly) (8.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StudentT' object has no attribute 'prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_824/1521504338.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStudentT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplot_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/AdaViT/notebooks/../../AdaViT/utils/visualize.py\u001b[0m in \u001b[0;36mplot_distribution\u001b[0;34m(target_dist, x_min, x_max, num_bins)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# Calculate the probability density for each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m       \u001b[0mdensities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m       \u001b[0;31m# Define x-axis values based on distribution or provided limits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StudentT' object has no attribute 'prob'"
     ]
    }
   ],
   "source": [
    "from AdaViT.utils.visualize import plot_distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_dist = torch.distributions.StudentT(loc=7, scale=0.25, df=5)\n",
    "plot_distribution(target_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "logits = vit(x)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained ViT from torch or timm weights \n",
    "\n",
    "Make sure the dimensions of the transformer match with the weights you download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading timm pretrained weights:  ['facebookresearch/deit:main', 'deit_small_patch16_224']\n",
      "Downloading timm pretrained weights:  ['facebookresearch/deit:main', 'deit_small_patch16_224']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/studio-lab-user/.cache/torch/hub/facebookresearch_deit_main\n",
      "/home/studio-lab-user/.cache/torch/hub/facebookresearch_deit_main/models.py:63: UserWarning: Overwriting deit_tiny_patch16_224 in registry with models.deit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
      "/home/studio-lab-user/.cache/torch/hub/facebookresearch_deit_main/models.py:78: UserWarning: Overwriting deit_small_patch16_224 in registry with models.deit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_small_patch16_224(pretrained=False, **kwargs):\n",
      "/home/studio-lab-user/.cache/torch/hub/facebookresearch_deit_main/models.py:93: UserWarning: Overwriting deit_base_patch16_224 in registry with models.deit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_base_patch16_224(pretrained=False, **kwargs):\n",
      "/home/studio-lab-user/.cache/torch/hub/facebookresearch_deit_main/models.py:108: UserWarning: Overwriting deit_tiny_distilled_patch16_224 in registry with models.deit_tiny_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_tiny_distilled_patch16_224(pretrained=False, **kwargs):\n",
      "/home/studio-lab-user/.cache/torch/hub/facebookresearch_deit_main/models.py:123: UserWarning: Overwriting deit_small_distilled_patch16_224 in registry with models.deit_small_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_small_distilled_patch16_224(pretrained=False, **kwargs):\n",
      "/home/studio-lab-user/.cache/torch/hub/facebookresearch_deit_main/models.py:138: UserWarning: Overwriting deit_base_distilled_patch16_224 in registry with models.deit_base_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_base_distilled_patch16_224(pretrained=False, **kwargs):\n",
      "/home/studio-lab-user/.cache/torch/hub/facebookresearch_deit_main/models.py:153: UserWarning: Overwriting deit_base_patch16_384 in registry with models.deit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_base_patch16_384(pretrained=False, **kwargs):\n",
      "/home/studio-lab-user/.cache/torch/hub/facebookresearch_deit_main/models.py:168: UserWarning: Overwriting deit_base_distilled_patch16_384 in registry with models.deit_base_distilled_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_base_distilled_patch16_384(pretrained=False, **kwargs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for a different number of classes. Replacing head with random weights. You should fine-tune the model.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VisionTransformer:\n\tsize mismatch for class_tokens: copying a param with shape torch.Size([1, 1, 384]) from checkpoint, the shape in current model is torch.Size([1, 1, 192]).\n\tsize mismatch for conv_proj.weight: copying a param with shape torch.Size([384, 3, 16, 16]) from checkpoint, the shape in current model is torch.Size([192, 3, 16, 16]).\n\tsize mismatch for conv_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.pos_embedding: copying a param with shape torch.Size([1, 197, 384]) from checkpoint, the shape in current model is torch.Size([1, 197, 192]).\n\tsize mismatch for encoder.layers.0.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.0.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.0.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.0.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.0.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.0.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.0.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.0.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.0.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.0.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.0.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.0.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.1.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.1.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.1.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.1.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.1.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.1.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.2.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.2.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.2.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.2.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.2.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.2.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.3.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.3.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.3.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.3.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.3.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.3.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.4.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.4.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.4.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.4.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.4.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.4.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.5.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.5.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.5.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.5.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.5.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.5.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.6.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.6.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.6.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.6.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.6.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.6.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.7.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.7.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.7.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.7.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.7.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.7.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.8.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.8.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.8.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.8.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.8.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.8.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.9.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.9.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.9.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.9.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.9.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.9.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.10.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.10.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.10.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.10.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.10.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.10.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.11.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.11.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.11.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.11.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.11.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.11.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.ln.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.ln.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([10, 384]) from checkpoint, the shape in current model is torch.Size([10, 192]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1224/3402036447.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for timm any pretrained weights are supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m vit = VisionTransformer(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/AdaViT/notebooks/../../AdaViT/models/vit.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim, dropout, attention_dropout, num_classes, representation_size, num_registers, num_class_tokens, torch_pretrained_weights, timm_pretrained_weights, remove_layers)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_pretrained_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimm_pretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/AdaViT/notebooks/../../AdaViT/models/vit.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, torch_pretrained_weights, timm_pretrained_weights)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mtimm_pretrained_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimm_pretrained_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0madapted_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapt_timm_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimm_pretrained_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapted_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2154\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VisionTransformer:\n\tsize mismatch for class_tokens: copying a param with shape torch.Size([1, 1, 384]) from checkpoint, the shape in current model is torch.Size([1, 1, 192]).\n\tsize mismatch for conv_proj.weight: copying a param with shape torch.Size([384, 3, 16, 16]) from checkpoint, the shape in current model is torch.Size([192, 3, 16, 16]).\n\tsize mismatch for conv_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.pos_embedding: copying a param with shape torch.Size([1, 197, 384]) from checkpoint, the shape in current model is torch.Size([1, 197, 192]).\n\tsize mismatch for encoder.layers.0.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.0.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.0.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.0.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.0.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.0.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.0.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.0.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.0.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.0.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.0.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.0.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.1.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.1.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.1.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.1.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.1.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.1.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.1.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.2.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.2.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.2.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.2.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.2.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.2.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.2.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.3.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.3.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.3.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.3.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.3.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.3.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.3.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.4.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.4.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.4.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.4.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.4.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.4.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.4.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.5.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.5.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.5.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.5.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.5.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.5.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.5.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.6.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.6.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.6.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.6.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.6.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.6.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.6.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.7.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.7.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.7.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.7.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.7.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.7.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.7.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.8.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.8.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.8.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.8.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.8.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.8.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.8.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.9.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.9.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.9.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.9.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.9.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.9.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.9.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.10.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.10.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.10.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.10.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.10.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.10.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.10.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.ln_1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.self_attention.self_attention.in_proj_weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([576, 192]).\n\tsize mismatch for encoder.layers.11.self_attention.self_attention.in_proj_bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([576]).\n\tsize mismatch for encoder.layers.11.self_attention.self_attention.out_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for encoder.layers.11.self_attention.self_attention.out_proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.ln_2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.layers.11.mlp.fc1.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for encoder.layers.11.mlp.fc1.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layers.11.mlp.fc2.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for encoder.layers.11.mlp.fc2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.ln.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for encoder.ln.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([10, 384]) from checkpoint, the shape in current model is torch.Size([10, 192])."
     ]
    }
   ],
   "source": [
    "from torchvision.models.vision_transformer import ViT_B_16_Weights\n",
    "\n",
    "# see https://github.com/pytorch/vision/blob/806dba678d5b01f6e8a46f7c48fdf8c09369a267/torchvision/models/vision_transformer.py#L351\n",
    "# for timm any pretrained weights are supported\n",
    "\n",
    "vit = VisionTransformer(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=10,\n",
    "    hidden_dim=192,\n",
    "    num_layers=12,\n",
    "    num_class_tokens=1,\n",
    "    num_heads=3,\n",
    "    mlp_dim=768,\n",
    "    dropout=0.1,\n",
    "    #torch_pretrained_weights=str(ViT_B_16_Weights['IMAGENET1K_V1']),\n",
    "    timm_pretrained_weights=['facebookresearch/deit:main', 'deit_small_patch16_224']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also provide local paths to torch or timm pretrained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load state from checkpoint \n",
    "\n",
    "During training checkpoints are saved automatically, you can load them like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from AdaViT.utils.utils import load_state\n",
    "\n",
    "#model, optimizer, epoch, model_args, noise_args = load_state(path='../runs/2024-01-24-10-30-25/checkpoints/epoch_000.pth', model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pretrained Vit params into other Vit \n",
    "\n",
    "You can also initialize a vit with a different architecture from pre-trained weights, as long as dimensions match. If the model has different head dim or additional paramters, they will be randomly initalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../runs/2024-01-24-10-30-25/checkpoints/epoch_100.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1035/712637059.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrankvit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../runs/2024-01-24-10-30-25/checkpoints/epoch_100.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrankvit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrankvit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/AdaViT/notebooks/../../AdaViT/utils/utils.py\u001b[0m in \u001b[0;36mload_state\u001b[0;34m(path, model, optimizer, strict)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# create model based on saved state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../runs/2024-01-24-10-30-25/checkpoints/epoch_100.pth'"
     ]
    }
   ],
   "source": [
    "from AdaViT.models.rankvit import RankVisionTransformer\n",
    "\n",
    "rankvit = RankVisionTransformer(\n",
    "    image_size = 160, \n",
    "    patch_size  = 8, \n",
    "    num_classes  = 10, \n",
    "    hidden_dim = 256, \n",
    "    mlp_dim = 768, \n",
    "    num_layers = 4, \n",
    "    num_heads = 4\n",
    "    )\n",
    "\n",
    "rankvit, model_args, *_ = load_state(path='../runs/2024-01-24-10-30-25/checkpoints/epoch_100.pth', model=rankvit)\n",
    "rankvit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
