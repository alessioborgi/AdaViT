{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":109264,"sourceType":"datasetVersion","datasetId":56828},{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506},{"sourceId":2251350,"sourceType":"datasetVersion","datasetId":1354190},{"sourceId":7270893,"sourceType":"datasetVersion","datasetId":4214880},{"sourceId":7319863,"sourceType":"datasetVersion","datasetId":4247810}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TO-DO\n\nWe need to try/finish to do:\n- 1 Group: \n    - Implement attention map.\n    - Try patchyfing with convolutions.\n    - Find out how Tensorboard works. \n- 2 Group:\n    - Try Ensemble on Classification (Potential Bright Idea :) \n    - Try different versions of Attention (ARPR).\n    - Try different versions of Embedding (LaPE, 2D Positional Embeddings). \n\n- Others:\n    - Make a version with MNIST. \n    - Put LogSoftmax also in the MHSA? \n    - Retry to use Augmentation. \n    - Start writing Report. ","metadata":{}},{"cell_type":"markdown","source":"# (AdaViT) ADAPTIVE VISION TRANSFORMERS\n\n### SAPIENZA UNIVESITY of ROME (DEPARTMENT OF COMPUTER, CONTROL AND MANAGEMENT ENGINEEERING)\n\n### COURSE IN ARTIFICIAL INTELLIGENCE AND ROBOTICS (ACADEMIC YEAR 2023/2024)\n\n### NEURAL NETWORK's PROJECT\n- Professors: SIMONE SCARDAPANE, DANILO COMMINIELLO \n\n### AUTHORS:\n- **ALESSIO BORGI**\n    - University ID: 1952442\n    - Email: borgi.1952442@studenti.uniroma.it\n- **ANGELO GIANFELICE**\n    - University ID: 1851260\n    - Email: gianfelice.1851260@studenti.uniroma.it\n    \n### ","metadata":{}},{"cell_type":"markdown","source":"## TABLE OF CONTENTS:\n- **0: IMPORTING LIBRARIES AND SETTING THE SEEDS**\n- **1: DATASET INSPECTION**\n    - **1.1: CREATION OF THE LABEL DICTIONARY**\n    - **1.2: DISPLAYING EXAMPLES OF THE DATASET (ENCODED LABELS)**\n    - **1.3: DISPLAYING EXAMPLES OF THE DATASET (DECODED LABELS)**\n- **2:**\n- **3:**\n- **4:**\n- **5:** \n- **6:** ","metadata":{}},{"cell_type":"markdown","source":"## INTRODUCTION\n\n### (ViT) Vision Transformer's Idea\n\nIn this Notebook, we aim to present a clever implementation of **Vision Transformers (ViT)**. The Vision Transformer (ViT) represents a paradigm in computer vision by leveraging transformer-based architectures for **Image Classification** tasks. \n\nUnlike traditional **Convolutional Neural Networks (CNNs)**,which rely on local receptive fields (**Locality Principle**) and **Hierarchical Feature Extraction** through convolutional layers, ViT dispenses with the conventional convolutional layers and adopts a **Transformer Model**, originally designed for sequential data processing, such as in NLP(\"Natural Language Processing\"). \n\nIf we look this idea with a birds-eye point of view, we have that ViT's core idea involves treating an input image as a sequence of non-overlapping fixed-size patches, linearly embeds them, and appends learnable positional embeddings, that are added to encode spatial information. The resulting sequence of patch embeddings is then fed into a stack of transformer encoder blocks. Each transformer block consists of multi-head self-attention mechanisms and feedforward neural networks, enabling the model to capture both local and global contextual information, enhancing the model's ability to recognize complex patterns and relationships. \n\nOne **ViT's distinguishing Feature** is its ability to **capture Long-Range Dependencies in Images** through self-attention, facilitating understanding of the visual content. This stands in contrast to CNNs, which may struggle with information propagation across distant image regions due to the locality of convolutional operations. ViT's self-attention mechanism allows it to efficiently model relationships between distant patches, promoting effective feature learning for image recognition. Moreover, ViT exhibits promising **Scalability Advantages**, particularly when dealing with large datasets and high-resolution images. The absence of spatial hierarchies in ViT's architecture enables more straightforward parallelization, facilitating training on powerful hardware accelerators. This scalability, coupled with competitive performance on standard image classification benchmarks, positions ViT as a versatile and efficient alternative to traditional CNN-based approaches.\n\n\n### (AdaViT) Adaptive Vision Transformer's Modification\nDespite their remarkable advantages, Vision Transformers (ViTs) often face **Challenges** due to their fixed 2D positional encodings. This rigidity limits their ability to adapt to input image sizes and variations, hindering their applicability to real-world scenarios. To address these shortcomings, **Adaptive Vision Transformers (AdaViT)** emerge as a promising solution, introducing a **Dynamic Attention Mechanism** that allows the model to **selectively  take into account only Relevant Regions of the Input Image**, thereby overcoming the spatial resolution limitations of traditional ViTs. \n\nAdaViT enhances the vision transformer block by integrating an **Adaptive Halting Module**, which computes a **Halting Probability for each Token**. Remarkably, this module leverages the existing block parameters and utilizes a single neuron from the last dense layer in each block to calculate the halting probability, introducing **no additional Parameters** and **no additional Computational Overhead**. Tokens are discarded upon meeting the halting condition. Through the adaptive halting of tokens, we selectively perform dense computations only on those tokens considered informative for the given task. Consequently, as the vision transformer progresses through successive blocks, fewer tokens are processed, resulting in accelerated inference. \n\nThis improvements leads to immediate and significant **Speed-Up at Inference Time** on standard computational platforms, without the need for additional tuning.","metadata":{}},{"cell_type":"markdown","source":"### ABSTRACT & CONTRIBUTIONS\n\nThe **Vision Transformer (ViT)** has emerged as a powerful architecture for Image Classification, but it faces challenges related to computational efficiency and adaptability to varying levels of task complexity. In response to these challenges, we delve into **Adaptive Vision Transformer (A-ViT)**, a method designed to enhance the adaptability and computational efficiency of ViT models.\n\nThe core contribution of our work is...","metadata":{}},{"cell_type":"markdown","source":"### 0: IMPORTING LIBRARIES AND SETTING THE SEEDS\n\nIn this stage, we import essential libraries and modules, laying the groundwork for subsequent code implementation and experimentation. Additionally, this step emphasizes the establishment of seed values, practice done for ensuring reproducibility in data processing, model training, and evaluation. ","metadata":{}},{"cell_type":"code","source":"\n# Importing PyTorch-related Libraries.\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToPILImage\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchmetrics.classification import Accuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n\n# Importing PyTorch Lightning-Related Libraries.\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\nfrom pytorch_lightning.callbacks import TQDMProgressBar, LearningRateMonitor, ModelCheckpoint\n\n# Importing General Libraries.\nimport os\nimport csv\nimport PIL\nimport random\nimport numpy as np\nfrom PIL import Image\nimport seaborn as sns\nfrom pathlib import Path\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:40.566864Z","iopub.execute_input":"2024-01-04T21:00:40.567325Z","iopub.status.idle":"2024-01-04T21:00:40.578839Z","shell.execute_reply.started":"2024-01-04T21:00:40.567289Z","shell.execute_reply":"2024-01-04T21:00:40.577223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results.\n    \n    Arguments:\n        - seed {int} : Number of the seed.\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    pl.seed_everything(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n# Set the seed.\nseed_everything(31)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:40.581273Z","iopub.execute_input":"2024-01-04T21:00:40.581719Z","iopub.status.idle":"2024-01-04T21:00:40.598079Z","shell.execute_reply.started":"2024-01-04T21:00:40.581685Z","shell.execute_reply":"2024-01-04T21:00:40.596470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1: DATASET INSPECTION\n\nWe subsequently focus on experiencing a comprehensive exploration and understanding of the dataset at hand. This phase involves an in-depth examination of the data's structure, characteristics, and content. We delve into visualizing dataset images along with their corresponding labels. \n\n#### TINY IMAGENET\nThe **TinyImageNet** dataset is a subset of the 1.2 million images in ImageNet, specifically selected to be 20% as large as ImageNet while preserving the same distribution of classes. This makes it a more manageable dataset for our case where we have limited computational power, training and evaluating vision models, while still providing a challenging and diverse representation of real-world images.\n\nTinyImageNet consists of **200 Classes, each** containing **640 Images**. The class distribution is similar to that of ImageNet, with a slight overrepresentation of animals and objects commonly found in everyday life. The images are downsampled to **64x64** pixels, making them smaller and less computationally expensive to handle than full-resolution ImageNet images.\n","metadata":{}},{"cell_type":"markdown","source":"#### 1.1: CREATION OF THE LABEL DICTIONARY\n\nHere, we initiate the creation of a **Label Dictionary** to establish a meaningful mapping between encoded labels and their corresponding actual labels. The process extracts this from a file containing label information. The resulting dictionary serves as a key reference for interpreting encoded labels and facilitates a clearer understanding of the dataset's class labels. ","metadata":{}},{"cell_type":"code","source":"\n# Initialize the Mapping Dictionary to be empty.\nmapping_dict = {}\n\n# Open the file in read mode.\nwith open('/kaggle/input/tiny-imagenet/tiny-imagenet-200/words.txt', 'r') as file:\n    \n    # Read each line from the file.\n    for line in file:\n        # Split the line into tokens based on whitespace.\n        tokens = line.strip().split('\\t')\n        \n        # Check if there are at least two tokens.\n        if len(tokens) >= 2:\n            # Extract the encoded label (left) and actual label (right).\n            encoded_label, actual_label = tokens[0], tokens[1]\n            \n            # Add the mapping to the dictionary.\n            mapping_dict[encoded_label] = actual_label\n\n# Print the mapping dictionary.\n#print(mapping_dict)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:40.600227Z","iopub.execute_input":"2024-01-04T21:00:40.601896Z","iopub.status.idle":"2024-01-04T21:00:40.757348Z","shell.execute_reply.started":"2024-01-04T21:00:40.601827Z","shell.execute_reply":"2024-01-04T21:00:40.756395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2: DISPLAYING EXAMPLES OF THE DATASET (ENCODED LABELS)\n\nIn this exploration code, we aim to load and explore the **Tiny ImageNet** dataset. After loading the dataset, the class names and their respective counts are extracted. We visualize a grid containing ten randomly selected images from the dataset, showcasing both the image itself and its associated **Class Label (Encoded)**. ","metadata":{}},{"cell_type":"code","source":"\n# Loading the dataset using ImageFolder.\ndataset0 = datasets.ImageFolder(root=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", transform=None)\n\n# Extract class names and their counts.\nclass_names = dataset0.classes\nclass_counts = [dataset0.targets.count(i) for i in range(len(class_names))]\n\n# Setting the seed.\nnp.random.seed(31)\n\n# Create a grid of 10 images with labels.\nplt.figure(figsize=(15, 8))\n\nfor i in range(10):\n    \n    # Randomly select an image and its corresponding label.\n    index = np.random.randint(len(dataset0))\n    image, encoded_label = dataset0[index]\n    \n    # Look up the actual label using the mapping dictionary.\n    actual_label = mapping_dict.get(class_names[encoded_label], \"Unknown Label\")\n    \n    # Trim the label if it exceeds the maximum length.\n    actual_label_trimmed = actual_label[:15] + '...' if len(actual_label) > 15 else actual_label\n\n    # Display the image with its label..\n    plt.subplot(2, 5, i+1)\n    plt.imshow(np.array(image))  \n    plt.title(f\"Label: {actual_label_trimmed}\", wrap=True)\n    plt.axis('off')\n\n# Displaying Dataset examples.\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:40.760301Z","iopub.execute_input":"2024-01-04T21:00:40.761063Z","iopub.status.idle":"2024-01-04T21:00:43.347865Z","shell.execute_reply.started":"2024-01-04T21:00:40.761017Z","shell.execute_reply":"2024-01-04T21:00:43.346413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3: DISPLAYING EXAMPLES OF THE DATASET (DECODED LABELS)\n\nIn this exploration code, we aim to load and explore the **Tiny ImageNet** dataset, together with the **(Decoded) Class Labels**, making use of the Label Dictionary extracted in *step 1.1*. ","metadata":{}},{"cell_type":"code","source":"\n# Loading the dataset using ImageFolder.\ndataset0 = datasets.ImageFolder(root=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", transform=None)\n\n# Extract class names and their counts.\nclass_names = dataset0.classes\nclass_counts = [dataset0.targets.count(i) for i in range(len(class_names))]\n\n# Setting the seed.\nnp.random.seed(31)\n\n# Create a grid of 10 images with labels.\nplt.figure(figsize=(15, 8))\n\nfor i in range(10):\n    \n    # Randomly select an image and its corresponding label.\n    index = np.random.randint(len(dataset0))\n    image, encoded_label = dataset0[index]\n    \n    # Look up the actual label using the mapping dictionary.\n    actual_label = mapping_dict.get(class_names[encoded_label], \"Unknown Label\")\n    \n    # Trim the label if it exceeds the maximum length.\n    actual_label_trimmed = actual_label[:15] + '...' if len(actual_label) > 15 else actual_label\n\n    # Display the image with its label..\n    plt.subplot(2, 5, i+1)\n    plt.imshow(np.array(image))  \n    plt.title(f\"Label: {actual_label_trimmed}\", wrap=True)\n    plt.axis('off')\n\n# Displaying Dataset examples.\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:43.349869Z","iopub.execute_input":"2024-01-04T21:00:43.350343Z","iopub.status.idle":"2024-01-04T21:00:45.740509Z","shell.execute_reply.started":"2024-01-04T21:00:43.350302Z","shell.execute_reply":"2024-01-04T21:00:45.739172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2: DATA-MODULE DEFINITION","metadata":{}},{"cell_type":"markdown","source":"#### 2.0: CUSTOMIZED TRANSFORM CLASS","metadata":{}},{"cell_type":"code","source":"\nclass AdaViT_Transformations:\n    \n    def __init__(self):\n        \n        # Constructor - Nothing to initialize in this case\n        pass\n\n    def __call__(self, sample):\n        \"\"\"\n        Call method to perform transformations on the input sample.\n\n        Args:\n        - sample (PIL.Image.Image or torch.Tensor): Input image sample.\n\n        Returns:\n        - transformed_sample (torch.Tensor): Transformed image sample.\n        \"\"\"\n\n        # Define a series of image transformations using \"torchvision.Compose\" function.\n        transform = transforms.Compose([\n            transforms.ToTensor(),  \n            # Additional transformations can be added here.\n            # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  \n            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n        ])\n\n        # Apply the defined transformations to the input sample.\n        transformed_sample = transform(sample)\n\n        return transformed_sample","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:45.742708Z","iopub.execute_input":"2024-01-04T21:00:45.743278Z","iopub.status.idle":"2024-01-04T21:00:45.752544Z","shell.execute_reply.started":"2024-01-04T21:00:45.743224Z","shell.execute_reply":"2024-01-04T21:00:45.751252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1: CUSTOMIZED TRAINING SET VERSION","metadata":{}},{"cell_type":"code","source":"\nclass CustomTrainingTinyImagenet(ImageFolder):\n    \n    def __init__(self, root, transform=None):\n        \"\"\"\n        Custom dataset class for Tiny ImageNet Training data.\n\n        Args:\n        - root (str): Root directory containing the dataset.\n        - transform (callable, optional): Optional transform to be applied to the Input Image.\n        \"\"\"\n        super(CustomTrainingTinyImagenet, self).__init__(root, transform=transform)\n\n        # Create mappings between class labels and numerical indices\n        self.class_to_index = {cls: idx for idx, cls in enumerate(sorted(self.classes))}\n        self.index_to_class = {idx: cls for cls, idx in self.class_to_index.items()}\n\n    def __getitem__(self, index):\n        \"\"\"\n        Method to retrieve an item from the dataset.\n\n        Args:\n        - index (int): Index of the item to retrieve.\n\n        Returns:\n        - sample (torch.Tensor): Transformed image sample.\n        - target (int): Numerical index corresponding to the class label.\n        \"\"\"\n        # Retrieve the item and its label from the Dataset.\n        path, target = self.samples[index]\n\n        # Load the image using the default loader.\n        sample = self.loader(path)\n\n        # Apply the specified transformations, if any.\n        if self.transform is not None:\n            sample = self.transform(sample)\n\n        # Adjust the directory depth to get the target label.\n        target_str = os.path.basename(os.path.dirname(os.path.dirname(path)))\n\n        # Convert string label to numerical index using the mapping.\n        target = self.class_to_index[target_str]\n\n        return sample, target\n\n    def get_class_from_index(self, index):\n        \"\"\"\n        Method to retrieve the class label from a numerical index.\n\n        Args:\n        - index (int): Numerical index corresponding to the class label.\n\n        Returns:\n        - class_label (str): Class label corresponding to the numerical index.\n        \"\"\"\n        \n        return self.index_to_class[index]","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:45.754032Z","iopub.execute_input":"2024-01-04T21:00:45.754456Z","iopub.status.idle":"2024-01-04T21:00:45.770798Z","shell.execute_reply.started":"2024-01-04T21:00:45.754422Z","shell.execute_reply":"2024-01-04T21:00:45.769540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2: CUSTOMIZED VALIDATION SET VERSION","metadata":{}},{"cell_type":"code","source":"\nclass CustomValidationTinyImagenet(pl.LightningDataModule):\n    \n    def __init__(self, root, transform=None):\n        \"\"\"\n        Custom data module for Tiny ImageNet Validation data.\n\n        Args:\n        - root (str): Root directory containing the dataset.\n        - transform (callable, optional): Optional transform to be applied to the Input Image.\n        \"\"\"\n        self.root = Path(root)\n        self.transform = transform\n\n        # Load and preprocess labels\n        self.labels = self.load_labels()\n        self.label_to_index = {label: idx for idx, label in enumerate(sorted(set(self.labels.values())))}\n        self.index_to_label = {idx: label for label, idx in self.label_to_index.items()}\n\n    def load_labels(self):\n        \"\"\"\n        Method to load and Pre-Process Labels from the Validation Dataset.\n\n        Returns:\n        - labels (dict): Dictionary mapping image names to labels.\n        \"\"\"\n        label_path = \"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/val_annotations.txt\"\n        labels = {}\n\n        with open(label_path, \"r\") as f:\n            lines = f.readlines()\n\n        for line in lines:\n            parts = line.split(\"\\t\")\n            image_name, label = parts[0], parts[1]\n            labels[image_name] = label\n\n        return labels\n\n    def __len__(self):\n        \"\"\"\n        Method to get the length of the dataset.\n\n        Returns:\n        - length (int): Number of items in the dataset.\n        \"\"\"\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Method to retrieve an item from the dataset.\n\n        Args:\n        - index (int): Index of the item to retrieve.\n\n        Returns:\n        - image (torch.Tensor): Transformed image sample.\n        - label (int): Numerical index corresponding to the class label.\n        \"\"\"\n        image_name = f\"val_{index}.JPEG\"\n        image_path = self.root / image_name\n\n        # Open the image using PIL and convert to RGB.\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Apply the specified transformations, if any.\n        if self.transform:\n            image = self.transform(image)\n\n        # Use the get method to handle cases where the key is not present.\n        label_str = self.labels.get(image_name, 'Label not found')\n\n        # Convert string label to numerical index using the mapping.\n        label = self.label_to_index[label_str]\n\n        return image, label\n\n    def get_label_from_index(self, index):\n        \"\"\"\n        Method to retrieve the class label from a numerical index.\n\n        Args:\n        - index (int): Numerical index corresponding to the class label.\n\n        Returns:\n        - class_label (str): Class label corresponding to the numerical index.\n        \"\"\"\n        return self.index_to_label[index]","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:45.772672Z","iopub.execute_input":"2024-01-04T21:00:45.773185Z","iopub.status.idle":"2024-01-04T21:00:45.790988Z","shell.execute_reply.started":"2024-01-04T21:00:45.773138Z","shell.execute_reply":"2024-01-04T21:00:45.789489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3: GENERAL DATA-MODULE DEFINITION","metadata":{}},{"cell_type":"code","source":"\nclass AViT_DataModule(pl.LightningDataModule):\n    \n    def __init__(self, train_data_dir, val_data_dir, batch_size, num_workers=4):\n        \"\"\"\n        Custom data module for AViT model training and validation.\n\n        Args:\n        - train_data_dir (str): Directory path for the training dataset.\n        - val_data_dir (str): Directory path for the validation dataset.\n        - batch_size (int): Batch size for training and validation DataLoader.\n        - num_workers (int, optional): Number of workers for DataLoader (default is 4).\n        \"\"\"\n        super(AViT_DataModule, self).__init__()\n        self.train_data_dir = train_data_dir\n        self.val_data_dir = val_data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        # Use AdaViT transformations for data augmentation\n        self.transform = AdaViT_Transformations()\n\n    def setup(self, stage=None):\n        \"\"\"\n        Method to load and configure datasets for Training and Validation.\n\n        Args:\n        - stage (str, optional): 'fit' for Training and 'test' for Validation (default is None).\n        \"\"\"\n        # Load Train dataset using CustomTrainingTinyImagenet with the new directory structure.\n        self.train_dataset = CustomTrainingTinyImagenet(self.train_data_dir, transform=self.transform)\n\n        # Load Validation dataset.\n        self.val_dataset = CustomValidationTinyImagenet(self.val_data_dir, transform=self.transform)\n\n    def train_dataloader(self):\n        \"\"\"\n        Method to return the DataLoader for the Training Dataset.\n\n        Returns:\n        - train_dataloader (DataLoader): DataLoader for Training.\n        \"\"\"\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n\n    def val_dataloader(self):\n        \"\"\"\n        Method to return the DataLoader for the Validation Dataset.\n\n        Returns:\n        - val_dataloader (DataLoader): DataLoader for Validation.\n        \"\"\"\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:45.795681Z","iopub.execute_input":"2024-01-04T21:00:45.796084Z","iopub.status.idle":"2024-01-04T21:00:45.808405Z","shell.execute_reply.started":"2024-01-04T21:00:45.796050Z","shell.execute_reply":"2024-01-04T21:00:45.807320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.4: TESTING TRAINING AND VALIDATION DATALOADERS","metadata":{}},{"cell_type":"code","source":"\ndef show_images_labels(images, labels, title):\n    \"\"\"\n    Display Images with corresponding Labels.\n\n    Parameters:\n    - images (list of tensors): List of Image tensors.\n    - labels (list): List of corresponding Labels.\n    - title (str): Title for the entire subplot.\n\n    Returns:\n    None\n    \"\"\"\n    # Create a Subplot with 1 row and len(images) columns.\n    fig, axs = plt.subplots(1, len(images), figsize=(8, 4))\n    \n    # Set the title for the entire subplot.\n    fig.suptitle(title)\n\n    # Iterate over Images and Labels.\n    for i, (img, label) in enumerate(zip(images, labels)):\n        # Display each Image in a subplot.\n        axs[i].imshow(transforms.ToPILImage()(img))\n        \n        # Set the title for each subplot with the corresponding label.\n        axs[i].set_title(f\"Label: {label}\")\n        \n        # Turn off axis labels for better Visualization.\n        axs[i].axis('off')\n\n    # Show the entire subplot.\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:45.810164Z","iopub.execute_input":"2024-01-04T21:00:45.810520Z","iopub.status.idle":"2024-01-04T21:00:45.822801Z","shell.execute_reply.started":"2024-01-04T21:00:45.810489Z","shell.execute_reply":"2024-01-04T21:00:45.821706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define the AViT_DataModule.\ndata_module = AViT_DataModule(\n    train_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\",\n    val_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/images/\",\n    batch_size=512  \n)\n\n# Setup the Dataloaders.\ndata_module.setup()\n\n# Get a batch from the Training DataLoader.\ntrain_dataloader = data_module.train_dataloader()\ntrain_batch = next(iter(train_dataloader))\n\n# Get a batch from the Validation DataLoader.\nval_dataloader = data_module.val_dataloader()\nval_batch = next(iter(val_dataloader))\n\n# Show two Images from the Training Batch.\nshow_images_labels(train_batch[0][:2], train_batch[1][:2], title='Training Batch')\n\n# Show two Images from the  Validation Batch\nshow_images_labels(val_batch[0][:2], val_batch[1][:2], title='Validation Batch')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:45.824934Z","iopub.execute_input":"2024-01-04T21:00:45.825363Z","iopub.status.idle":"2024-01-04T21:00:52.202925Z","shell.execute_reply.started":"2024-01-04T21:00:45.825319Z","shell.execute_reply":"2024-01-04T21:00:52.201209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3: MODEL DEFINITION","metadata":{}},{"cell_type":"markdown","source":"#### 3.0: PATCHING FUNCTION DEFINITION","metadata":{}},{"cell_type":"code","source":"\ndef Make_Patches_from_Image(images, n_patches):\n    \"\"\"\n    Extract patches from input images.\n\n    Parameters:\n    - images (torch.Tensor): Input images tensor with shape (batch_size, channels, height, width).\n    - n_patches (int): Number of patches in each dimension.\n\n    Returns:\n    torch.Tensor: Extracted patches tensor with shape (batch_size, n_patches^2, patch_size^2 * channels).\n    \"\"\"\n    # Get the dimensions of the input images.\n    n, c, h, w = images.shape\n\n    # Ensure that the input images are square.\n    assert h == w, \"make_patches_from_image method is implemented for square images only!\"\n\n    # Initialize a tensor to store the extracted patches.\n    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n    patch_size = h // n_patches\n\n    # Loop over each image in the batch.\n    for idx, image in enumerate(images):\n        # Loop over each patch in both dimensions.\n        for i in range(n_patches):\n            for j in range(n_patches):\n                # Extract the patch from the image.\n                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n                # Flatten the patch and store it in the patches tensor.\n                patches[idx, i * n_patches + j] = patch.flatten()\n\n    return patches\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:52.205384Z","iopub.execute_input":"2024-01-04T21:00:52.205858Z","iopub.status.idle":"2024-01-04T21:00:52.215683Z","shell.execute_reply.started":"2024-01-04T21:00:52.205819Z","shell.execute_reply":"2024-01-04T21:00:52.214663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Helper function to Visualize Patches.\ndef visualize_patches(images, n_patches, title):\n    \"\"\"\n    Visualize patches extracted from Images.\n\n    Parameters:\n    - images (torch.Tensor): Input images tensor with shape (batch_size, channels, height, width).\n    - n_patches (int): Number of patches in each dimension.\n    - title (str): Title for the entire subplot.\n\n    Returns:\n    None\n    \"\"\"\n    # Extract patches from the input images using the make_patches_from_image function.\n    patches = Make_Patches_from_Image(images, n_patches)\n    \n    # Create a subplot for visualizing patches.\n    fig, axs = plt.subplots(n_patches, n_patches, figsize=(8, 8))\n    fig.suptitle(title)\n    \n    # Calculate the patch size based on the input images.\n    patch_size = images.shape[-1] // n_patches\n\n    # Loop over each patch in both dimensions.\n    for i in range(n_patches):\n        for j in range(n_patches):\n            # Calculate the index of the patch.\n            patch_index = i * n_patches + j\n            # Reshape each patch to (3, patch_size, patch_size).\n            patch = patches[0, patch_index].reshape(3, patch_size, patch_size).cpu().numpy()\n            # Display the patch in the subplot.\n            axs[i, j].imshow(patch.transpose(1, 2, 0))\n            axs[i, j].axis('off')\n\n    # Show the entire subplot.\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:52.217171Z","iopub.execute_input":"2024-01-04T21:00:52.217567Z","iopub.status.idle":"2024-01-04T21:00:52.233920Z","shell.execute_reply.started":"2024-01-04T21:00:52.217537Z","shell.execute_reply":"2024-01-04T21:00:52.232595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize patches for a Training Image.\nvisualize_patches(train_batch[0], n_patches=8, title='Training Patches')\n\n# Visualize patches for a Validation Image\nvisualize_patches(val_batch[0], n_patches=8, title='Validation Patches')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:52.235489Z","iopub.execute_input":"2024-01-04T21:00:52.235830Z","iopub.status.idle":"2024-01-04T21:00:59.219333Z","shell.execute_reply.started":"2024-01-04T21:00:52.235802Z","shell.execute_reply":"2024-01-04T21:00:59.217963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1: POSITIONAL EMBEDDING DEFINITION","metadata":{}},{"cell_type":"code","source":"def get_positional_embeddings_Basic(sequence_length, d):\n    result = torch.ones(sequence_length, d)\n    for i in range(sequence_length):\n        for j in range(d):\n            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:59.220759Z","iopub.execute_input":"2024-01-04T21:00:59.221169Z","iopub.status.idle":"2024-01-04T21:00:59.228499Z","shell.execute_reply.started":"2024-01-04T21:00:59.221095Z","shell.execute_reply":"2024-01-04T21:00:59.227061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.1.1: SINUSOIDAL POSITIONAL ENCODING (SPE)\n\n","metadata":{}},{"cell_type":"code","source":"\ndef get_positional_embeddings_SPE(sequence_length, d):\n    \"\"\"\n    Generate Positional Embeddings for the Transformer Model.\n\n    Parameters:\n    - sequence_length (int): Length of the input sequence.\n    - d (int): Dimension of the embeddings.\n\n    Returns:\n    torch.Tensor: Positional Embeddings tensor of shape (sequence_length, d).\n    \"\"\"\n    # Generate a tensor of positions from 0 to sequence_length - 1.\n    positions = torch.arange(0, sequence_length).float().view(-1, 1)\n    \n    # Calculate div_term for both sin and cos terms.\n    div_term = torch.exp(torch.arange(0, d, 2).float() * -(np.log(10000.0) / d))\n\n    # Initialize the embeddings tensor with zeros.\n    embeddings = torch.zeros(sequence_length, d)\n    \n    # Compute sin and cos terms and assign them to the embeddings tensor.\n    embeddings[:, 0::2] = torch.sin(positions / div_term)\n    embeddings[:, 1::2] = torch.cos(positions / div_term)\n\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:59.230600Z","iopub.execute_input":"2024-01-04T21:00:59.231812Z","iopub.status.idle":"2024-01-04T21:00:59.242974Z","shell.execute_reply.started":"2024-01-04T21:00:59.231767Z","shell.execute_reply":"2024-01-04T21:00:59.241904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.1.2: LAYER-ADAPTIVE POSITIONAL EMBEDDING (LaPE)","metadata":{}},{"cell_type":"code","source":"\ndef get_positional_embeddings_LaPE(sequence_length, d, num_layers):\n    \"\"\"\n    Generate Layer-adaptive Positional Embeddings for the Transformer Model.\n\n    Parameters:\n    - sequence_length (int): Length of the input sequence.\n    - d (int): Dimension of the embeddings.\n    - num_layers (int): Number of layers in the Transformer model.\n\n    Returns:\n    torch.Tensor: Layer-adaptive Positional Embeddings tensor of shape (sequence_length, d, num_layers).\n    \"\"\"\n    # Generate a tensor of positions from 0 to sequence_length - 1.\n    positions = torch.arange(0, sequence_length).float().view(-1, 1)\n\n    # Precompute div_terms for each layer.\n    div_terms = torch.exp(torch.arange(0, d, 2).float() * -(np.log(10000.0) / d))\n\n    # Initialize the embeddings tensor with zeros.\n    embeddings = torch.zeros(sequence_length, d, num_layers)\n\n    # Divide the sequence_length by 2 once for efficiency.\n    seq_len_div_2 = sequence_length // 2\n\n    # Compute sin and cos terms for each layer and assign them to the embeddings tensor.\n    for layer in range(num_layers):\n        embeddings[:, :, layer][:, 0:seq_len_div_2] = torch.sin(positions / div_terms[layer])\n        embeddings[:, :, layer][:, seq_len_div_2:] = torch.cos(positions / div_terms[layer])\n\n    return embeddings\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:59.245468Z","iopub.execute_input":"2024-01-04T21:00:59.246709Z","iopub.status.idle":"2024-01-04T21:00:59.259064Z","shell.execute_reply.started":"2024-01-04T21:00:59.246630Z","shell.execute_reply":"2024-01-04T21:00:59.257513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.1.2.1: IMAGE POSITIONAL EMBEDDINGS.","metadata":{}},{"cell_type":"markdown","source":"##### 3.1.3: VISUALIZE POSITIONAL EMBEDDINGS","metadata":{}},{"cell_type":"code","source":"\n# Helper function to Visualize Positional Embeddings.\ndef visualize_positional_embeddings(embeddings):\n    \"\"\"\n    Visualize the Positional Embeddings.\n\n    Parameters:\n    - embeddings (torch.Tensor): Positional embeddings tensor.\n\n    Returns:\n    None\n    \"\"\"\n    \n    # Get the number of dimensions (d) from the Embeddings Tensor.\n    d = embeddings.size(1)\n\n    # Set the figure size for a larger image.\n    plt.figure(figsize=(12, 6))\n\n    # Plot each dimension separately.\n    for i in range(d):\n        plt.plot(embeddings[:, i].numpy(), label=f'Dimension {i}')\n\n    # Set plot labels.\n    plt.xlabel('Position')\n    plt.ylabel('Embedding Value')\n    plt.title('Visualization of Positional Embeddings')\n\n    # Place the legend on the right and diminish its size.\n    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n    \n    # Show the plot.\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:59.260878Z","iopub.execute_input":"2024-01-04T21:00:59.261415Z","iopub.status.idle":"2024-01-04T21:00:59.275795Z","shell.execute_reply.started":"2024-01-04T21:00:59.261366Z","shell.execute_reply":"2024-01-04T21:00:59.274529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function to Visualize Positional Embeddings as a Heatmap.\ndef visualize_positional_embeddings_heatmap(embeddings):\n    \"\"\"\n    Visualize the Positional Embeddings as a Heatmap.\n\n    Parameters:\n    - embeddings (torch.Tensor): Positional embeddings tensor.\n\n    Returns:\n    None\n    \"\"\"\n    \n    # Get the number of dimensions (d) from the Embeddings Tensor.\n    d = embeddings.size(1)\n\n    # Set the figure size for a larger image.\n    plt.figure(figsize=(12, 6))\n\n    # Create a heatmap for the positional embeddings.\n    sns.heatmap(embeddings.T.numpy(), cmap='viridis', cbar_kws={'label': 'Embedding Value'})\n\n    # Set plot labels and title.\n    plt.xlabel('Position')\n    plt.ylabel('Dimension')\n    plt.title('Visualization of Positional Embeddings (Heatmap)')\n    \n    # Show the plot.\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:59.277561Z","iopub.execute_input":"2024-01-04T21:00:59.278395Z","iopub.status.idle":"2024-01-04T21:00:59.291726Z","shell.execute_reply.started":"2024-01-04T21:00:59.278359Z","shell.execute_reply":"2024-01-04T21:00:59.290198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positional_embeddings = get_positional_embeddings_SPE(65, 32)\nvisualize_positional_embeddings(positional_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:00:59.293272Z","iopub.execute_input":"2024-01-04T21:00:59.293899Z","iopub.status.idle":"2024-01-04T21:01:00.235496Z","shell.execute_reply.started":"2024-01-04T21:00:59.293863Z","shell.execute_reply":"2024-01-04T21:01:00.234223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positional_embeddings = get_positional_embeddings_Basic(65, 32)\nvisualize_positional_embeddings(positional_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:00.237245Z","iopub.execute_input":"2024-01-04T21:01:00.237610Z","iopub.status.idle":"2024-01-04T21:01:01.050839Z","shell.execute_reply.started":"2024-01-04T21:01:00.237578Z","shell.execute_reply":"2024-01-04T21:01:01.048511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positional_embeddings = get_positional_embeddings_LaPE(65, 32, 4)\nvisualize_positional_embeddings(positional_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:01.052688Z","iopub.execute_input":"2024-01-04T21:01:01.053126Z","iopub.status.idle":"2024-01-04T21:01:03.105288Z","shell.execute_reply.started":"2024-01-04T21:01:01.053075Z","shell.execute_reply":"2024-01-04T21:01:03.103820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positional_embeddings = get_positional_embeddings_SPE(65, 32)\nvisualize_positional_embeddings_heatmap(positional_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:03.107523Z","iopub.execute_input":"2024-01-04T21:01:03.107975Z","iopub.status.idle":"2024-01-04T21:01:03.981775Z","shell.execute_reply.started":"2024-01-04T21:01:03.107935Z","shell.execute_reply":"2024-01-04T21:01:03.980565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positional_embeddings = get_positional_embeddings_Basic(65, 32)\nvisualize_positional_embeddings_heatmap(positional_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:03.987268Z","iopub.execute_input":"2024-01-04T21:01:03.987664Z","iopub.status.idle":"2024-01-04T21:01:04.891362Z","shell.execute_reply.started":"2024-01-04T21:01:03.987632Z","shell.execute_reply":"2024-01-04T21:01:04.890201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2: MULTI-HEAD SELF-ATTENTION DEFINITION","metadata":{}},{"cell_type":"code","source":"\nclass MyMHSA(nn.Module):\n    \n    def __init__(self, d, n_heads=2):\n        \"\"\"\n        Multi-Head Self Attention (MHSA) Module.\n\n        Parameters:\n        - d (int): Dimension of the input tokens.\n        - n_heads (int): Number of attention heads.\n\n        Returns:\n        None\n        \"\"\"\n        \n        super(MyMHSA, self).__init__()\n        self.d = d\n        self.n_heads = n_heads\n\n        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n\n        # Split the dimension into n_heads parts.\n        d_head = int(d / n_heads)\n        \n        # Linear mappings for Query(q), Key(k), and Value(v) for each head.\n        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        \n        self.d_head = d_head\n        self.softmax = nn.Softmax(dim=-1)\n        \n        # Initialize weights.\n        self.initialize_weights_msa()\n        \n    def forward(self, sequences):\n        \"\"\"\n        Forward pass of the MHSA module.\n\n        Parameters:\n        - sequences (torch.Tensor): Input token sequences with shape (N, seq_length, token_dim).\n\n        Returns:\n        torch.Tensor: Output tensor after MHSA with shape (N, seq_length, item_dim).\n        \"\"\"\n        \n        result = []\n        for sequence in sequences:\n            \n            seq_result = []\n            for head in range(self.n_heads):\n                \n                # Compute the q,k,v for every head. \n                q_mapping, k_mapping, v_mapping = self.q_mappings[head], self.k_mappings[head], self.v_mappings[head]\n\n                # Extract the corresponding part of the sequence for the current head.\n                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n\n                # Calculate attention scores and apply softmax.\n                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n                seq_result.append(attention @ v)\n            \n            # Concatenate the results coming from the different Heads and Stack Vertically the result.\n            result.append(torch.hstack(seq_result))\n        \n        # Concatenate results for all the sequences.\n        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n    \n    def initialize_weights_msa(self):\n        \"\"\"\n        Initialize weights for linear layers in the MHSA module.\n\n        Parameters:\n        None\n\n        Returns:\n        None\n        \"\"\"\n        \n        # Initialize weights for the q, k, v values.\n        for q_mapping, k_mapping, v_mapping in zip(self.q_mappings, self.k_mappings, self.v_mappings):\n            nn.init.xavier_uniform_(q_mapping.weight)\n            nn.init.xavier_uniform_(k_mapping.weight)\n            nn.init.xavier_uniform_(v_mapping.weight)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:04.893916Z","iopub.execute_input":"2024-01-04T21:01:04.894546Z","iopub.status.idle":"2024-01-04T21:01:04.915708Z","shell.execute_reply.started":"2024-01-04T21:01:04.894496Z","shell.execute_reply":"2024-01-04T21:01:04.914522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3: ViT BLOCK DEFINITION","metadata":{}},{"cell_type":"code","source":"class MyViTBlock(nn.Module):\n    def __init__(self, hidden_d, n_heads, mlp_ratio=10):\n        super(MyViTBlock, self).__init__()\n        \n        self.hidden_d = hidden_d\n        self.n_heads = n_heads\n\n        self.norm1 = nn.LayerNorm(hidden_d)\n        self.mhsa = MyMHSA(hidden_d, n_heads)\n        self.norm2 = nn.LayerNorm(hidden_d)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n            nn.GELU(),\n            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n        )\n        \n        # Initialize weights.\n        self.initialize_weights_block()\n\n    def forward(self, x):\n        \n        out = x + self.mhsa(self.norm1(x))\n        out = out + self.mlp(self.norm2(out))\n        return out\n    \n    def initialize_weights_block(self):\n        \n        # Initialize weights for linear layers in mlp.\n        for layer in self.mlp:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:04.917340Z","iopub.execute_input":"2024-01-04T21:01:04.917719Z","iopub.status.idle":"2024-01-04T21:01:04.931789Z","shell.execute_reply.started":"2024-01-04T21:01:04.917686Z","shell.execute_reply":"2024-01-04T21:01:04.930693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4: ViT MODEL DEFINITION","metadata":{}},{"cell_type":"markdown","source":"##### 3.4.1: TARGET DISTRIBUTIONAL PRIOR DEFINITION","metadata":{}},{"cell_type":"code","source":"\ndef get_distribution_target(n_layers=4, exp_stop_depth=3):\n    \"\"\"\n    Generate the Target Distributional Prior.\n\n    Parameters:\n    - n_layers (int): Length of the distribution(number of transformers blocks or layers).\n    - exp_stop_depth (int): Depth of the target distribution(expected stopping depth).\n\n    Returns:\n    numpy.ndarray: Target distributional prior.\n    \"\"\"\n    \n    # Generate a series of values from 0 to length - 1.\n    data = np.arange(n_layers)\n    \n    # Generate a Gausian Normal Distribution centered around target_depth.\n    data = norm.pdf(data, loc=exp_stop_depth, scale=1)\n    \n    # Scale the distribution to have a maximum value of 1.\n    scaling_factor = (0.98) / sum(data[:exp_stop_depth])\n    data *= scaling_factor\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:04.933531Z","iopub.execute_input":"2024-01-04T21:01:04.933896Z","iopub.status.idle":"2024-01-04T21:01:04.949281Z","shell.execute_reply.started":"2024-01-04T21:01:04.933866Z","shell.execute_reply":"2024-01-04T21:01:04.948011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.4.2: MYVIT CLASS DEFINITION","metadata":{}},{"cell_type":"code","source":"\nclass MyViT(nn.Module):\n        \n    def __init__(self, chw, n_patches, n_blocks, hidden_d, n_heads, out_d):\n        \"\"\"\n        Initialize the MyViT model.\n\n        Parameters:\n        - chw (tuple): Input shape (C, H, W).\n        - n_patches (int): Number of patches.\n        - n_blocks (int): Number of transformer blocks.\n        - hidden_d (int): Dimension of the hidden layer.\n        - n_heads (int): Number of attention heads.\n        - out_d (int): Output dimension.\n        \"\"\"\n        \n        # Super Constructor.\n        super(MyViT, self).__init__()\n        \n        # Attributes.\n        self.chw = chw # ( C , H , W )\n        self.n_patches = n_patches\n        self.n_blocks = n_blocks\n        self.n_heads = n_heads\n        self.hidden_d = hidden_d\n        self.mlp_ratio=100\n        \n        # Halting Prior Distribution Loss and Target Distribution.\n        self.ponder_loss = 0\n        self.distr_prior_loss = 0\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n        self.distr_target = torch.Tensor(get_distribution_target())\n        \n        # Input and Patches Sizes.\n        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n\n        # 1) Linear Mapper.\n        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n        \n        # 2) Learnable Classification Token.\n        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n        \n        # 3) Positional Embedding.\n        self.register_buffer('positional_embeddings', get_positional_embeddings_SPE(n_patches ** 2 + 1, self.hidden_d), persistent=False)\n        \n        # 4) Transformer Encoder Blocks.\n        self.blocks = nn.ModuleList([MyViTBlock(self.hidden_d, self.n_heads) for _ in range(self.n_blocks)])\n        \n        # 5) Classification MLP.\n        self.mlp = nn.Sequential(\n            nn.Linear(self.hidden_d, self.mlp_ratio * self.hidden_d),\n            nn.GELU(),\n            nn.Linear(self.mlp_ratio * self.hidden_d, out_d),\n            nn.LogSoftmax(dim=-1)\n        )\n        \n        # Initialize weights.\n        self.initialize_weights()\n\n    def forward(self, images):\n        \"\"\"\n        Forward pass of the MyViT model.\n\n        Parameters:\n        - images (torch.Tensor): Input images tensor.\n\n        Returns:\n        torch.Tensor: Output tensor.\n        \"\"\"\n        \n        # Dividing Images into Patches.\n        n, c, h, w = images.shape\n        patches = Make_Patches_from_Image(images, self.n_patches).to(self.positional_embeddings.device)\n        \n        # Running Linear Layer Tokenization.\n        # Map the Vector corresponding to each patch to the Hidden Size Dimension.\n        tokens = self.linear_mapper(patches)\n        \n        # Adding Classification Token to the Tokens.\n        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n        \n        # Adding Positional Embedding.\n        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n        \n        ### Halting Procedure ###\n        total_token_count=len(out[1]) # out.shape = [512,65,32]\n        batch_size = out.size()[0]  # The batch size\n        cumul_token = torch.zeros(batch_size,total_token_count)\n        r = torch.ones(batch_size,total_token_count)\n        rho = torch.zeros(batch_size,total_token_count)\n        mask = torch.ones(batch_size,total_token_count)\n        \n        # Halting Hyperparameters.\n        gamma = 5\n        beta = -10\n        alpha_p = 5e-4\n        alpha_d = 0.1\n        eps = 0.01\n        threshold = 1 - eps\n        output = None  # Final output of Adaptive Vision Transformer Block.\n        halting_score_list=[] # List of Layer Halting Score Average.\n        \n        # Transformer Blocks.\n        for i,block in enumerate(self.blocks):\n            \n            # Previous Layers Token are masked.\n            out.data = out.data * mask.float().view(batch_size,total_token_count, 1)\n            \n            # Pass data trough each layer(block).\n            out = block(out.data) #out.shape = [512,65,32]\n            \n            # Compute Halting Scores.                 \n            t_0 = out[:,:,0] #out[:,:,0] = contains all the halting scores of images tokens ( t_O.shape = [512,65])\n            h_score = torch.sigmoid(gamma*t_0 + beta)\n           \n            # Update list with mean of halting score just computed.\n            halting_score_list.append(torch.mean(h_score[1:])) \n            \n            # Set all token halting score to one if we reached last layer(block).\n            if i == len(self.blocks)-1:\n                h_score = torch.ones(batch_size,total_token_count) \n                \n            # Last Layer Protection.\n            out = out *  mask.float().view(batch_size,total_token_count, 1) #out.shape = [512,65,32]\n            \n            # Update Accumulator.\n            cumul_token = cumul_token + h_score #cumul_token.shape = [512,65]\n            \n            #update rho.\n            rho = rho + mask.float() #rho.shape = [512,65]\n        \n            # Case 1: Threshold reached in this Iteration.\n            halted_token = cumul_token > threshold #shape [512,65]\n            halted_token = halted_token.float() * mask.float()  #shape [512,65]\n            out1 = out * r.view(batch_size, total_token_count, 1) * halted_token.view(batch_size, total_token_count, 1) # [512,65,32] * [512,65,1] * [512,65,1]\n            rho = rho + (r * halted_token)  #shape [512,65]\n\n            # Case 2: Threshold not reached.\n            not_halted_token = cumul_token < threshold\n            not_halted_token = not_halted_token.float()\n            r = r - (not_halted_token.float() * h_score)\n            out2 = out * h_score.view(batch_size, total_token_count, 1) * not_halted_token.view(batch_size, total_token_count, 1)\n            \n            # Update the mask.\n            mask = cumul_token < threshold\n            \n            if output is None:\n                output = out1 + out2\n            else:\n                output = output + (out1 + out2)\n                \n        # Halting Prior Distribution.\n        halting_distribution = torch.stack(halting_score_list)\n        halting_distribution /= torch.sum(halting_distribution)\n        halting_distribution = torch.clamp(halting_distribution, 0.01, 0.99) \n        \n        # Kullback-Leibler Divergence. \n        self.distr_prior_loss = alpha_d * self.kl_loss(halting_distribution, self.distr_target)\n        \n        # Ponder Loss.\n        self.ponder_loss = alpha_p * torch.mean(rho)\n        \n        # Getting the Classification Token only.\n        output = output[:, 0] #shape=[512,32]\n        \n        return self.mlp(output) # Map to output dimension (classification head).\n    \n\n    def initialize_weights(self):\n        \"\"\"\n        Initialize weights for linear layers, embeddings, etc.\n        \"\"\"\n        \n        # Initialize Weights for Linear Layers, Embeddings, etc.\n        nn.init.xavier_uniform_(self.linear_mapper.weight)\n        nn.init.normal_(self.class_token.data)\n\n        # Initialize Weights for Classification MLP.\n        nn.init.xavier_uniform_(self.mlp[0].weight)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:04.951232Z","iopub.execute_input":"2024-01-04T21:01:04.951927Z","iopub.status.idle":"2024-01-04T21:01:04.986333Z","shell.execute_reply.started":"2024-01-04T21:01:04.951892Z","shell.execute_reply":"2024-01-04T21:01:04.984908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.5: GENERAL AViT MODEL DEFINITION","metadata":{}},{"cell_type":"code","source":"\nclass AViT_Model(MyViT, pl.LightningModule):\n    \n    def __init__(self, input_d, n_patches, n_blocks, hidden_d, n_heads, out_d):\n        \"\"\"\n        Initialize the AViT_Model, a LightningModule using MyViT as a base.\n\n        Parameters:\n        - input_d (int): Dimension of the input.\n        - n_patches (int): Number of patches.\n        - n_blocks (int): Number of transformer blocks.\n        - hidden_d (int): Dimension of the hidden layer.\n        - n_heads (int): Number of attention heads.\n        - out_d (int): Output dimension.\n        \"\"\"\n        super(AViT_Model, self).__init__(input_d, n_patches, n_blocks, hidden_d, n_heads, out_d)\n\n        # Definition of the Cross Entropy Loss.\n        self.loss = CrossEntropyLoss()\n\n        # Definition of Accuracies, F1Score, Precision, and Recall Metrics.\n        self.acc_top1 = Accuracy(task=\"multiclass\", num_classes=out_d)\n        self.acc_top3 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=3)\n        self.acc_top5 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=5)\n        self.acc_top10 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=10)\n        self.f1score = MulticlassF1Score(num_classes=out_d, average='macro')\n        self.precision = MulticlassPrecision(num_classes=out_d, average='macro')\n        self.recall = MulticlassRecall(num_classes=out_d, average='macro')\n\n        # Definition of lists to be used in the \"on_ ... _epoch_end\" functions.\n        self.training_step_outputs = []\n        self.validation_step_outputs = []\n        self.test_step_outputs = []\n\n    def _step(self, batch):\n        \"\"\"\n        Common computation of the metrics among Training, Validation, and Test Set.\n\n        Parameters:\n        - batch (tuple): Input batch tuple.\n\n        Returns:\n        tuple: Tuple containing loss and various metrics.\n        \"\"\"\n        x, y = batch\n        preds = self(x)\n        loss = self.loss(preds, y) + self.ponder_loss + self.distr_prior_loss\n        acc1 = self.acc_top1(preds, y)\n        acc3 = self.acc_top3(preds, y)\n        acc5 = self.acc_top5(preds, y)\n        acc10 = self.acc_top10(preds, y)\n        f1score = self.f1score(preds, y)\n        precision = self.precision(preds, y)\n        recall = self.recall(preds, y)\n\n        return loss, acc1, acc3, acc5, acc10, f1score, precision, recall\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Training step function.\n\n        Parameters:\n        - batch (tuple): Input batch tuple.\n        - batch_idx (int): Batch index.\n\n        Returns:\n        torch.Tensor: Training loss.\n        \"\"\"\n        # Compute the Training Loss and Accuracy.\n        loss, acc, _, _, _, _, _, _ = self._step(batch)\n\n        # Create a Dictionary to represent the output of the Training step.\n        training_step_output = {\n            \"train_loss\": loss.item(),\n            \"train_acc\": acc.item()\n        }\n\n        # Append the dictionary to the list.\n        self.training_step_outputs.append(training_step_output)\n\n        # Perform logging.\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        Validation step function.\n\n        Parameters:\n        - batch (tuple): Input batch tuple.\n        - batch_idx (int): Batch index.\n\n        Returns:\n        None\n        \"\"\"\n        # Compute the Validation Loss and Accuracy.\n        loss, acc1, acc3, acc5, acc10, _, _, _ = self._step(batch)\n\n        # Create a Dictionary to represent the output of the validation step.\n        validation_step_output = {\n            \"val_loss\": loss.item(),\n            \"val_acc\": acc1.item(),\n            \"val_acc_3\": acc3.item(),\n            \"val_acc_5\": acc5.item(),\n            \"val_acc_10\": acc10.item(),\n        }\n\n        # Append the dictionary to the list.\n        self.validation_step_outputs.append(validation_step_output)\n\n        # Perform logging.\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc\", acc1, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc_3\", acc3, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc_5\", acc5, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc_10\", acc10, on_epoch=True, prog_bar=True, logger=True)\n\n    def on_validation_epoch_end(self):\n        \"\"\"\n        Method called at the end of the validation epoch.\n\n        Returns:\n        None\n        \"\"\"\n        # Calculate the Mean Loss and Accuracy from the list of dictionaries.\n        loss_tot = torch.tensor([item[\"val_loss\"] for item in self.validation_step_outputs]).mean()\n        acc_tot = torch.tensor([item[\"val_acc\"] for item in self.validation_step_outputs]).mean()\n        acc_tot_3 = torch.tensor([item[\"val_acc_3\"] for item in self.validation_step_outputs]).mean()\n        acc_tot_5 = torch.tensor([item[\"val_acc_5\"] for item in self.validation_step_outputs]).mean()\n        acc_tot_10 = torch.tensor([item[\"val_acc_10\"] for item in self.validation_step_outputs]).mean()\n\n        # Log the mean values.\n        self.log(\"val_loss\", loss_tot)\n        self.log(\"val_acc\", acc_tot)\n        self.log(\"val_acc_3\", acc_tot_3)\n        self.log(\"val_acc_5\", acc_tot_5)\n        self.log(\"val_acc_10\", acc_tot_10)\n\n        # Print messages.\n        message_loss = f'Epoch {self.current_epoch} Validation Loss -> {loss_tot}'\n        message_accuracy = f'      Validation Accuracy -> {acc_tot}'\n        message_accuracy_3 = f'      Validation Accuracy Top-3 -> {acc_tot_3}'\n        message_accuracy_5 = f'      Validation Accuracy Top-5-> {acc_tot_5}'\n        message_accuracy_10 = f'      Validation Accuracy Top-10-> {acc_tot_10}'\n        print(message_loss + message_accuracy + message_accuracy_3 + message_accuracy_5 + message_accuracy_10)\n\n        # Clear the list to free memory.\n        self.validation_step_outputs.clear()\n\n\n    def configure_optimizers(self):\n        \"\"\"\n        Configure the optimizer.\n\n        Returns:\n        torch.optim.Optimizer: The optimizer.\n        \"\"\"\n        # Configure the Adam Optimizer.\n        optimizer = optim.Adam(self.parameters(), lr=1.5e-3, weight_decay=1.5e-4)\n\n        # Configure the Cosine Annealing Learning Rate Scheduler.\n        # scheduler = CosineAnnealingLR(optimizer, T_max=5, eta_min=1.5e-6)\n\n        # return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n        return optimizer\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:04.988615Z","iopub.execute_input":"2024-01-04T21:01:04.989515Z","iopub.status.idle":"2024-01-04T21:01:05.021903Z","shell.execute_reply.started":"2024-01-04T21:01:04.989471Z","shell.execute_reply":"2024-01-04T21:01:05.020875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4: MODEL TRAINING","metadata":{}},{"cell_type":"markdown","source":"#### 4.1: CALLBACKS DEFINITION","metadata":{}},{"cell_type":"code","source":"# Checkpoint CallBack Definition.\nmy_checkpoint_call = ModelCheckpoint(\n    dirpath=\"/kaggle/working/checkpoints/\",\n    filename=\"Best_Model\",\n    monitor=\"val_acc\",\n    mode=\"max\",\n    save_top_k=1,\n    save_last=True\n)\n\n# Learning Rate CallBack Definition.\nmy_lr_monitor_call = LearningRateMonitor(logging_interval=\"epoch\")\n\n# Early Stopping CallBack Definition.\nmy_early_stopping_call = pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=30, mode=\"min\", min_delta=0.001)\n\n# Progress Bar CallBack Definition.\nmy_progress_bar_call = TQDMProgressBar(refresh_rate=10)\n\n# TensorBoardLogger CallBack Definition.\ntb_logger = TensorBoardLogger(save_dir=\"/kaggle/working/logs\", name=\"AViT\")\n\n# CSV CallBack Definition.\ncsv_logger = CSVLogger(\"/kaggle/working/logs\", name=\"AViT\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:05.023529Z","iopub.execute_input":"2024-01-04T21:01:05.024699Z","iopub.status.idle":"2024-01-04T21:01:05.040828Z","shell.execute_reply.started":"2024-01-04T21:01:05.024654Z","shell.execute_reply":"2024-01-04T21:01:05.039537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2: MODEL INSTANTIATION & TRAINING","metadata":{}},{"cell_type":"code","source":"\n# Instantiate the Adaptive Vision Transformer Model.\nmodel = AViT_Model((3, 64, 64), \n                   n_patches=8, \n                   n_blocks=4, \n                   hidden_d=32, \n                   n_heads=4, \n                   out_d = 200)\n\ndatamodule = AViT_DataModule(train_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", \n                             val_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/images/\", \n                             batch_size=512)\n\n# Setup the Dataloaders. \ndata_module.setup()\n\n# Create a PyTorch Lightning Trainer.\ntrainer = pl.Trainer(\n    max_epochs=25,\n    accelerator=\"auto\", \n    devices=\"auto\",\n    log_every_n_steps=1,\n    logger=tb_logger,\n    callbacks=[my_progress_bar_call,\n               my_checkpoint_call,\n               my_lr_monitor_call,\n               my_early_stopping_call,\n               ]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:05.042472Z","iopub.execute_input":"2024-01-04T21:01:05.042834Z","iopub.status.idle":"2024-01-04T21:01:05.944836Z","shell.execute_reply.started":"2024-01-04T21:01:05.042804Z","shell.execute_reply":"2024-01-04T21:01:05.943495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrainer.fit(model, datamodule)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T21:01:05.946430Z","iopub.execute_input":"2024-01-04T21:01:05.946896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5: BEST MODEL EXTRAPOLATION","metadata":{}},{"cell_type":"code","source":"# Get the path of the best Model.\nbest_model_path = my_checkpoint_call.best_model_path\n\n# Load the best model from the Checkpoint.\nbest_model = AViT_Model.load_from_checkpoint(\n    checkpoint_path=best_model_path,\n    input_d=(3, 64, 64),\n    n_patches=8,\n    n_blocks=4,\n    hidden_d=32,\n    n_heads=4,\n    out_d=200\n)\n\n# Access the Best Model's Accuracy.\nbest_model_accuracy = trainer.checkpoint_callback.best_model_score.item()\nprint(f\"Best Model Accuracy: {best_model_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6: SAVING THE BEST MODEL","metadata":{}},{"cell_type":"code","source":"# Save it as a pth file.\n# Specify the path where you want to save the model.\nmodel_path = f\"/kaggle/working/best_model_acc_{best_model_accuracy:.5f}.pth\"\n\n# Save the model's state dict to the specified file.\ntorch.save(best_model.state_dict(), model_path)\n\n# Save it as a CheckPoint (Specific of PyTorch Lightning = Model State Dictionary + Training State + Optimizer State).\n# Specify the path where you want to save the model checkpoint.\nckpt_path = f\"/kaggle/working/best_model_acc_{best_model_accuracy:.5f}.ckpt\"\n\n# Save the model's state dict to the specified file.\ntorch.save(best_model.state_dict(), ckpt_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7: TRAINING FROM A SAVED CHECKPOINT","metadata":{}},{"cell_type":"markdown","source":"# Load the Best Model from the Checkpoint.\ncheckpoint = torch.load(\"/kaggle/input/adavit-model-checkpoints/best_model_acc.pth\")\n\n# 'Checkpoint' is an OrderedDict or an odict_keys Object.\ncheckpoint_keys = list(checkpoint.keys()) if isinstance(checkpoint, OrderedDict) else checkpoint.keys()\n\n# Convert checkpoint_keys to a Dictionary.\nstate_dict = {key: checkpoint[key] for key in checkpoint_keys}\n\n# Instantiate the Loaded Model (same schema as the checkpoint).\nloaded_model = AViT_Model((3, 64, 64), \n                   n_patches=8, \n                   n_blocks=4, \n                   hidden_d=32, \n                   n_heads=4, \n                   out_d = 200)\n\n# Now, load the state_dict into the Model. \nloaded_model.load_state_dict(state_dict)","metadata":{}},{"cell_type":"markdown","source":"\n# Resume the Trainer from the last Checkpoint.\nresume_trainer = pl.Trainer(\n    max_epochs=1,\n    accelerator=\"auto\", \n    devices=\"auto\",\n    log_every_n_steps=1,\n    logger=tb_logger,\n    callbacks=[my_progress_bar_call,\n               my_checkpoint_call,\n               my_lr_monitor_call,\n               my_early_stopping_call,\n               ]\n)\n\n# Train the Model.\nresume_trainer.fit(new_model, datamodule)","metadata":{}},{"cell_type":"markdown","source":"### 8: EXPERIMENTS & RESULTS","metadata":{}},{"cell_type":"markdown","source":"# Results AdaViT with resized images (224x224): (self.mlp_ratio = 50 and eps = 0.5 and exp_stop_depth = 1)\n- Epoch 0 Validation Loss -> 4.93233585357666      Validation Accuracy -> 0.03160041198134422      Validation Accuracy Top-3 -> 0.0744427889585495       Validation Accuracy Top-5-> 0.11160960048437119      Validation Accuracy Top-10-> 0.18392692506313324\n- Epoch 4 Validation Loss -> 4.874544143676758      Validation Accuracy -> 0.036310892552137375      Validation Accuracy Top-3 -> 0.09129710495471954   Validation Accuracy Top-5-> 0.13243336975574493      Validation Accuracy Top-10-> 0.21198873221874237\n- Epoch 9 Validation Loss -> 4.814379692077637      Validation Accuracy -> 0.04263556748628616      Validation Accuracy Top-3 -> 0.1028837338089943     Validation Accuracy Top-5-> 0.1462373584508896      Validation Accuracy Top-10-> 0.23507583141326904\n- Epoch 15 Validation Loss -> 4.776520729064941      Validation Accuracy -> 0.046668197959661484      Validation Accuracy Top-3 -> 0.11104090511798859   Validation Accuracy Top-5-> 0.1590360701084137      Validation Accuracy Top-10-> 0.24327895045280457\n\n# Results AdaViT with hidden_d = 192: (self.mlp_ratio = default(100) and eps = default(0.01) and batch_size = 50)\n- Epoch 0 Validation Loss -> 4.723392963409424      Validation Accuracy -> 0.05169999971985817      Validation Accuracy Top-3 -> 0.11590000241994858     Validation Accuracy Top-5-> 0.1664000004529953      Validation Accuracy Top-10-> 0.2556000053882599\n- Epoch 4 Validation Loss -> 4.3997578620910645      Validation Accuracy -> 0.0828000009059906      Validation Accuracy Top-3 -> 0.1834000051021576     Validation Accuracy Top-5-> 0.251800000667572      Validation Accuracy Top-10-> 0.36180001497268677\n- Epoch 9 Validation Loss -> 4.2333807945251465      Validation Accuracy -> 0.10979999601840973      Validation Accuracy Top-3 -> 0.22219999134540558   Validation Accuracy Top-5-> 0.29259997606277466      Validation Accuracy Top-10-> 0.4101000130176544\n- Epoch 14 Validation Loss -> 4.199795722961426      Validation Accuracy -> 0.11620000004768372      Validation Accuracy Top-3 -> 0.2290000021457672     Validation Accuracy Top-5-> 0.3011000156402588      Validation Accuracy Top-10-> 0.42090004682540894\n","metadata":{}},{"cell_type":"markdown","source":"### 9: CONCLUSIONS & FUTURE WORK","metadata":{}},{"cell_type":"markdown","source":"### 10: REFERENCES\n\n- https://arxiv.org/pdf/2112.07658.pdf\n- https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c","metadata":{}}]}