{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":109264,"sourceType":"datasetVersion","datasetId":56828},{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506},{"sourceId":2251350,"sourceType":"datasetVersion","datasetId":1354190}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TO-DO\n- Refactor the names/ add comments to better understand. \n- Set predefined Transforms, for both Training (ToTensor + Standardize +  Data Augmentation)\n- Set predefined Transforms, Validation & Test (ToTensor + Standardize) \n- Add the possibility to save weights and restart the training from them.\n- implement attention map.\n- increase n_heads,n_blocks\n- try logSoftmax\n- Implement Halting Thing: assign to each token an halting score $h_{k}^{l}$ where k= k-th token and l = layers depth  ","metadata":{}},{"cell_type":"markdown","source":"https://arxiv.org/pdf/2112.07658.pdf\nhttps://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c","metadata":{}},{"cell_type":"markdown","source":"### 0: IMPORTING LIBRARIES AND SETTING THE SEEDS","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import ImageFolder\nfrom torchmetrics.classification import Accuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n\nfrom pathlib import Path\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.callbacks import TQDMProgressBar, LearningRateMonitor, ModelCheckpoint\nfrom torch.optim.lr_scheduler import StepLR\n\nfrom typing import Tuple\nimport PIL\nimport random\nfrom PIL import Image\nimport csv\nimport numpy as np\nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.utils.class_weight import compute_class_weight\nimport math\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:09.310475Z","iopub.execute_input":"2023-12-24T19:05:09.311708Z","iopub.status.idle":"2023-12-24T19:05:21.376855Z","shell.execute_reply.started":"2023-12-24T19:05:09.311661Z","shell.execute_reply":"2023-12-24T19:05:21.375573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results.\n    \n    Arguments:\n        - seed {int} : Number of the seed.\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    pl.seed_everything(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set the seed.\nseed_everything(31)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:21.379175Z","iopub.execute_input":"2023-12-24T19:05:21.379725Z","iopub.status.idle":"2023-12-24T19:05:21.395575Z","shell.execute_reply.started":"2023-12-24T19:05:21.379691Z","shell.execute_reply":"2023-12-24T19:05:21.394330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1: DATA INSPECTION","metadata":{}},{"cell_type":"markdown","source":"#### 1.1: CREATION OF THE LABEL DICTIONARY","metadata":{}},{"cell_type":"code","source":"mapping_dict = {}\n\n# Open the file in read mode.\nwith open('/kaggle/input/tiny-imagenet/tiny-imagenet-200/words.txt', 'r') as file:\n    \n    # Read each line from the file.\n    for line in file:\n        # Split the line into tokens based on whitespace.\n        tokens = line.strip().split('\\t')\n        \n        # Check if there are at least two tokens.\n        if len(tokens) >= 2:\n            # Extract the encoded label (left) and actual label (right).\n            encoded_label, actual_label = tokens[0], tokens[1]\n            \n            # Add the mapping to the dictionary.\n            mapping_dict[encoded_label] = actual_label\n\n# Print the mapping dictionary.\n#print(mapping_dict)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:21.396944Z","iopub.execute_input":"2023-12-24T19:05:21.397348Z","iopub.status.idle":"2023-12-24T19:05:21.572739Z","shell.execute_reply.started":"2023-12-24T19:05:21.397313Z","shell.execute_reply":"2023-12-24T19:05:21.571372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2: DISPLAYING EXAMPLES OF THE DATASET","metadata":{}},{"cell_type":"code","source":"# Loading the dataset using ImageFolder.\ndataset0 = datasets.ImageFolder(root=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", transform=None)\n\n# Extract class names and their counts.\nclass_names = dataset0.classes\nclass_counts = [dataset0.targets.count(i) for i in range(len(class_names))]\nnp.random.seed(31)\n\n# Create a grid of 10 images with labels.\nplt.figure(figsize=(15, 8))\nfor i in range(10):\n    \n    # Randomly select an image and its corresponding label.\n    index = np.random.randint(len(dataset0))\n    image, label = dataset0[index]\n\n    # Display the image with its label\n    plt.subplot(2, 5, i+1)\n    plt.imshow(np.array(image))  # Convert the PIL Image to a numpy array\n    plt.title(f\"Label: {class_names[label]}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:21.574581Z","iopub.execute_input":"2023-12-24T19:05:21.574978Z","iopub.status.idle":"2023-12-24T19:05:44.221773Z","shell.execute_reply.started":"2023-12-24T19:05:21.574944Z","shell.execute_reply":"2023-12-24T19:05:44.220119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3: DISPLAYING EXAMPLES OF THE DATASET WITH DECODED LABELS","metadata":{}},{"cell_type":"code","source":"# Assuming you have already loaded the dataset using ImageFolder\ndataset0 = datasets.ImageFolder(root=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", transform=None)\n\n# Extract class names and their counts\nclass_names = dataset0.classes\nclass_counts = [dataset0.targets.count(i) for i in range(len(class_names))]\n\nnp.random.seed(31)\n# Create a grid of 10 images with labels\nplt.figure(figsize=(15, 8))\nfor i in range(10):\n    # Randomly select an image and its corresponding label\n    index = np.random.randint(len(dataset0))\n    image, encoded_label = dataset0[index]\n    # Look up the actual label using the mapping dictionary\n    actual_label = mapping_dict.get(class_names[encoded_label], \"Unknown Label\")\n    \n    # Trim the label if it exceeds the maximum length.\n    actual_label_trimmed = actual_label[:15] + '...' if len(actual_label) > 15 else actual_label\n\n    # Display the image with its label.\n    plt.subplot(2, 5, i+1)\n    plt.imshow(np.array(image))  # Convert the PIL Image to a numpy array\n    plt.title(f\"Label: {actual_label_trimmed}\", wrap=True)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:44.225902Z","iopub.execute_input":"2023-12-24T19:05:44.227053Z","iopub.status.idle":"2023-12-24T19:05:46.908117Z","shell.execute_reply.started":"2023-12-24T19:05:44.226997Z","shell.execute_reply":"2023-12-24T19:05:46.907228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2: DATA-MODULE DEFINITION","metadata":{}},{"cell_type":"markdown","source":"#### 2.0: CUSTOMIZED TRANSFORM CLASS","metadata":{}},{"cell_type":"code","source":"class AdaViT_Transformations:\n    \n    def __init__(self):\n        pass\n        #self.image_size = image_size\n\n    def __call__(self, sample):\n        \n        # Implementation of a series of Customized Transformations.\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            #transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        return transform(sample)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:46.909538Z","iopub.execute_input":"2023-12-24T19:05:46.910467Z","iopub.status.idle":"2023-12-24T19:05:46.916390Z","shell.execute_reply.started":"2023-12-24T19:05:46.910429Z","shell.execute_reply":"2023-12-24T19:05:46.915161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1: CUSTOMIZED TRAINING SET VERSION","metadata":{}},{"cell_type":"code","source":"class CustomTrainingTinyImagenet(ImageFolder):\n    \n    def __init__(self, root, transform=None):\n        super(CustomTrainingTinyImagenet, self).__init__(root, transform=transform)\n        self.class_to_index = {cls: idx for idx, cls in enumerate(sorted(self.classes))}\n        self.index_to_class = {idx: cls for cls, idx in self.class_to_index.items()}\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n\n        # Adjust the directory depth\n        target_str = os.path.basename(os.path.dirname(os.path.dirname(path)))  # Remove two os.path.dirname\n\n        # Convert string label to numerical index\n        target = self.class_to_index[target_str]\n\n        return sample, target\n\n    def get_class_from_index(self, index):\n        return self.index_to_class[index]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:46.918396Z","iopub.execute_input":"2023-12-24T19:05:46.918761Z","iopub.status.idle":"2023-12-24T19:05:46.941492Z","shell.execute_reply.started":"2023-12-24T19:05:46.918731Z","shell.execute_reply":"2023-12-24T19:05:46.940321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2: CUSTOMIZED VALIDATION SET VERSION","metadata":{}},{"cell_type":"code","source":"class CustomValidationTinyImagenet(pl.LightningDataModule):\n    \n    def __init__(self, root, transform=None):\n        self.root = Path(root)\n        self.transform = transform\n        self.labels = self.load_labels()\n        self.label_to_index = {label: idx for idx, label in enumerate(sorted(set(self.labels.values())))}\n        self.index_to_label = {idx: label for label, idx in self.label_to_index.items()}\n\n    def load_labels(self):\n        label_path = \"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/val_annotations.txt\"\n        labels = {}\n\n        with open(label_path, \"r\") as f:\n            lines = f.readlines()\n\n        for line in lines:\n            parts = line.split(\"\\t\")\n            image_name, label = parts[0], parts[1]\n            labels[image_name] = label\n\n        return labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        image_name = f\"val_{index}.JPEG\"\n        image_path = self.root / image_name\n\n        image = Image.open(image_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        # Use get method to handle cases where the key is not present\n        label_str = self.labels.get(image_name, 'Label not found')\n\n        # Convert string label to numerical index\n        label = self.label_to_index[label_str]\n\n        return image, label\n\n    def get_label_from_index(self, index):\n        return self.index_to_label[index]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:46.943486Z","iopub.execute_input":"2023-12-24T19:05:46.943968Z","iopub.status.idle":"2023-12-24T19:05:46.961470Z","shell.execute_reply.started":"2023-12-24T19:05:46.943922Z","shell.execute_reply":"2023-12-24T19:05:46.960016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3: GENERAL DATA-MODULE DEFINITION","metadata":{}},{"cell_type":"code","source":"class AViT_DataModule(pl.LightningDataModule):\n    \n    def __init__(self, train_data_dir, val_data_dir, batch_size, num_workers=4):\n        super(AViT_DataModule, self).__init__()\n        self.train_data_dir = train_data_dir\n        self.val_data_dir = val_data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        \n        self.transform = AdaViT_Transformations()\n\n    def setup(self, stage=None):\n        # Load Train dataset using CustomTrainingTinyImagenet with the new directory structure\n        self.train_dataset = CustomTrainingTinyImagenet(self.train_data_dir, transform=self.transform)\n\n        # Load Validation dataset\n        self.val_dataset = CustomValidationTinyImagenet(self.val_data_dir, transform=self.transform)\n\n    def train_dataloader(self):\n        # Return the DataLoader for the training dataset\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n\n    def val_dataloader(self):\n        # Return the DataLoader for the validation dataset\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:46.963308Z","iopub.execute_input":"2023-12-24T19:05:46.963684Z","iopub.status.idle":"2023-12-24T19:05:46.978590Z","shell.execute_reply.started":"2023-12-24T19:05:46.963637Z","shell.execute_reply":"2023-12-24T19:05:46.977053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.4: TESTING TRAINING AND VALIDATION DATALOADERS","metadata":{}},{"cell_type":"code","source":"# Define the AViT_DataModule\ndata_module = AViT_DataModule(\n    train_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\",\n    val_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/images/\",\n    batch_size=512  # Adjust the batch size as needed\n)\n\n# Setup the dataloaders\ndata_module.setup()\n\n# Get a batch from the training DataLoader\ntrain_dataloader = data_module.train_dataloader()\ntrain_batch = next(iter(train_dataloader))\n\n# Get a batch from the validation DataLoader\nval_dataloader = data_module.val_dataloader()\nval_batch = next(iter(val_dataloader))\n\n# Display images and labels\ndef show_images_labels(images, labels, title):\n    fig, axs = plt.subplots(1, len(images), figsize=(8, 4))\n    fig.suptitle(title)\n\n    for i, (img, label) in enumerate(zip(images, labels)):\n        axs[i].imshow(transforms.ToPILImage()(img))\n        axs[i].set_title(f\"Label: {label}\")\n        axs[i].axis('off')\n    \n    plt.show()\n\n# Show two images from the training batch\nshow_images_labels(train_batch[0][:2], train_batch[1][:2], title='Training Batch')\n\n# Show two images from the validation batch\nshow_images_labels(val_batch[0][:2], val_batch[1][:2], title='Validation Batch')","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:46.980561Z","iopub.execute_input":"2023-12-24T19:05:46.981017Z","iopub.status.idle":"2023-12-24T19:05:55.398928Z","shell.execute_reply.started":"2023-12-24T19:05:46.980905Z","shell.execute_reply":"2023-12-24T19:05:55.397441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3: MODEL DEFINITION","metadata":{}},{"cell_type":"markdown","source":"#### 3.0: PATCHING FUNCTION DEFINITION","metadata":{}},{"cell_type":"code","source":"def Make_Patches_from_Image(images, n_patches):\n    n, c, h, w = images.shape\n\n    assert h == w, \"Make_Patches_from_Image method is implemented for square images only\"\n\n    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n    patch_size = h // n_patches\n\n    for idx, image in enumerate(images):\n        for i in range(n_patches):\n            for j in range(n_patches):\n                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n                patches[idx, i * n_patches + j] = patch.flatten()\n    return patches","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:55.401505Z","iopub.execute_input":"2023-12-24T19:05:55.402039Z","iopub.status.idle":"2023-12-24T19:05:55.410630Z","shell.execute_reply.started":"2023-12-24T19:05:55.401985Z","shell.execute_reply":"2023-12-24T19:05:55.409700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function to visualize patches\ndef visualize_patches(images, n_patches, title):\n    patches = Make_Patches_from_Image(images, n_patches)\n    \n    fig, axs = plt.subplots(n_patches, n_patches, figsize=(8, 8))\n    fig.suptitle(title)\n    \n    patch_size = images.shape[-1] // n_patches\n\n    for i in range(n_patches):\n        for j in range(n_patches):\n            patch_index = i * n_patches + j\n            # Reshape each patch to (3, patch_size, patch_size)\n            patch = patches[0, patch_index].reshape(3, patch_size, patch_size).cpu().numpy()\n            axs[i, j].imshow(patch.transpose(1, 2, 0))\n            axs[i, j].axis('off')\n\n    plt.show()\n\n# Visualize patches for a training image\nvisualize_patches(train_batch[0], n_patches=4, title='Training Patches')\n\n# Visualize patches for a validation image\nvisualize_patches(val_batch[0], n_patches=4, title='Validation Patches')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:55.411943Z","iopub.execute_input":"2023-12-24T19:05:55.412774Z","iopub.status.idle":"2023-12-24T19:05:56.790595Z","shell.execute_reply.started":"2023-12-24T19:05:55.412738Z","shell.execute_reply":"2023-12-24T19:05:56.789308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1: POSITIONAL EMBEDDING DEFINITION","metadata":{}},{"cell_type":"code","source":"def get_positional_embeddings(sequence_length, d):\n    result = torch.ones(sequence_length, d)\n    for i in range(sequence_length):\n        for j in range(d):\n            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:56.791872Z","iopub.execute_input":"2023-12-24T19:05:56.792248Z","iopub.status.idle":"2023-12-24T19:05:56.799929Z","shell.execute_reply.started":"2023-12-24T19:05:56.792217Z","shell.execute_reply":"2023-12-24T19:05:56.798761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2: MULTI-HEAD SELF-ATTENTION DEFINITION","metadata":{}},{"cell_type":"code","source":"class MyMSA(nn.Module):\n    def __init__(self, d, n_heads=2):\n        super(MyMSA, self).__init__()\n        self.d = d\n        self.n_heads = n_heads\n\n        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n\n        d_head = int(d / n_heads)\n        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.d_head = d_head\n        self.softmax = nn.Softmax(dim=-1)\n        \n        # Initialize weights.\n        self.initialize_weights_msa()\n        \n    def forward(self, sequences):\n        # Sequences has shape (N, seq_length, token_dim)\n        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n        result = []\n        for sequence in sequences:\n            seq_result = []\n            for head in range(self.n_heads):\n                q_mapping = self.q_mappings[head]\n                k_mapping = self.k_mappings[head]\n                v_mapping = self.v_mappings[head]\n\n                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n\n                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n                seq_result.append(attention @ v)\n            result.append(torch.hstack(seq_result))\n        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n    \n    def initialize_weights_msa(self):\n        # Initialize weights for linear layers\n        for q_mapping, k_mapping, v_mapping in zip(self.q_mappings, self.k_mappings, self.v_mappings):\n            nn.init.xavier_uniform_(q_mapping.weight)\n            nn.init.xavier_uniform_(k_mapping.weight)\n            nn.init.xavier_uniform_(v_mapping.weight)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:56.805447Z","iopub.execute_input":"2023-12-24T19:05:56.805882Z","iopub.status.idle":"2023-12-24T19:05:56.822981Z","shell.execute_reply.started":"2023-12-24T19:05:56.805845Z","shell.execute_reply":"2023-12-24T19:05:56.821747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3: ViT BLOCK DEFINITION","metadata":{}},{"cell_type":"code","source":"class MyViTBlock(nn.Module):\n    def __init__(self, hidden_d, n_heads, mlp_ratio=10):\n        super(MyViTBlock, self).__init__()\n        \n        self.hidden_d = hidden_d\n        self.n_heads = n_heads\n\n        self.norm1 = nn.LayerNorm(hidden_d)\n        self.mhsa = MyMSA(hidden_d, n_heads)\n        self.norm2 = nn.LayerNorm(hidden_d)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n            nn.GELU(),\n            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n        )\n        \n        # Initialize weights.\n        self.initialize_weights_block()\n\n    def forward(self, x):\n        \n        out = x + self.mhsa(self.norm1(x))\n        out = out + self.mlp(self.norm2(out))\n        return out\n    \n    def initialize_weights_block(self):\n        \n        # Initialize weights for linear layers in mlp.\n        for layer in self.mlp:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:56.824784Z","iopub.execute_input":"2023-12-24T19:05:56.825276Z","iopub.status.idle":"2023-12-24T19:05:56.839234Z","shell.execute_reply.started":"2023-12-24T19:05:56.825228Z","shell.execute_reply":"2023-12-24T19:05:56.838048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4: ViT MODEL DEFINITION","metadata":{}},{"cell_type":"code","source":"class MyViT(nn.Module):\n        \n    def __init__(self, chw, n_patches, n_blocks, hidden_d, n_heads, out_d):\n        # Super constructor\n        super(MyViT, self).__init__()\n        \n        # Attributes\n        self.chw = chw # ( C , H , W )\n        self.n_patches = n_patches\n        self.n_blocks = n_blocks\n        self.n_heads = n_heads\n        self.hidden_d = hidden_d\n        self.mlp_ratio=100\n        \n        # Input and patches sizes\n        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n\n        # 1) Linear mapper\n        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n        \n        # 2) Learnable classification token\n        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n        \n        # 3) Positional embedding\n        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n        \n        # 4) Transformer encoder blocks\n        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n        \n        # 5) Classification MLPk\n        self.mlp = nn.Sequential(\n            nn.Linear(self.hidden_d, self.mlp_ratio * self.hidden_d),\n            nn.GELU(),\n            nn.Linear(self.mlp_ratio * self.hidden_d, out_d),\n            nn.LogSoftmax(dim=-1)\n        )\n        \n        # Initialize weights\n        self.initialize_weights()\n\n    def forward(self, images):\n        # Dividing images into patches\n        n, c, h, w = images.shape\n        patches = Make_Patches_from_Image(images, self.n_patches).to(self.positional_embeddings.device)\n        \n        # Running linear layer tokenization\n        # Map the vector corresponding to each patch to the hidden size dimension\n        tokens = self.linear_mapper(patches)\n        \n        # Adding classification token to the tokens\n        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n        \n        # Adding positional embedding\n        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n        \n        # Transformer Blocks\n        for block in self.blocks:\n            out = block(out)\n            \n        # Getting the classification token only\n        out = out[:, 0]\n        \n        return self.mlp(out) # Map to output dimension, output category distribution\n    \n\n    def initialize_weights(self):\n        \n        # Initialize weights for linear layers, embeddings, etc.\n        nn.init.xavier_uniform_(self.linear_mapper.weight)\n        nn.init.normal_(self.class_token.data)\n\n        # Initialize weights for classification MLP\n        nn.init.xavier_uniform_(self.mlp[0].weight)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:56.843213Z","iopub.execute_input":"2023-12-24T19:05:56.843800Z","iopub.status.idle":"2023-12-24T19:05:56.862087Z","shell.execute_reply.started":"2023-12-24T19:05:56.843763Z","shell.execute_reply":"2023-12-24T19:05:56.860731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.5: GENERAL AViT MODEL DEFINITION","metadata":{}},{"cell_type":"code","source":"class AViT_Model(MyViT, pl.LightningModule):\n    \n    def __init__(self, input_d, n_patches, n_blocks, hidden_d, n_heads, out_d):\n        \n        super(AViT_Model, self).__init__(input_d, n_patches, n_blocks, hidden_d, n_heads, out_d)\n\n        # Definition of the Weighted Cross Entropy Loss + Label Smoothing.\n        self.loss = CrossEntropyLoss()\n\n        # Definition of Accuracy, F1Score, Precision and Recall Metrics.\n        self.acc = Accuracy(task=\"multiclass\", num_classes=out_d)\n        self.f1score = MulticlassF1Score(num_classes=out_d, average='macro')\n        self.precision = MulticlassPrecision(num_classes=out_d, average='macro')\n        self.recall = MulticlassRecall(num_classes=out_d, average='macro')\n        \n        # Definition of lists to be used in the \"on_ ... _epoch_end\" functions.\n        self.training_step_outputs = []\n        self.validation_step_outputs = []\n        self.test_step_outputs = []\n    \n    def _step(self, batch):\n        \n        # Common computation of the metrics among Training, Validation and Test Set.\n        x, y = batch\n        preds = self(x)\n        loss = self.loss(preds, y)\n        acc = self.acc(preds, y)\n        f1score = self.f1score(preds, y)\n        precision = self.precision(preds, y)\n        recall = self.recall(preds, y)\n        \n        return loss, acc, f1score, precision, recall\n    \n    \n    def training_step(self, batch, batch_idx):\n        \n        # Compute the Training Loss and Accracy.\n        loss, acc, _, _, _ = self._step(batch)\n\n        # Create a dictionary to represent the output of the training step.\n        training_step_output = {\n            \"train_loss\": loss.item(),\n            \"train_acc\": acc.item()\n        }\n\n        # Append the dictionary to the list.\n        self.training_step_outputs.append(training_step_output)\n\n        # Perform logging.\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \n        # Compute the Validation Loss and Accuracy.\n        loss, acc, _, _, _ = self._step(batch)\n\n        # Create a dictionary to represent the output of the validation step.\n        validation_step_output = {\n            \"val_loss\": loss.item(),\n            \"val_acc\": acc.item()\n        }\n\n        # Append the dictionary to the list.\n        self.validation_step_outputs.append(validation_step_output)\n\n        # Perform logging.\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n\n    def on_validation_epoch_end(self):\n        \n        # Calculate the Mean Loss and Accuracy from the list of dictionaries.\n        loss_tot = torch.tensor([item[\"val_loss\"] for item in self.validation_step_outputs]).mean()\n        acc_tot = torch.tensor([item[\"val_acc\"] for item in self.validation_step_outputs]).mean()\n\n        # Log the mean values.\n        self.log(\"val_loss\", loss_tot)\n        self.log(\"val_acc\", acc_tot)\n\n        # Print messages.\n        message_loss = f'Epoch {self.current_epoch} Validation Loss -> {loss_tot}'\n        message_accuracy = f'      Validation Accuracy -> {acc_tot}'\n        print(message_loss + message_accuracy)\n\n        # Clear the list to free memory.\n        self.validation_step_outputs.clear()\n\n    \n    def configure_optimizers(self):\n        \n        # Configure the Adam Optimizer.\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n        #scheduler = StepLR(optimizer, step_size=10, gamma=0.5)  # Adjust parameters as needed\n        #return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n        \n        return optimizer\n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:56.863927Z","iopub.execute_input":"2023-12-24T19:05:56.864727Z","iopub.status.idle":"2023-12-24T19:05:56.888191Z","shell.execute_reply.started":"2023-12-24T19:05:56.864681Z","shell.execute_reply":"2023-12-24T19:05:56.886863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4: MODEL TRAINING","metadata":{}},{"cell_type":"markdown","source":"#### 4.1: CALLBACKS DEFINITION","metadata":{}},{"cell_type":"code","source":"# Checkpoint CallBack Definition.\nmy_checkpoint_call = ModelCheckpoint(\n    dirpath=\"/kaggle/working/checkpoints/\",\n    filename=\"Best_Model\",\n    monitor=\"val_loss\",\n    mode=\"min\",\n    save_top_k=1\n)\n\n# Learning Rate CallBack Definition.\nmy_lr_monitor_call = LearningRateMonitor(logging_interval=\"epoch\")\n\n# Early Stopping CallBack Definition.\nmy_early_stopping_call = pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=30, mode=\"min\", min_delta=0.001)\n\n# Progress Bar CallBack Definition.\nmy_progress_bar_call = TQDMProgressBar(refresh_rate=10)\n\n# TensorBoardLogger CallBack Definition.\ntb_logger = TensorBoardLogger(\"/kaggle/working/logs\", name=\"AViT\")\n\n# CSV CallBack Definition.\ncsv_logger = CSVLogger(\"/kaggle/working/logs\", name=\"AViT\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:56.889651Z","iopub.execute_input":"2023-12-24T19:05:56.890671Z","iopub.status.idle":"2023-12-24T19:05:56.966524Z","shell.execute_reply.started":"2023-12-24T19:05:56.890619Z","shell.execute_reply":"2023-12-24T19:05:56.965293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2: MODEL INSTANTIATION & TRAINING","metadata":{}},{"cell_type":"code","source":"# Initialize the model and data module.\n\n# Image Flattened Dimension = 64*64*3 = 4096*3 = 12288\n# n_patches = 16\n# chw = (3, 64, 64)\n# patch_size = (3, 64/16, 64/16) = (3, 4, 4) = (#channels, #h_patch, #w_patch)\n# 256 patch of 48 elements = 256 * 48 = 12288\n# hidden_d = linear mapping output dimension\n\n# n_block = how many times the Transformer Block is repeated. \n# n_heads = how many times we repeat self-attention.\n# out_d = number of output classes.\n\nmodel = AViT_Model((3, 64, 64), \n                   n_patches=8, \n                   n_blocks=4, \n                   hidden_d=32, \n                   n_heads=4, \n                   out_d = 200)\n\ndatamodule = AViT_DataModule(train_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", \n                             val_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/images/\", \n                             batch_size=512)\n\n# Setup the dataloaders\ndata_module.setup()\n\n# Create a PyTorch Lightning Trainer.\ntrainer = pl.Trainer(\n    max_epochs=25,\n    accelerator=\"auto\", \n    devices=\"auto\",\n    log_every_n_steps=1,\n    logger=tb_logger,\n    callbacks=[my_progress_bar_call,\n               my_checkpoint_call,\n               my_lr_monitor_call,\n               my_early_stopping_call,\n               ]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:56.968068Z","iopub.execute_input":"2023-12-24T19:05:56.968549Z","iopub.status.idle":"2023-12-24T19:05:59.209201Z","shell.execute_reply.started":"2023-12-24T19:05:56.968515Z","shell.execute_reply":"2023-12-24T19:05:59.207951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrainer.fit(model, datamodule)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:05:59.210641Z","iopub.execute_input":"2023-12-24T19:05:59.210996Z","iopub.status.idle":"2023-12-24T19:27:04.467471Z","shell.execute_reply.started":"2023-12-24T19:05:59.210955Z","shell.execute_reply":"2023-12-24T19:27:04.466124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(result)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T20:13:39.639695Z","iopub.execute_input":"2023-12-24T20:13:39.640167Z","iopub.status.idle":"2023-12-24T20:13:39.934000Z","shell.execute_reply.started":"2023-12-24T20:13:39.640126Z","shell.execute_reply":"2023-12-24T20:13:39.932378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5: BEST MODEL EXTRAPOLATION","metadata":{}},{"cell_type":"code","source":"best_model_path = my_checkpoint_call.best_model_path\nbest_model = AViT_Model.load_from_checkpoint(checkpoint_path=best_model_path,\n                                             input_d=(3, 64, 64), \n                                             n_patches=8, \n                                             n_blocks=4, \n                                             hidden_d=32,\n                                             n_heads=4, \n                                             out_d = 200)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:27:04.478790Z","iopub.execute_input":"2023-12-24T19:27:04.479315Z","iopub.status.idle":"2023-12-24T19:27:05.535786Z","shell.execute_reply.started":"2023-12-24T19:27:04.479266Z","shell.execute_reply":"2023-12-24T19:27:05.534266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6: SAVING THE BEST MODEL","metadata":{}},{"cell_type":"code","source":"# Save it as a pth file.\n# Specify the path where you want to save the model.\nmodel_path = f\"/kaggle/working/best_model_acc.pth\"\n\n# Save the model's state dict to the specified file.\ntorch.save(best_model.state_dict(), model_path)\n\n# Save it as a CheckPoint (Specific of PyTorch Lightning = Model State Dictionary + Training State + Optimizer State)\n# Specify the path where you want to save the model checkpoint.\nckpt_path = f\"/kaggle/working/best_model_acc.ckpt\"\n\n# Save the model's state dict to the specified file\ntorch.save(best_model.state_dict(), ckpt_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:27:05.537840Z","iopub.execute_input":"2023-12-24T19:27:05.538272Z","iopub.status.idle":"2023-12-24T19:27:05.869860Z","shell.execute_reply.started":"2023-12-24T19:27:05.538234Z","shell.execute_reply":"2023-12-24T19:27:05.868297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7: TRAINING FROM A SAVED CHECKPOINT","metadata":{}},{"cell_type":"markdown","source":"# Specify the path to the checkpoint file\ncheckpoint_path = \"/kaggle/working/best_model_acc_0.001.ckpt\"\n\n# Create an instance of your Lightning model (AViT_Model in this case)\nloaded_model = AViT_Model.load_from_checkpoint(checkpoint_path=checkpoint_path)\n\ndatamodule = AViT_DataModule(train_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", \n                             val_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/images/\", \n                             batch_size=512)\n\n# Specify other Trainer configurations\ntrainer = Trainer(\n    max_epochs=5,\n    accelerator=\"auto\", \n    devices=\"auto\",\n    log_every_n_steps=1,\n    logger=tb_logger,\n    callbacks=[my_progress_bar_call,\n               my_checkpoint_call,\n               my_lr_monitor_call,\n               my_early_stopping_call,\n               ]\n    \n    resume_from_checkpoint=checkpoint_path \n)\n\n# Start training\nresult = trainer.fit(loaded_model, datamodule)\n","metadata":{}},{"cell_type":"markdown","source":"best_model_path = my_checkpoint_call.best_model_path\nbest_model = AViT_Model.load_from_checkpoint(checkpoint_path=best_model_path, \n                                             input_d=(3, 64, 64), \n                                             n_patches=4, \n                                             n_blocks=1, \n                                             hidden_d=1, \n                                             n_heads=1, \n                                             out_d = 200)","metadata":{}},{"cell_type":"markdown","source":"# Save it as a pth file.\n# Specify the path where you want to save the model.\nmodel_path = f\"/kaggle/working/best_model_acc_{result[0]['val_acc']:.5f}.pth\"\n\n# Save the model's state dict to the specified file.\ntorch.save(best_model.state_dict(), model_path)\n\n# Save it as a CheckPoint (Specific of PyTorch Lightning = Model State Dictionary + Training State + Optimizer State)\n# Specify the path where you want to save the model checkpoint.\nckpt_path = f\"/kaggle/working/best_model_acc_{result[0]['val_acc']:.5f}.ckpt\"\n\n# Save the model's state dict to the specified file\ntorch.save(best_model.state_dict(), ckpt_path)","metadata":{}}]}