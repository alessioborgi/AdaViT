{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":109264,"sourceType":"datasetVersion","datasetId":56828},{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506},{"sourceId":2251350,"sourceType":"datasetVersion","datasetId":1354190}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TO-DO\n- Refactor the names/ add comments to better understand. \n- Add the possibility to save weights and restart the training from them.\n- Implement attention map.\n- Find out how Tensorboard works. \n- Ty Ensemble on Classification (Potential Bright Idea :) \n- try patchyfing with convolutions","metadata":{}},{"cell_type":"markdown","source":"### QUESTIONS/DOUBTS\n- 3. A-ViT, second column, after equation (4): ...\"We incorporate H(.) into the existing Vision trasnformer block by allocating a single neuron in the MLP layer to do the task\". ","metadata":{}},{"cell_type":"markdown","source":"### REFERENCES\n\n- https://arxiv.org/pdf/2112.07658.pdf\n- https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c","metadata":{}},{"cell_type":"markdown","source":"### 0: IMPORTING LIBRARIES AND SETTING THE SEEDS","metadata":{}},{"cell_type":"code","source":"\n# Importing PyTorch-related libraries\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToPILImage\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchmetrics.classification import Accuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n\n# Importing PyTorch Lightning-Related libraries\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\nfrom pytorch_lightning.callbacks import TQDMProgressBar, LearningRateMonitor, ModelCheckpoint\n\n# Importing General Libraries\nimport os\nimport csv\nimport PIL\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:50.040053Z","iopub.execute_input":"2023-12-30T16:56:50.041872Z","iopub.status.idle":"2023-12-30T16:56:50.051778Z","shell.execute_reply.started":"2023-12-30T16:56:50.041812Z","shell.execute_reply":"2023-12-30T16:56:50.049925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results.\n    \n    Arguments:\n        - seed {int} : Number of the seed.\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    pl.seed_everything(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set the seed.\nseed_everything(31)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:50.054599Z","iopub.execute_input":"2023-12-30T16:56:50.054992Z","iopub.status.idle":"2023-12-30T16:56:50.074533Z","shell.execute_reply.started":"2023-12-30T16:56:50.054960Z","shell.execute_reply":"2023-12-30T16:56:50.073205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1: DATA INSPECTION","metadata":{}},{"cell_type":"markdown","source":"#### 1.1: CREATION OF THE LABEL DICTIONARY","metadata":{}},{"cell_type":"code","source":"\n# Initialize the Mapping Dictionary to be empty.\nmapping_dict = {}\n\n# Open the file in read mode.\nwith open('/kaggle/input/tiny-imagenet/tiny-imagenet-200/words.txt', 'r') as file:\n    \n    # Read each line from the file.\n    for line in file:\n        # Split the line into tokens based on whitespace.\n        tokens = line.strip().split('\\t')\n        \n        # Check if there are at least two tokens.\n        if len(tokens) >= 2:\n            # Extract the encoded label (left) and actual label (right).\n            encoded_label, actual_label = tokens[0], tokens[1]\n            \n            # Add the mapping to the dictionary.\n            mapping_dict[encoded_label] = actual_label\n\n# Print the mapping dictionary.\n#print(mapping_dict)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:50.076225Z","iopub.execute_input":"2023-12-30T16:56:50.077097Z","iopub.status.idle":"2023-12-30T16:56:50.189601Z","shell.execute_reply.started":"2023-12-30T16:56:50.077058Z","shell.execute_reply":"2023-12-30T16:56:50.188475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2: DISPLAYING EXAMPLES OF THE DATASET","metadata":{}},{"cell_type":"code","source":"\n# Loading the dataset using ImageFolder.\ndataset0 = datasets.ImageFolder(root=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", transform=None)\n\n# Extract class names and their counts.\nclass_names = dataset0.classes\nclass_counts = [dataset0.targets.count(i) for i in range(len(class_names))]\n\n# Setting the seed.\nnp.random.seed(31)\n\n# Create a grid of 10 images with labels.\nplt.figure(figsize=(15, 8))\nfor i in range(10):\n    \n    # Randomly select an image and its corresponding label.\n    index = np.random.randint(len(dataset0))\n    image, label = dataset0[index]\n\n    # Display the image with its label\n    plt.subplot(2, 5, i+1)\n    plt.imshow(np.array(image))  # Convert the PIL Image to a numpy array\n    plt.title(f\"Label: {class_names[label]}\")\n    plt.axis('off')\n\n# Displaying Datasets examples.\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:50.192143Z","iopub.execute_input":"2023-12-30T16:56:50.192554Z","iopub.status.idle":"2023-12-30T16:56:52.225946Z","shell.execute_reply.started":"2023-12-30T16:56:50.192517Z","shell.execute_reply":"2023-12-30T16:56:52.224525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3: DISPLAYING EXAMPLES OF THE DATASET WITH DECODED LABELS","metadata":{}},{"cell_type":"code","source":"\n# Loading the dataset using ImageFolder.\ndataset0 = datasets.ImageFolder(root=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", transform=None)\n\n# Extract class names and their counts.\nclass_names = dataset0.classes\nclass_counts = [dataset0.targets.count(i) for i in range(len(class_names))]\n\n# Setting the seed.\nnp.random.seed(31)\n\n# Create a grid of 10 images with labels.\nplt.figure(figsize=(15, 8))\n\nfor i in range(10):\n    \n    # Randomly select an image and its corresponding label.\n    index = np.random.randint(len(dataset0))\n    image, encoded_label = dataset0[index]\n    \n    # Look up the actual label using the mapping dictionary.\n    actual_label = mapping_dict.get(class_names[encoded_label], \"Unknown Label\")\n    \n    # Trim the label if it exceeds the maximum length.\n    actual_label_trimmed = actual_label[:15] + '...' if len(actual_label) > 15 else actual_label\n\n    # Display the image with its label..\n    plt.subplot(2, 5, i+1)\n    plt.imshow(np.array(image))  \n    plt.title(f\"Label: {actual_label_trimmed}\", wrap=True)\n    plt.axis('off')\n\n# Displaying Dataset examples.\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:52.227538Z","iopub.execute_input":"2023-12-30T16:56:52.228687Z","iopub.status.idle":"2023-12-30T16:56:54.652385Z","shell.execute_reply.started":"2023-12-30T16:56:52.228637Z","shell.execute_reply":"2023-12-30T16:56:54.651171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2: DATA-MODULE DEFINITION","metadata":{}},{"cell_type":"markdown","source":"#### 2.0: CUSTOMIZED TRANSFORM CLASS","metadata":{}},{"cell_type":"code","source":"\nclass AdaViT_Transformations:\n    \n    def __init__(self):\n        \n        # Constructor - Nothing to initialize in this case\n        pass\n\n    def __call__(self, sample):\n        \"\"\"\n        Call method to perform transformations on the input sample.\n\n        Args:\n        - sample (PIL.Image.Image or torch.Tensor): Input image sample.\n\n        Returns:\n        - transformed_sample (torch.Tensor): Transformed image sample.\n        \"\"\"\n\n        # Define a series of image transformations using \"torchvision.Compose\" function.\n        transform = transforms.Compose([\n            transforms.ToTensor(),  \n            # Additional transformations can be added here.\n            # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  \n            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n        ])\n\n        # Apply the defined transformations to the input sample.\n        transformed_sample = transform(sample)\n\n        return transformed_sample","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:54.655475Z","iopub.execute_input":"2023-12-30T16:56:54.655974Z","iopub.status.idle":"2023-12-30T16:56:54.663782Z","shell.execute_reply.started":"2023-12-30T16:56:54.655927Z","shell.execute_reply":"2023-12-30T16:56:54.662368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1: CUSTOMIZED TRAINING SET VERSION","metadata":{}},{"cell_type":"code","source":"\nclass CustomTrainingTinyImagenet(ImageFolder):\n    \n    def __init__(self, root, transform=None):\n        \"\"\"\n        Custom dataset class for Tiny ImageNet Training data.\n\n        Args:\n        - root (str): Root directory containing the dataset.\n        - transform (callable, optional): Optional transform to be applied to the Input Image.\n        \"\"\"\n        super(CustomTrainingTinyImagenet, self).__init__(root, transform=transform)\n\n        # Create mappings between class labels and numerical indices\n        self.class_to_index = {cls: idx for idx, cls in enumerate(sorted(self.classes))}\n        self.index_to_class = {idx: cls for cls, idx in self.class_to_index.items()}\n\n    def __getitem__(self, index):\n        \"\"\"\n        Method to retrieve an item from the dataset.\n\n        Args:\n        - index (int): Index of the item to retrieve.\n\n        Returns:\n        - sample (torch.Tensor): Transformed image sample.\n        - target (int): Numerical index corresponding to the class label.\n        \"\"\"\n        # Retrieve the item and its label from the Dataset.\n        path, target = self.samples[index]\n\n        # Load the image using the default loader.\n        sample = self.loader(path)\n\n        # Apply the specified transformations, if any.\n        if self.transform is not None:\n            sample = self.transform(sample)\n\n        # Adjust the directory depth to get the target label.\n        target_str = os.path.basename(os.path.dirname(os.path.dirname(path)))\n\n        # Convert string label to numerical index using the mapping.\n        target = self.class_to_index[target_str]\n\n        return sample, target\n\n    def get_class_from_index(self, index):\n        \"\"\"\n        Method to retrieve the class label from a numerical index.\n\n        Args:\n        - index (int): Numerical index corresponding to the class label.\n\n        Returns:\n        - class_label (str): Class label corresponding to the numerical index.\n        \"\"\"\n        \n        return self.index_to_class[index]","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:54.665234Z","iopub.execute_input":"2023-12-30T16:56:54.665658Z","iopub.status.idle":"2023-12-30T16:56:54.680807Z","shell.execute_reply.started":"2023-12-30T16:56:54.665624Z","shell.execute_reply":"2023-12-30T16:56:54.679693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2: CUSTOMIZED VALIDATION SET VERSION","metadata":{}},{"cell_type":"code","source":"\nclass CustomValidationTinyImagenet(pl.LightningDataModule):\n    \n    def __init__(self, root, transform=None):\n        \"\"\"\n        Custom data module for Tiny ImageNet Validation data.\n\n        Args:\n        - root (str): Root directory containing the dataset.\n        - transform (callable, optional): Optional transform to be applied to the Input Image.\n        \"\"\"\n        self.root = Path(root)\n        self.transform = transform\n\n        # Load and preprocess labels\n        self.labels = self.load_labels()\n        self.label_to_index = {label: idx for idx, label in enumerate(sorted(set(self.labels.values())))}\n        self.index_to_label = {idx: label for label, idx in self.label_to_index.items()}\n\n    def load_labels(self):\n        \"\"\"\n        Method to load and Pre-Process Labels from the Validation Dataset.\n\n        Returns:\n        - labels (dict): Dictionary mapping image names to labels.\n        \"\"\"\n        label_path = \"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/val_annotations.txt\"\n        labels = {}\n\n        with open(label_path, \"r\") as f:\n            lines = f.readlines()\n\n        for line in lines:\n            parts = line.split(\"\\t\")\n            image_name, label = parts[0], parts[1]\n            labels[image_name] = label\n\n        return labels\n\n    def __len__(self):\n        \"\"\"\n        Method to get the length of the dataset.\n\n        Returns:\n        - length (int): Number of items in the dataset.\n        \"\"\"\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Method to retrieve an item from the dataset.\n\n        Args:\n        - index (int): Index of the item to retrieve.\n\n        Returns:\n        - image (torch.Tensor): Transformed image sample.\n        - label (int): Numerical index corresponding to the class label.\n        \"\"\"\n        image_name = f\"val_{index}.JPEG\"\n        image_path = self.root / image_name\n\n        # Open the image using PIL and convert to RGB.\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Apply the specified transformations, if any.\n        if self.transform:\n            image = self.transform(image)\n\n        # Use the get method to handle cases where the key is not present.\n        label_str = self.labels.get(image_name, 'Label not found')\n\n        # Convert string label to numerical index using the mapping.\n        label = self.label_to_index[label_str]\n\n        return image, label\n\n    def get_label_from_index(self, index):\n        \"\"\"\n        Method to retrieve the class label from a numerical index.\n\n        Args:\n        - index (int): Numerical index corresponding to the class label.\n\n        Returns:\n        - class_label (str): Class label corresponding to the numerical index.\n        \"\"\"\n        return self.index_to_label[index]","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:54.682766Z","iopub.execute_input":"2023-12-30T16:56:54.683183Z","iopub.status.idle":"2023-12-30T16:56:54.698658Z","shell.execute_reply.started":"2023-12-30T16:56:54.683146Z","shell.execute_reply":"2023-12-30T16:56:54.697299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3: GENERAL DATA-MODULE DEFINITION","metadata":{}},{"cell_type":"code","source":"\nclass AViT_DataModule(pl.LightningDataModule):\n    def __init__(self, train_data_dir, val_data_dir, batch_size, num_workers=4):\n        \"\"\"\n        Custom data module for AViT model training and validation.\n\n        Args:\n        - train_data_dir (str): Directory path for the training dataset.\n        - val_data_dir (str): Directory path for the validation dataset.\n        - batch_size (int): Batch size for training and validation DataLoader.\n        - num_workers (int, optional): Number of workers for DataLoader (default is 4).\n        \"\"\"\n        super(AViT_DataModule, self).__init__()\n        self.train_data_dir = train_data_dir\n        self.val_data_dir = val_data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        # Use AdaViT transformations for data augmentation\n        self.transform = AdaViT_Transformations()\n\n    def setup(self, stage=None):\n        \"\"\"\n        Method to load and configure datasets for Training and Validation.\n\n        Args:\n        - stage (str, optional): 'fit' for Training and 'test' for Validation (default is None).\n        \"\"\"\n        # Load Train dataset using CustomTrainingTinyImagenet with the new directory structure.\n        self.train_dataset = CustomTrainingTinyImagenet(self.train_data_dir, transform=self.transform)\n\n        # Load Validation dataset.\n        self.val_dataset = CustomValidationTinyImagenet(self.val_data_dir, transform=self.transform)\n\n    def train_dataloader(self):\n        \"\"\"\n        Method to return the DataLoader for the Training Dataset.\n\n        Returns:\n        - train_dataloader (DataLoader): DataLoader for Training.\n        \"\"\"\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n\n    def val_dataloader(self):\n        \"\"\"\n        Method to return the DataLoader for the Validation Dataset.\n\n        Returns:\n        - val_dataloader (DataLoader): DataLoader for Validation.\n        \"\"\"\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:54.700044Z","iopub.execute_input":"2023-12-30T16:56:54.700437Z","iopub.status.idle":"2023-12-30T16:56:54.715936Z","shell.execute_reply.started":"2023-12-30T16:56:54.700400Z","shell.execute_reply":"2023-12-30T16:56:54.714632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.4: TESTING TRAINING AND VALIDATION DATALOADERS","metadata":{}},{"cell_type":"code","source":"\ndef show_images_labels(images, labels, title):\n    \"\"\"\n    Display Images with corresponding Labels.\n\n    Parameters:\n    - images (list of tensors): List of Image tensors.\n    - labels (list): List of corresponding Labels.\n    - title (str): Title for the entire subplot.\n\n    Returns:\n    None\n    \"\"\"\n    # Create a Subplot with 1 row and len(images) columns.\n    fig, axs = plt.subplots(1, len(images), figsize=(8, 4))\n    \n    # Set the title for the entire subplot.\n    fig.suptitle(title)\n\n    # Iterate over Images and Labels.\n    for i, (img, label) in enumerate(zip(images, labels)):\n        # Display each Image in a subplot.\n        axs[i].imshow(transforms.ToPILImage()(img))\n        \n        # Set the title for each subplot with the corresponding label.\n        axs[i].set_title(f\"Label: {label}\")\n        \n        # Turn off axis labels for better Visualization.\n        axs[i].axis('off')\n\n    # Show the entire subplot.\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:54.717875Z","iopub.execute_input":"2023-12-30T16:56:54.718425Z","iopub.status.idle":"2023-12-30T16:56:54.737807Z","shell.execute_reply.started":"2023-12-30T16:56:54.718334Z","shell.execute_reply":"2023-12-30T16:56:54.736491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define the AViT_DataModule.\ndata_module = AViT_DataModule(\n    train_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\",\n    val_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/images/\",\n    batch_size=512  \n)\n\n# Setup the Dataloaders.\ndata_module.setup()\n\n# Get a batch from the Training DataLoader.\ntrain_dataloader = data_module.train_dataloader()\ntrain_batch = next(iter(train_dataloader))\n\n# Get a batch from the Validation DataLoader.\nval_dataloader = data_module.val_dataloader()\nval_batch = next(iter(val_dataloader))\n\n# Show two Images from the Training Batch.\nshow_images_labels(train_batch[0][:2], train_batch[1][:2], title='Training Batch')\n\n# Show two Images from the  Validation Batch\nshow_images_labels(val_batch[0][:2], val_batch[1][:2], title='Validation Batch')","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:56:54.742077Z","iopub.execute_input":"2023-12-30T16:56:54.742579Z","iopub.status.idle":"2023-12-30T16:57:02.357832Z","shell.execute_reply.started":"2023-12-30T16:56:54.742535Z","shell.execute_reply":"2023-12-30T16:57:02.356737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3: MODEL DEFINITION","metadata":{}},{"cell_type":"markdown","source":"#### 3.0: PATCHING FUNCTION DEFINITION","metadata":{}},{"cell_type":"code","source":"def Make_Patches_from_Image(images, n_patches):\n    n, c, h, w = images.shape\n\n    assert h == w, \"Make_Patches_from_Image method is implemented for square images only!\"\n\n    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n    patch_size = h // n_patches\n\n    for idx, image in enumerate(images):\n        for i in range(n_patches):\n            for j in range(n_patches):\n                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n                patches[idx, i * n_patches + j] = patch.flatten()\n    return patches","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:02.359203Z","iopub.execute_input":"2023-12-30T16:57:02.360521Z","iopub.status.idle":"2023-12-30T16:57:02.369787Z","shell.execute_reply.started":"2023-12-30T16:57:02.360470Z","shell.execute_reply":"2023-12-30T16:57:02.368114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function to visualize patches\ndef visualize_patches(images, n_patches, title):\n    patches = Make_Patches_from_Image(images, n_patches)\n    \n    fig, axs = plt.subplots(n_patches, n_patches, figsize=(8, 8))\n    fig.suptitle(title)\n    \n    patch_size = images.shape[-1] // n_patches\n\n    for i in range(n_patches):\n        for j in range(n_patches):\n            patch_index = i * n_patches + j\n            # Reshape each patch to (3, patch_size, patch_size)\n            patch = patches[0, patch_index].reshape(3, patch_size, patch_size).cpu().numpy()\n            axs[i, j].imshow(patch.transpose(1, 2, 0))\n            axs[i, j].axis('off')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:02.371205Z","iopub.execute_input":"2023-12-30T16:57:02.371570Z","iopub.status.idle":"2023-12-30T16:57:02.382897Z","shell.execute_reply.started":"2023-12-30T16:57:02.371537Z","shell.execute_reply":"2023-12-30T16:57:02.381690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize patches for a Training Image.\nvisualize_patches(train_batch[0], n_patches=8, title='Training Patches')\n\n# Visualize patches for a Validation Image\nvisualize_patches(val_batch[0], n_patches=8, title='Validation Patches')","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:02.384322Z","iopub.execute_input":"2023-12-30T16:57:02.385419Z","iopub.status.idle":"2023-12-30T16:57:07.263728Z","shell.execute_reply.started":"2023-12-30T16:57:02.385375Z","shell.execute_reply":"2023-12-30T16:57:07.262465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1: POSITIONAL EMBEDDING DEFINITION","metadata":{}},{"cell_type":"code","source":"def get_positional_embeddings(sequence_length, d):\n    result = torch.ones(sequence_length, d)\n    for i in range(sequence_length):\n        for j in range(d):\n            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:07.266010Z","iopub.execute_input":"2023-12-30T16:57:07.266809Z","iopub.status.idle":"2023-12-30T16:57:07.274993Z","shell.execute_reply.started":"2023-12-30T16:57:07.266760Z","shell.execute_reply":"2023-12-30T16:57:07.274064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2: MULTI-HEAD SELF-ATTENTION DEFINITION","metadata":{}},{"cell_type":"code","source":"class MyMSA(nn.Module):\n    def __init__(self, d, n_heads=2):\n        super(MyMSA, self).__init__()\n        self.d = d\n        self.n_heads = n_heads\n\n        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n\n        d_head = int(d / n_heads)\n        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.d_head = d_head\n        self.softmax = nn.Softmax(dim=-1)\n        \n        # Initialize weights.\n        self.initialize_weights_msa()\n        \n    def forward(self, sequences):\n        # Sequences has shape (N, seq_length, token_dim)\n        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n        result = []\n        for sequence in sequences:\n            seq_result = []\n            for head in range(self.n_heads):\n                q_mapping = self.q_mappings[head]\n                k_mapping = self.k_mappings[head]\n                v_mapping = self.v_mappings[head]\n\n                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n\n                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n                seq_result.append(attention @ v)\n            result.append(torch.hstack(seq_result))\n        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n    \n    def initialize_weights_msa(self):\n        # Initialize weights for linear layers\n        for q_mapping, k_mapping, v_mapping in zip(self.q_mappings, self.k_mappings, self.v_mappings):\n            nn.init.xavier_uniform_(q_mapping.weight)\n            nn.init.xavier_uniform_(k_mapping.weight)\n            nn.init.xavier_uniform_(v_mapping.weight)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:07.276782Z","iopub.execute_input":"2023-12-30T16:57:07.277594Z","iopub.status.idle":"2023-12-30T16:57:07.294950Z","shell.execute_reply.started":"2023-12-30T16:57:07.277544Z","shell.execute_reply":"2023-12-30T16:57:07.293788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3: ViT BLOCK DEFINITION","metadata":{}},{"cell_type":"code","source":"class MyViTBlock(nn.Module):\n    def __init__(self, hidden_d, n_heads, mlp_ratio=10):\n        super(MyViTBlock, self).__init__()\n        \n        self.hidden_d = hidden_d\n        self.n_heads = n_heads\n\n        self.norm1 = nn.LayerNorm(hidden_d)\n        self.mhsa = MyMSA(hidden_d, n_heads)\n        self.norm2 = nn.LayerNorm(hidden_d)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n            nn.GELU(),\n            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n        )\n        \n        # Initialize weights.\n        self.initialize_weights_block()\n\n    def forward(self, x):\n        \n        out = x + self.mhsa(self.norm1(x))\n        out = out + self.mlp(self.norm2(out))\n        return out\n    \n    def initialize_weights_block(self):\n        \n        # Initialize weights for linear layers in mlp.\n        for layer in self.mlp:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:07.296622Z","iopub.execute_input":"2023-12-30T16:57:07.297288Z","iopub.status.idle":"2023-12-30T16:57:07.310715Z","shell.execute_reply.started":"2023-12-30T16:57:07.297249Z","shell.execute_reply":"2023-12-30T16:57:07.309343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4: ViT MODEL DEFINITION","metadata":{}},{"cell_type":"code","source":"\ndef get_distribution_target(length=4, max=1, target_depth=3, buffer=0.02):\n    \"\"\"\n    This generates the target distributional prior\n    \"\"\"\n    # this gets the distributional target to regularize the ACT halting scores towards\n    # now get a serios of length\n    data = np.arange(length)\n    data = norm.pdf(data, loc=target_depth, scale=1)\n    scaling_factor = (1.-buffer) / sum(data[:target_depth])\n    data *= scaling_factor\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:07.312874Z","iopub.execute_input":"2023-12-30T16:57:07.313299Z","iopub.status.idle":"2023-12-30T16:57:07.325694Z","shell.execute_reply.started":"2023-12-30T16:57:07.313263Z","shell.execute_reply":"2023-12-30T16:57:07.324617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyViT(nn.Module):\n        \n    def __init__(self, chw, n_patches, n_blocks, hidden_d, n_heads, out_d):\n        # Super constructor\n        super(MyViT, self).__init__()\n        self.ponder_loss = 0\n        self.distr_prior_loss = 0\n        # Attributes\n        self.chw = chw # ( C , H , W )\n        self.n_patches = n_patches\n        self.n_blocks = n_blocks\n        self.n_heads = n_heads\n        self.hidden_d = hidden_d\n        self.mlp_ratio=100\n        #halting prior distribution loss and target distribution\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n        self.distr_target = torch.Tensor(get_distribution_target())\n        \n        # Input and patches sizes\n        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n\n        # 1) Linear mapper\n        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n        \n        # 2) Learnable classification token\n        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n        \n        # 3) Positional embedding\n        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n        \n        # 4) Transformer encoder blocks\n        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n        \n        # 5) Classification MLPk\n        self.mlp = nn.Sequential(\n            nn.Linear(self.hidden_d, self.mlp_ratio * self.hidden_d),\n            nn.GELU(),\n            nn.Linear(self.mlp_ratio * self.hidden_d, out_d),\n            nn.LogSoftmax(dim=-1)\n        )\n        \n        # Initialize weights\n        self.initialize_weights()\n\n    def forward(self, images):\n        # Dividing images into patches\n        n, c, h, w = images.shape\n        patches = Make_Patches_from_Image(images, self.n_patches).to(self.positional_embeddings.device)\n        \n        # Running linear layer tokenization\n        # Map the vector corresponding to each patch to the hidden size dimension\n        tokens = self.linear_mapper(patches)\n        \n        # Adding classification token to the tokens\n        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n        \n        # Adding positional embedding\n        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n        \n        ### Halting Procedure###\n        total_token_count=len(out[1]) # out.shape = [512,65,32]\n        bs = out.size()[0]  # The batch size\n        c_token = torch.zeros(bs,total_token_count)\n        r = torch.ones(bs,total_token_count)\n        rho = torch.zeros(bs,total_token_count)\n        mask = torch.ones(bs,total_token_count)\n        \n        #halting hyperparameters\n        gamma = 5\n        beta = -10\n        alpha_p = 5e-4\n        alpha_d = 0.1\n        eps = 0.01\n        output=None #final output of vision transformer block\n        halting_score_layer=[] #list of layer halting score average\n        \n        # Transformer Blocks\n        for i,block in enumerate(self.blocks):\n            \n            #previous layers token are masked\n            out.data = out.data * mask.float().view(bs,total_token_count, 1)\n            \n            #pass data trough each layer(block)\n            out = block(out.data) #out.shape = [512,65,32]\n            \n            #compute halting scores                 \n            t_0 = out[:,:,0] #out[:,:,0] = contains all the halting scores of images tokens ( t_O.shape = [512,65])\n            h_score = torch.sigmoid(gamma*t_0 + beta)\n            h=[-1,h_score]\n            _, h_token = h\n            \n            #update list with mean of halting score just computed\n            halting_score_layer.append(torch.mean(h[1][1:])) \n            \n            #set all token halting score to one if we reached last layer(block)\n            if i == len(self.blocks)-1:\n                h_token = torch.ones(bs,total_token_count) \n                \n            #last layer protection\n            out = out * mask.float().view(bs,total_token_count, 1) #out.shape = [512,65,32]\n            \n            #update accumulator\n            c_token = c_token + h_token #c_token.shape = [512,65]\n            \n            #update #rho\n            rho += mask.float() #rho.shape = [512,65]\n        \n            # Case 1: threshold reached in this iteration\n            # token part\n            reached_token = c_token > 1 - eps #shape [512,65]\n            reached_token = reached_token.float() * mask.float()  #shape [512,65]\n            delta1 = out * r.view(bs, total_token_count, 1) * reached_token.view(bs, total_token_count, 1) # [512,65,32] * [512,65,1] * [512,65,1]\n            rho = rho + r * reached_token  #shape [512,65]\n\n            # Case 2: threshold not reached\n            # token part\n            not_reached_token = c_token < 1 - eps\n            not_reached_token = not_reached_token.float()\n            r = r - (not_reached_token.float() * h_token)\n            delta2 = out * h_token.view(bs, total_token_count, 1) * not_reached_token.view(bs, total_token_count, 1)\n            \n            # Update the mask\n            mask = c_token < 1 - eps\n            \n            if output is None:\n                output = delta1 + delta2\n            else:\n                output = output + (delta1 + delta2)\n                \n        #Halting prior distribution\n        halting_score_distr = torch.stack(halting_score_layer)\n        halting_score_distr = halting_score_distr / torch.sum(halting_score_distr)\n        halting_score_distr = torch.clamp(halting_score_distr, 0.01, 0.99) \n        \n        # KL loss\n        self.distr_prior_loss = alpha_d * self.kl_loss(halting_score_distr.log(), self.distr_target)\n        \n        # Ponder loss\n        self.ponder_loss = alpha_p * torch.mean(rho)\n        \n        # Getting the classification token only\n        output = output[:, 0] #shape=[512,32]\n        \n        return self.mlp(output) # Map to output dimension (classification head)\n    \n\n    def initialize_weights(self):\n        \n        # Initialize weights for linear layers, embeddings, etc.\n        nn.init.xavier_uniform_(self.linear_mapper.weight)\n        nn.init.normal_(self.class_token.data)\n\n        # Initialize weights for classification MLP\n        nn.init.xavier_uniform_(self.mlp[0].weight)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:07.327772Z","iopub.execute_input":"2023-12-30T16:57:07.328576Z","iopub.status.idle":"2023-12-30T16:57:07.357430Z","shell.execute_reply.started":"2023-12-30T16:57:07.328527Z","shell.execute_reply":"2023-12-30T16:57:07.355829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.5: GENERAL AViT MODEL DEFINITION","metadata":{}},{"cell_type":"code","source":"class AViT_Model(MyViT, pl.LightningModule):\n    \n    def __init__(self, input_d, n_patches, n_blocks, hidden_d, n_heads, out_d):\n        \n        super(AViT_Model, self).__init__(input_d, n_patches, n_blocks, hidden_d, n_heads, out_d)\n\n        # Definition of the Weighted Cross Entropy Loss + Label Smoothing.\n        self.loss = CrossEntropyLoss()\n        #self.ponder_loss = self.ponder_loss\n\n        # Definition of Accuracies, F1Score, Precision and Recall Metrics.\n        self.acc_top1 = Accuracy(task=\"multiclass\", num_classes=out_d)\n        self.acc_top3 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=3)\n        self.acc_top5 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=5)\n        self.acc_top10 = Accuracy(task=\"multiclass\", num_classes=out_d, top_k=10)\n        self.f1score = MulticlassF1Score(num_classes=out_d, average='macro')\n        self.precision = MulticlassPrecision(num_classes=out_d, average='macro')\n        self.recall = MulticlassRecall(num_classes=out_d, average='macro')\n        \n        # Definition of lists to be used in the \"on_ ... _epoch_end\" functions.\n        self.training_step_outputs = []\n        self.validation_step_outputs = []\n        self.test_step_outputs = []\n    \n    def _step(self, batch):\n        # Common computation of the metrics among Training, Validation and Test Set.\n        x, y = batch\n        preds = self(x) \n        loss = self.loss(preds, y) + self.ponder_loss + self.distr_prior_loss\n        acc1 = self.acc_top1(preds, y)\n        acc3 = self.acc_top3(preds,y)\n        acc5 = self.acc_top5(preds,y)\n        acc10 = self.acc_top10(preds,y)\n        f1score = self.f1score(preds, y)\n        precision = self.precision(preds, y)\n        recall = self.recall(preds, y)\n        \n        return loss, acc1, acc3, acc5, acc10, f1score, precision, recall\n    \n    \n    def training_step(self, batch, batch_idx):\n        \n        # Compute the Training Loss and Accracy.\n        loss, acc, _, _, _, _, _, _ = self._step(batch)\n\n        # Create a dictionary to represent the output of the training step.\n        training_step_output = {\n            \"train_loss\": loss.item(),\n            \"train_acc\": acc.item()\n        }\n\n        # Append the dictionary to the list.\n        self.training_step_outputs.append(training_step_output)\n\n        # Perform logging.\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \n        # Compute the Validation Loss and Accuracy.\n        loss, acc1, acc3, acc5, acc10, _, _, _ = self._step(batch)\n\n        # Create a dictionary to represent the output of the validation step.\n        validation_step_output = {\n            \"val_loss\": loss.item(),\n            \"val_acc\": acc1.item(), \n            \"val_acc_3\": acc3.item(), \n            \"val_acc_5\": acc5.item(), \n            \"val_acc_10\": acc10.item(), \n        }\n\n        # Append the dictionary to the list.\n        self.validation_step_outputs.append(validation_step_output)\n\n        # Perform logging.\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc\", acc1, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc_3\", acc3, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc_5\", acc5, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_acc_10\", acc10, on_epoch=True, prog_bar=True, logger=True)\n\n    def on_validation_epoch_end(self):\n        \n        # Calculate the Mean Loss and Accuracy from the list of dictionaries.\n        loss_tot = torch.tensor([item[\"val_loss\"] for item in self.validation_step_outputs]).mean()\n        acc_tot = torch.tensor([item[\"val_acc\"] for item in self.validation_step_outputs]).mean()\n        acc_tot_3 = torch.tensor([item[\"val_acc_3\"] for item in self.validation_step_outputs]).mean()\n        acc_tot_5 = torch.tensor([item[\"val_acc_5\"] for item in self.validation_step_outputs]).mean()\n        acc_tot_10 = torch.tensor([item[\"val_acc_10\"] for item in self.validation_step_outputs]).mean()\n\n        # Log the mean values.\n        self.log(\"val_loss\", loss_tot)\n        self.log(\"val_acc\", acc_tot)\n        self.log(\"val_acc_3\", acc_tot_3)\n        self.log(\"val_acc_5\", acc_tot_5)\n        self.log(\"val_acc_10\", acc_tot_10)\n\n        # Print messages.\n        message_loss = f'Epoch {self.current_epoch} Validation Loss -> {loss_tot}'\n        message_accuracy = f'      Validation Accuracy -> {acc_tot}'\n        message_accuracy_3 = f'      Validation Accuracy Top-3 -> {acc_tot_3}'\n        message_accuracy_5 = f'      Validation Accuracy Top-5-> {acc_tot_5}'\n        message_accuracy_10 = f'      Validation Accuracy Top-10-> {acc_tot_10}'\n        print(message_loss + message_accuracy + message_accuracy_3 + message_accuracy_5 +message_accuracy_10 )\n\n        # Clear the list to free memory.\n        self.validation_step_outputs.clear()\n\n    \n    def configure_optimizers(self):\n        \n        # Configure the Adam Optimizer.\n        optimizer = optim.Adam(self.parameters(), lr=1.5e-3, weight_decay=1.5e-4)\n        \n        # Configure the Cosine Annealing Learning Rate Scheduler.\n        scheduler = CosineAnnealingLR(optimizer, T_max=5, eta_min=1.5e-6)\n        \n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:07.359478Z","iopub.execute_input":"2023-12-30T16:57:07.359993Z","iopub.status.idle":"2023-12-30T16:57:07.388213Z","shell.execute_reply.started":"2023-12-30T16:57:07.359946Z","shell.execute_reply":"2023-12-30T16:57:07.387094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4: MODEL TRAINING","metadata":{}},{"cell_type":"markdown","source":"#### 4.1: CALLBACKS DEFINITION","metadata":{}},{"cell_type":"code","source":"# Checkpoint CallBack Definition.\nmy_checkpoint_call = ModelCheckpoint(\n    dirpath=\"/kaggle/working/checkpoints/\",\n    filename=\"Best_Model\",\n    monitor=\"val_loss\",\n    mode=\"min\",\n    save_top_k=1\n)\n\n# Learning Rate CallBack Definition.\nmy_lr_monitor_call = LearningRateMonitor(logging_interval=\"epoch\")\n\n# Early Stopping CallBack Definition.\nmy_early_stopping_call = pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=30, mode=\"min\", min_delta=0.001)\n\n# Progress Bar CallBack Definition.\nmy_progress_bar_call = TQDMProgressBar(refresh_rate=10)\n\n# TensorBoardLogger CallBack Definition.\ntb_logger = TensorBoardLogger(\"/kaggle/working/logs\", name=\"AViT\")\n\n# CSV CallBack Definition.\ncsv_logger = CSVLogger(\"/kaggle/working/logs\", name=\"AViT\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:07.389804Z","iopub.execute_input":"2023-12-30T16:57:07.390441Z","iopub.status.idle":"2023-12-30T16:57:07.404009Z","shell.execute_reply.started":"2023-12-30T16:57:07.390387Z","shell.execute_reply":"2023-12-30T16:57:07.402872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2: MODEL INSTANTIATION & TRAINING","metadata":{}},{"cell_type":"code","source":"# Initialize the model and data module.\n\n# Image Flattened Dimension = 64*64*3 = 4096*3 = 12288\n# n_patches = 16\n# chw = (3, 64, 64)\n# patch_size = (3, 64/16, 64/16) = (3, 4, 4) = (#channels, #h_patch, #w_patch)\n# 256 patch of 48 elements = 256 * 48 = 12288\n# hidden_d = linear mapping output dimension\n\n# n_block = how many times the Transformer Block is repeated. \n# n_heads = how many times we repeat self-attention.\n# out_d = number of output classes.\n\nmodel = AViT_Model((3, 64, 64), \n                   n_patches=8, \n                   n_blocks=4, \n                   hidden_d=32, \n                   n_heads=4, \n                   out_d = 200)\n\ndatamodule = AViT_DataModule(train_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/train/\", \n                             val_data_dir=\"/kaggle/input/tiny-imagenet/tiny-imagenet-200/val/images/\", \n                             batch_size=512)\n\n# Setup the dataloaders\ndata_module.setup()\n\n# Create a PyTorch Lightning Trainer.\ntrainer = pl.Trainer(\n    max_epochs=25,\n    accelerator=\"auto\", \n    devices=\"auto\",\n    log_every_n_steps=1,\n    logger=tb_logger,\n    callbacks=[my_progress_bar_call,\n               my_checkpoint_call,\n               my_lr_monitor_call,\n               my_early_stopping_call,\n               ]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:07.405815Z","iopub.execute_input":"2023-12-30T16:57:07.406580Z","iopub.status.idle":"2023-12-30T16:57:08.185444Z","shell.execute_reply.started":"2023-12-30T16:57:07.406531Z","shell.execute_reply":"2023-12-30T16:57:08.184549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrainer.fit(model, datamodule)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:57:08.187137Z","iopub.execute_input":"2023-12-30T16:57:08.187851Z","iopub.status.idle":"2023-12-30T17:18:21.999518Z","shell.execute_reply.started":"2023-12-30T16:57:08.187807Z","shell.execute_reply":"2023-12-30T17:18:21.998424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(result)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T17:18:22.001134Z","iopub.execute_input":"2023-12-30T17:18:22.001533Z","iopub.status.idle":"2023-12-30T17:18:22.006272Z","shell.execute_reply.started":"2023-12-30T17:18:22.001494Z","shell.execute_reply":"2023-12-30T17:18:22.005095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5: BEST MODEL EXTRAPOLATION","metadata":{}},{"cell_type":"code","source":"best_model_path = my_checkpoint_call.best_model_path\nbest_model = AViT_Model.load_from_checkpoint(checkpoint_path=best_model_path,\n                                             input_d=(3, 64, 64), \n                                             n_patches=8, \n                                             n_blocks=4, \n                                             hidden_d=32,\n                                             n_heads=4, \n                                             out_d = 200)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T17:18:22.007593Z","iopub.execute_input":"2023-12-30T17:18:22.008143Z","iopub.status.idle":"2023-12-30T17:18:22.177387Z","shell.execute_reply.started":"2023-12-30T17:18:22.008092Z","shell.execute_reply":"2023-12-30T17:18:22.176094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6: SAVING THE BEST MODEL","metadata":{}},{"cell_type":"code","source":"# Save it as a pth file.\n# Specify the path where you want to save the model.\nmodel_path = f\"/kaggle/working/best_model_acc.pth\"\n\n# Save the model's state dict to the specified file.\ntorch.save(best_model.state_dict(), model_path)\n\n# Save it as a CheckPoint (Specific of PyTorch Lightning = Model State Dictionary + Training State + Optimizer State)\n# Specify the path where you want to save the model checkpoint.\nckpt_path = f\"/kaggle/working/best_model_acc.ckpt\"\n\n# Save the model's state dict to the specified file\ntorch.save(best_model.state_dict(), ckpt_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T17:18:22.184266Z","iopub.execute_input":"2023-12-30T17:18:22.184907Z","iopub.status.idle":"2023-12-30T17:18:22.221083Z","shell.execute_reply.started":"2023-12-30T17:18:22.184867Z","shell.execute_reply":"2023-12-30T17:18:22.219884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7: TRAINING FROM A SAVED CHECKPOINT","metadata":{}}]}