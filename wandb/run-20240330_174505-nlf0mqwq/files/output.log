I arrive here
I arrive here 0:  /home/studio-lab-user/sagemaker-studiolab-notebooks/AdaViT/~/sagemaker-studiolab-notebooks/AdaViT/configs/train_config.yaml
/home/studio-lab-user/sagemaker-studiolab-notebooks/AdaViT/~/sagemaker-studiolab-notebooks/AdaViT/configs/train_config.yaml
Error: [Errno 2] No such file or directory: '/home/studio-lab-user/sagemaker-studiolab-notebooks/AdaViT/~/sagemaker-studiolab-notebooks/AdaViT/configs/train_config.yaml'
I arrive here 2
The settings are:  {}
The settings is:  {}
Archive found at AdaViT/imagenette_dataset/imagenette.zip, skipping download
Extracted file found at AdaViT/imagenette_dataset/imagenette2-160, skipping extraction
Warning: num_classes is not used for Imagenette dataset.
Ignoring the argument and using default number of classes in this dataset (10).
Loading timm pretrained weights:  ['facebookresearch/deit:main', 'deit_tiny_patch16_224']
Downloading timm pretrained weights:  ['facebookresearch/deit:main', 'deit_tiny_patch16_224']
Loading weights for a different number of classes. Replacing head with random weights. You should fine-tune the model.
The Model is:  VisionTransformer(
  (conv_proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
  (encoder): ViTEncoder(
    (dropout): Dropout(p=0.0, inplace=False)
    (layers): Sequential(
      (0): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (1): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (2): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (3): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (4): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (5): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (6): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (7): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (8): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (9): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (10): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (11): ViTBlock(
        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttention(
          (self_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)
          )
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
    )
    (ln): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (head): Linear(in_features=192, out_features=10, bias=True)
)
Reinitializing class_tokens... Reinitialized!
Freezing all parameters except for those containing any of these words in their names:  ['gate', 'class', 'head', 'threshold', 'budget']
Trainable parameters: ['class_tokens', 'head.weight', 'head.bias']
Frozen parameters: ['conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.0.ln_1.weight', 'encoder.layers.0.ln_1.bias', 'encoder.layers.0.self_attention.self_attention.in_proj_weight', 'encoder.layers.0.self_attention.self_attention.in_proj_bias', 'encoder.layers.0.self_attention.self_attention.out_proj.weight', 'encoder.layers.0.self_attention.self_attention.out_proj.bias', 'encoder.layers.0.ln_2.weight', 'encoder.layers.0.ln_2.bias', 'encoder.layers.0.mlp.fc1.weight', 'encoder.layers.0.mlp.fc1.bias', 'encoder.layers.0.mlp.fc2.weight', 'encoder.layers.0.mlp.fc2.bias', 'encoder.layers.1.ln_1.weight', 'encoder.layers.1.ln_1.bias', 'encoder.layers.1.self_attention.self_attention.in_proj_weight', 'encoder.layers.1.self_attention.self_attention.in_proj_bias', 'encoder.layers.1.self_attention.self_attention.out_proj.weight', 'encoder.layers.1.self_attention.self_attention.out_proj.bias', 'encoder.layers.1.ln_2.weight', 'encoder.layers.1.ln_2.bias', 'encoder.layers.1.mlp.fc1.weight', 'encoder.layers.1.mlp.fc1.bias', 'encoder.layers.1.mlp.fc2.weight', 'encoder.layers.1.mlp.fc2.bias', 'encoder.layers.2.ln_1.weight', 'encoder.layers.2.ln_1.bias', 'encoder.layers.2.self_attention.self_attention.in_proj_weight', 'encoder.layers.2.self_attention.self_attention.in_proj_bias', 'encoder.layers.2.self_attention.self_attention.out_proj.weight', 'encoder.layers.2.self_attention.self_attention.out_proj.bias', 'encoder.layers.2.ln_2.weight', 'encoder.layers.2.ln_2.bias', 'encoder.layers.2.mlp.fc1.weight', 'encoder.layers.2.mlp.fc1.bias', 'encoder.layers.2.mlp.fc2.weight', 'encoder.layers.2.mlp.fc2.bias', 'encoder.layers.3.ln_1.weight', 'encoder.layers.3.ln_1.bias', 'encoder.layers.3.self_attention.self_attention.in_proj_weight', 'encoder.layers.3.self_attention.self_attention.in_proj_bias', 'encoder.layers.3.self_attention.self_attention.out_proj.weight', 'encoder.layers.3.self_attention.self_attention.out_proj.bias', 'encoder.layers.3.ln_2.weight', 'encoder.layers.3.ln_2.bias', 'encoder.layers.3.mlp.fc1.weight', 'encoder.layers.3.mlp.fc1.bias', 'encoder.layers.3.mlp.fc2.weight', 'encoder.layers.3.mlp.fc2.bias', 'encoder.layers.4.ln_1.weight', 'encoder.layers.4.ln_1.bias', 'encoder.layers.4.self_attention.self_attention.in_proj_weight', 'encoder.layers.4.self_attention.self_attention.in_proj_bias', 'encoder.layers.4.self_attention.self_attention.out_proj.weight', 'encoder.layers.4.self_attention.self_attention.out_proj.bias', 'encoder.layers.4.ln_2.weight', 'encoder.layers.4.ln_2.bias', 'encoder.layers.4.mlp.fc1.weight', 'encoder.layers.4.mlp.fc1.bias', 'encoder.layers.4.mlp.fc2.weight', 'encoder.layers.4.mlp.fc2.bias', 'encoder.layers.5.ln_1.weight', 'encoder.layers.5.ln_1.bias', 'encoder.layers.5.self_attention.self_attention.in_proj_weight', 'encoder.layers.5.self_attention.self_attention.in_proj_bias', 'encoder.layers.5.self_attention.self_attention.out_proj.weight', 'encoder.layers.5.self_attention.self_attention.out_proj.bias', 'encoder.layers.5.ln_2.weight', 'encoder.layers.5.ln_2.bias', 'encoder.layers.5.mlp.fc1.weight', 'encoder.layers.5.mlp.fc1.bias', 'encoder.layers.5.mlp.fc2.weight', 'encoder.layers.5.mlp.fc2.bias', 'encoder.layers.6.ln_1.weight', 'encoder.layers.6.ln_1.bias', 'encoder.layers.6.self_attention.self_attention.in_proj_weight', 'encoder.layers.6.self_attention.self_attention.in_proj_bias', 'encoder.layers.6.self_attention.self_attention.out_proj.weight', 'encoder.layers.6.self_attention.self_attention.out_proj.bias', 'encoder.layers.6.ln_2.weight', 'encoder.layers.6.ln_2.bias', 'encoder.layers.6.mlp.fc1.weight', 'encoder.layers.6.mlp.fc1.bias', 'encoder.layers.6.mlp.fc2.weight', 'encoder.layers.6.mlp.fc2.bias', 'encoder.layers.7.ln_1.weight', 'encoder.layers.7.ln_1.bias', 'encoder.layers.7.self_attention.self_attention.in_proj_weight', 'encoder.layers.7.self_attention.self_attention.in_proj_bias', 'encoder.layers.7.self_attention.self_attention.out_proj.weight', 'encoder.layers.7.self_attention.self_attention.out_proj.bias', 'encoder.layers.7.ln_2.weight', 'encoder.layers.7.ln_2.bias', 'encoder.layers.7.mlp.fc1.weight', 'encoder.layers.7.mlp.fc1.bias', 'encoder.layers.7.mlp.fc2.weight', 'encoder.layers.7.mlp.fc2.bias', 'encoder.layers.8.ln_1.weight', 'encoder.layers.8.ln_1.bias', 'encoder.layers.8.self_attention.self_attention.in_proj_weight', 'encoder.layers.8.self_attention.self_attention.in_proj_bias', 'encoder.layers.8.self_attention.self_attention.out_proj.weight', 'encoder.layers.8.self_attention.self_attention.out_proj.bias', 'encoder.layers.8.ln_2.weight', 'encoder.layers.8.ln_2.bias', 'encoder.layers.8.mlp.fc1.weight', 'encoder.layers.8.mlp.fc1.bias', 'encoder.layers.8.mlp.fc2.weight', 'encoder.layers.8.mlp.fc2.bias', 'encoder.layers.9.ln_1.weight', 'encoder.layers.9.ln_1.bias', 'encoder.layers.9.self_attention.self_attention.in_proj_weight', 'encoder.layers.9.self_attention.self_attention.in_proj_bias', 'encoder.layers.9.self_attention.self_attention.out_proj.weight', 'encoder.layers.9.self_attention.self_attention.out_proj.bias', 'encoder.layers.9.ln_2.weight', 'encoder.layers.9.ln_2.bias', 'encoder.layers.9.mlp.fc1.weight', 'encoder.layers.9.mlp.fc1.bias', 'encoder.layers.9.mlp.fc2.weight', 'encoder.layers.9.mlp.fc2.bias', 'encoder.layers.10.ln_1.weight', 'encoder.layers.10.ln_1.bias', 'encoder.layers.10.self_attention.self_attention.in_proj_weight', 'encoder.layers.10.self_attention.self_attention.in_proj_bias', 'encoder.layers.10.self_attention.self_attention.out_proj.weight', 'encoder.layers.10.self_attention.self_attention.out_proj.bias', 'encoder.layers.10.ln_2.weight', 'encoder.layers.10.ln_2.bias', 'encoder.layers.10.mlp.fc1.weight', 'encoder.layers.10.mlp.fc1.bias', 'encoder.layers.10.mlp.fc2.weight', 'encoder.layers.10.mlp.fc2.bias', 'encoder.layers.11.ln_1.weight', 'encoder.layers.11.ln_1.bias', 'encoder.layers.11.self_attention.self_attention.in_proj_weight', 'encoder.layers.11.self_attention.self_attention.in_proj_bias', 'encoder.layers.11.self_attention.self_attention.out_proj.weight', 'encoder.layers.11.self_attention.self_attention.out_proj.bias', 'encoder.layers.11.ln_2.weight', 'encoder.layers.11.ln_2.bias', 'encoder.layers.11.mlp.fc1.weight', 'encoder.layers.11.mlp.fc1.bias', 'encoder.layers.11.mlp.fc2.weight', 'encoder.layers.11.mlp.fc2.bias', 'encoder.ln.weight', 'encoder.ln.bias']
Using cache found in /home/studio-lab-user/.cache/torch/hub/facebookresearch_deit_main












Training epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:27<00:00,  2.70it/s]


Validation epoch 0 : 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:05<00:00,  5.34it/s]
Training epoch 1:   1%|██▋                                                                                                                                                                                                        | 1/74 [00:01<01:31,  1.25s/it]
Saving training state for epoch 0.
[WARNING] Plotting masks is only supported for models with a budget. Skipping...












Training epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:25<00:00,  2.85it/s]
Training epoch 2:   3%|█████▍                                                                                                                                                                                                     | 2/74 [00:01<00:54,  1.32it/s]









Training epoch 2:  81%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 60/74 [00:21<00:05,  2.76it/s]
Traceback (most recent call last):
  File "/home/studio-lab-user/sagemaker-studiolab-notebooks/AdaViT/train/train.py", line 218, in <module>
    train()
  File "/home/studio-lab-user/.conda/envs/studiolab/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/studio-lab-user/.conda/envs/studiolab/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/studio-lab-user/.conda/envs/studiolab/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/studio-lab-user/.conda/envs/studiolab/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/studio-lab-user/.conda/envs/studiolab/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/studio-lab-user/.conda/envs/studiolab/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/studio-lab-user/.conda/envs/studiolab/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/studio-lab-user/sagemaker-studiolab-notebooks/AdaViT/train/train.py", line 196, in train
    train_epoch(model, train_loader, optimizer, epoch)
  File "/home/studio-lab-user/sagemaker-studiolab-notebooks/AdaViT/train/train.py", line 124, in train_epoch
    logger.log({'train/total_loss': loss.detach().item(), 'train/classification_loss': main_loss.detach().item()} | add_loss_dict)
KeyboardInterrupt